{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "00a39298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import math\n",
    "import pickle\n",
    "import random\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import sklearn\n",
    "from catboost.datasets import msrank_10k\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class Solution:\n",
    "    def __init__(self, \n",
    "                 n_estimators: int = 50, \n",
    "                 lr: float = 0.6, \n",
    "                 ndcg_top_k: int = 10,\n",
    "                 subsample: float = 1.0, \n",
    "                 colsample_bytree: float = 1.0,\n",
    "                 train_name = \"msrank_train.csv\",\n",
    "                 test_name  = \"msrank_test.csv\",\n",
    "                 **kwarg):\n",
    "        \n",
    "        self.eps = 1.e-11       \n",
    "        self.std_sc = StandardScaler()\n",
    "        self.max_sc = MaxAbsScaler() \n",
    "        self.trees = []\n",
    "        self.ndcg_lst = []\n",
    "        self.tree_cols_indx = []\n",
    "        self.best_ndcg = None\n",
    "        \n",
    "        self.gain_scheme = 'exp2'\n",
    "        self.ndcg_top_k = ndcg_top_k\n",
    "        self.n_estimators = n_estimators\n",
    "        self.lr = lr\n",
    "        self.subsample = subsample\n",
    "        self.colsample_bytree = colsample_bytree\n",
    "        self.train_dataset_name = train_name\n",
    "        self.test_dataset_name = test_name\n",
    "\n",
    "        self._prepare_data()\n",
    "        self.num_samples,  self.num_input_features = self.X_train.shape\n",
    "        \n",
    "#         self.max_depth = max_depth\n",
    "#         self.min_samples_leaf = min_samples_leaf\n",
    "\n",
    "        self.tree_params = kwarg  \n",
    "\n",
    "    def _groups_count_vectorizer(self, inp_query_ids: np.ndarray) -> Dict:\n",
    "        uniq_indx={}\n",
    "        for ind in inp_query_ids:\n",
    "            if ind not in uniq_indx:\n",
    "                uniq_indx[ind]=1\n",
    "            else:\n",
    "                uniq_indx[ind]+=1\n",
    "                \n",
    "        return uniq_indx\n",
    "        \n",
    "    def _get_data(self) -> List[np.ndarray]:\n",
    "        train_df, test_df = msrank_10k()\n",
    "\n",
    "        X_train = train_df.drop([0, 1], axis=1).values\n",
    "        y_train = train_df[0].values\n",
    "        query_ids_train = train_df[1].values.astype(int)\n",
    "\n",
    "        X_test = test_df.drop([0, 1], axis=1).values\n",
    "        y_test = test_df[0].values\n",
    "        query_ids_test = test_df[1].values.astype(int)\n",
    "\n",
    "        return [X_train, y_train, query_ids_train, X_test, y_test, query_ids_test]\n",
    "\n",
    "    def _prepare_data(self) -> None:\n",
    "        (X_train, y_train, self.query_ids_train,\n",
    "            X_test, y_test, self.query_ids_test) = self._get_data()\n",
    "        \n",
    "        self.X_train = torch.FloatTensor(self._scale_features_in_query_groups(X_train, \n",
    "                                                            self.query_ids_train))\n",
    "        self.X_test  = torch.FloatTensor(self._scale_features_in_query_groups(X_test, \n",
    "                                                            self.query_ids_test))\n",
    "        \n",
    "        self.ys_train = torch.FloatTensor(y_train).view(-1,1)\n",
    "        self.ys_test  = torch.FloatTensor(y_test).view(-1,1)\n",
    "        \n",
    "        \n",
    "    def _scale_features_in_query_groups(self, inp_feat_array: np.ndarray,\n",
    "                                        inp_query_ids: np.ndarray) -> np.ndarray:\n",
    "      \n",
    "        uniq_indx =self._groups_count_vectorizer(inp_query_ids)\n",
    "\n",
    "        shift = 0\n",
    "        for ind, cnt in uniq_indx.items():\n",
    "\n",
    "            sample = inp_feat_array[shift: shift+cnt]\n",
    "            std_lst = sample.std(axis=0)\n",
    "            const_cols = np.where(abs(std_lst) <= self.eps)[0]\n",
    "\n",
    "            mask = np.zeros(sample.shape[-1], dtype=bool)\n",
    "            mask[const_cols] = True   \n",
    "            sample[:, mask] = self.max_sc.fit_transform(sample[:,mask])\n",
    "\n",
    "            if len(const_cols) < sample.shape[-1]:\n",
    "                mask = np.ones(sample.shape[-1], dtype=bool)\n",
    "                mask[const_cols] = False  \n",
    "                sample[:, mask] = self.std_sc.fit_transform(sample[:,mask])\n",
    "\n",
    "            inp_feat_array[shift: shift+cnt] = sample\n",
    "\n",
    "            shift+=cnt\n",
    "                                                            \n",
    "        return inp_feat_array \n",
    "\n",
    "\n",
    "    def compute_gain(self, y_value: float, gain_scheme: str) -> float:\n",
    "        if gain_scheme == 'const':\n",
    "            return y_value\n",
    "        elif gain_scheme == 'exp2':\n",
    "            return 2.**y_value - 1.\n",
    "        else:\n",
    "            return y_value\n",
    "    \n",
    "    def dcg_k(self, ys_true: torch.Tensor, ys_pred: torch.Tensor,\n",
    "                ndcg_top_k: int, gain_scheme: str) -> float:\n",
    " \n",
    "        input_shape = ys_pred.shape[0]\n",
    "        cat = torch.hstack((ys_pred, ys_true))\n",
    "        t_sorted = cat[cat[:, 0].sort(descending=True)[1]]\n",
    "    #     t_sorted = t_sorted.unsqueeze(0)\n",
    "\n",
    "        input_tensor = t_sorted[:ndcg_top_k].numpy()\n",
    "\n",
    "        factors = torch.Tensor([\n",
    "                                self.compute_gain(y_value = float(input_tensor[i,1]), \n",
    "                                             gain_scheme = gain_scheme)/math.log2(i+2)\n",
    "                                for i in range(input_tensor.shape[0])\n",
    "                               ]\n",
    "                              ).type(torch.float64)\n",
    "\n",
    "        return float(torch.sum(factors,dim=0).numpy())    \n",
    "\n",
    "    \n",
    "    def _ndcg_k(self, ys_true: torch.Tensor, ys_pred: torch.Tensor,\n",
    "                ndcg_top_k: int) -> float:\n",
    "            \n",
    "        _dcg_k = self.dcg_k(ys_true, ys_pred, ndcg_top_k, self.gain_scheme)\n",
    "                       \n",
    "        _idcg_k = self.idcg_k(ys_true, self.gain_scheme, ndcg_top_k)\n",
    "\n",
    "        res = _dcg_k/_idcg_k\n",
    "        \n",
    "        return res      \n",
    "\n",
    "    def idcg_k(self, ys_true: torch.FloatTensor, \n",
    "               gain_scheme: str, ndcg_top_k: int, is2Darr: bool = True) -> float:\n",
    "\n",
    "        if not is2Darr:\n",
    "            assert ys_true.dim() == 1\n",
    "            assert ys_pred.dim() == 1    \n",
    "\n",
    "            ys_true = ys_true.reshape((-1,1)).type(torch.float64)\n",
    "\n",
    "        input_shape = ys_true.shape[0]\n",
    "\n",
    "        t_sorted = ys_true.sort(descending=True, axis=0)[0][:ndcg_top_k]\n",
    "\n",
    "        input_tensor = t_sorted.numpy()\n",
    "\n",
    "        factors = torch.Tensor([\n",
    "                                self.compute_gain(y_value = input_tensor[i], \n",
    "                                                  gain_scheme = gain_scheme\n",
    "                                                 )/math.log2(i+2)\n",
    "                                for i in range(input_tensor.shape[0])\n",
    "                               ]\n",
    "                              ).type(torch.float64)    \n",
    "\n",
    "#         idcg = torch.cusum(factors,dim=0)[-1]\n",
    "        idcg_k = torch.sum(factors,dim=0)\n",
    "        \n",
    "        res = float(idcg_k.numpy())\n",
    "        \n",
    "        return res\n",
    "    \n",
    "    \n",
    "    def idcg(self, ys_true: torch.FloatTensor, gain_scheme: str, is2Darr: bool = True) -> float:\n",
    "\n",
    "        if not is2Darr:\n",
    "            assert ys_true.dim() == 1\n",
    "            assert ys_pred.dim() == 1    \n",
    "\n",
    "            ys_true = ys_true.reshape((-1,1)).type(torch.float64)\n",
    "\n",
    "        input_shape = ys_true.shape[0]\n",
    "\n",
    "        t_sorted = ys_true.sort(descending=True, axis=0)[0]\n",
    "\n",
    "        input_tensor = t_sorted.numpy()\n",
    "\n",
    "        factors = torch.Tensor([\n",
    "                                self.compute_gain(y_value = input_tensor[i], \n",
    "                                                  gain_scheme = gain_scheme\n",
    "                                                 )/math.log2(i+2)\n",
    "                                for i in range(input_tensor.shape[0])\n",
    "                               ]\n",
    "                              ).type(torch.float64)    \n",
    "\n",
    "#         idcg = torch.cusum(factors,dim=0)[-1]\n",
    "        idcg = torch.sum(factors,dim=0)\n",
    "        \n",
    "        res = float(idcg.squeeze(0).numpy())\n",
    "        \n",
    "        return res\n",
    "    \n",
    "    def compute_labels_in_batch(self, y_true):\n",
    "\n",
    "        rel_diff = y_true - y_true.t()\n",
    "        pos_pairs = (rel_diff > 0).type(torch.float32)\n",
    "        neg_pairs = (rel_diff < 0).type(torch.float32)\n",
    "        Sij = pos_pairs - neg_pairs\n",
    "        \n",
    "        return Sij\n",
    "\n",
    "    \n",
    "    def compute_gain_diff(self, y_true, gain_scheme):\n",
    "        if gain_scheme == \"exp2\":\n",
    "            gain_diff = torch.pow(2.0, y_true) - torch.pow(2.0, y_true.t())\n",
    "        elif gain_scheme == \"diff\":\n",
    "            gain_diff = y_true - y_true.t()\n",
    "        else:\n",
    "            raise ValueError(f\"{gain_scheme} method not supported\")\n",
    "        return gain_diff    \n",
    "    \n",
    "    \n",
    "    def _compute_lambdas(self, y_true: torch.FloatTensor, y_pred: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        \n",
    "        ideal_dcg = self.idcg(y_true, gain_scheme=self.gain_scheme)\n",
    "        \n",
    "        try:\n",
    "            N = 1 / ideal_dcg\n",
    "        except:\n",
    "            N = 1.\n",
    "            \n",
    "        _, rank_order = torch.sort(y_true, descending=True, axis=0)\n",
    "        rank_order += 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pos_pairs_score_diff = 1.0 + torch.exp((y_pred - y_pred.t()))\n",
    "\n",
    "            Sij = self.compute_labels_in_batch(y_true)\n",
    "            gain_diff = self.compute_gain_diff(y_true, self.gain_scheme)\n",
    "            decay_diff = (1.0 / torch.log2(rank_order + 1.0)) - (1.0 / torch.log2(rank_order.t() + 1.0))\n",
    "            delta_ndcg = torch.abs(N * gain_diff * decay_diff)\n",
    "            lambda_update =  (0.5 * (1 - Sij) - 1 / pos_pairs_score_diff) * delta_ndcg\n",
    "            lambda_update = torch.sum(lambda_update, dim=1, keepdim=True)\n",
    "\n",
    "            return lambda_update\n",
    "\n",
    "\n",
    "    def _calc_data_ndcg(self, queries_list: np.ndarray,\n",
    "                        true_labels: torch.FloatTensor, preds: torch.FloatTensor) -> float:\n",
    "        \n",
    "        uniq_indx = self._groups_count_vectorizer(queries_list)\n",
    "        with torch.no_grad(): \n",
    "            \n",
    "            shift = 0\n",
    "            ndcgs = []\n",
    "            for ind, cnt in uniq_indx.items():\n",
    "\n",
    "                batch_preds = preds[shift: shift+cnt]\n",
    "                batch_true = true_labels[shift: shift+cnt]\n",
    "                ndcg_score = self._ndcg_k(batch_true, batch_preds, self.ndcg_top_k)            \n",
    "                ndcgs.append(ndcg_score)\n",
    "\n",
    "                shift+=cnt\n",
    "            \n",
    "            res = np.mean(ndcgs) \n",
    "        \n",
    "        return res\n",
    " \n",
    "\n",
    "    def _train_one_tree(self, cur_tree_idx: int,\n",
    "                        train_preds: torch.FloatTensor\n",
    "                        ) -> Tuple[DecisionTreeRegressor, np.ndarray]:\n",
    "        \n",
    "        if cur_tree_idx == 1:\n",
    "            self.seed = cur_tree_idx\n",
    "  \n",
    "        uniq_indx = self._groups_count_vectorizer(self.query_ids_train)\n",
    "    \n",
    "        with torch.no_grad(): \n",
    "            \n",
    "            shift = 0\n",
    "            lambda_update = torch.Tensor().view(-1,1)\n",
    "            for ind, cnt in uniq_indx.items(): \n",
    "                \n",
    "                batch_x = self.X_train[shift: shift+cnt]\n",
    "                batch_y = self.ys_train[shift: shift+cnt]\n",
    "                tr_preds = train_preds[shift: shift+cnt]\n",
    "                lambda_batch = self._compute_lambdas(batch_y, tr_preds)\n",
    "                lambda_update = torch.vstack((lambda_update,lambda_batch)) \n",
    "                \n",
    "                shift+=cnt\n",
    "            \n",
    "#             print(f\"one tree --> lambdas min {lambda_update.min()} | max {lambda_update.max()}\")\n",
    "            \n",
    "            rand_samples =  torch.randperm(self.num_samples)[:int(self.subsample*self.num_samples)]\n",
    "    \n",
    "            rand_colsample = torch.randperm(self.num_input_features)[:int(self.colsample_bytree*self.num_input_features)]\n",
    "        \n",
    "            tr_subset = self.X_train[rand_samples,:][:,rand_colsample].numpy().astype(np.float64)\n",
    "            lambdas_subset = lambda_update[rand_samples].numpy().astype(np.float64)\n",
    "            \n",
    "            imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "            \n",
    "            X = imp_mean.fit_transform(tr_subset)\n",
    "            y = lambdas_subset #imp_mean.fit_transform(lambdas_subset) \n",
    "                        \n",
    "            dtree = DecisionTreeRegressor( \n",
    "                                           random_state = self.seed,\n",
    "                                            **self.tree_params\n",
    "                                         )\n",
    "\n",
    "            dtree.fit(X, y)\n",
    "\n",
    "            return dtree, rand_colsample.numpy() \n",
    "\n",
    "        \n",
    "    def _update_terminal_regions(self, tree, X, y, lambdas, y_pred,\n",
    "                                 sample_mask = None):\n",
    "        \n",
    "        terminal_regions = tree.apply(X)\n",
    "        masked_terminal_regions = terminal_regions.copy()\n",
    "    #     masked_terminal_regions[~sample_mask] = -1\n",
    "\n",
    "        for leaf in np.where(tree.tree_.children_left ==\n",
    "                             sklearn.tree._tree.TREE_LEAF)[0]:\n",
    "            terminal_region = np.where(masked_terminal_regions == leaf)\n",
    "            suml = np.sum(lambdas[terminal_region])\n",
    "    #         sumd = np.sum(deltas[terminal_region])\n",
    "    #         tree.value[leaf, 0, 0] = 0.0 if sumd == 0.0 else (suml / sumd)\n",
    "            tree.tree_.value[leaf, 0, 0] = 0.0 if suml == 0.0 else suml\n",
    "\n",
    "        y_pred += tree.tree_.value[terminal_regions, 0, 0] * self.lr\n",
    "        \n",
    "        return y_pred, tree\n",
    "\n",
    "\n",
    "    def fit(self):\n",
    "        \n",
    "        np.random.seed(0)\n",
    "        \n",
    "        ##train cum preds\n",
    "        lamb_preds_tr = torch.zeros_like(self.ys_train) + self.eps\n",
    "        ## val cum preds\n",
    "        results = torch.zeros_like(self.ys_test) + self.eps\n",
    "        \n",
    "        for i in range(1, self.n_estimators+1):\n",
    "            \n",
    "            dtree, cols_indx = self._train_one_tree( cur_tree_idx = i,\n",
    "                                                     train_preds = lamb_preds_tr\n",
    "                                                   )  \n",
    "            \n",
    "            X = self.X_train[:,cols_indx].numpy()\n",
    "#             y = self.ys_train[cols_indx].numpy()\n",
    "            lamb_update_tr = dtree.predict(X)\n",
    "            lamb_preds_tr -= torch.Tensor(lamb_update_tr[:,np.newaxis])*self.lr\n",
    "            \n",
    "            self.trees.append(dtree)\n",
    "            self.tree_cols_indx.append(cols_indx)\n",
    "            \n",
    "#             lamb_preds_tr, dtree = self._update_terminal_regions(tree = dtree, \n",
    "#                                                                  X = X, \n",
    "#                                                                  y = y, \n",
    "#                                                                  lambdas = lamb_update_tr, \n",
    "#                                                                  y_pred = lamb_preds_tr.view(-1).numpy())                   \n",
    "#             lamb_preds_tr = torch.Tensor(lamb_preds_tr).view(-1,1)\n",
    "\n",
    "\n",
    "            X_ts = self.X_test[:,cols_indx]\n",
    "            lamb_update_ts = dtree.predict(X_ts)  \n",
    "            results -= self.lr * torch.Tensor(lamb_update_ts[:,np.newaxis])\n",
    "            mean_ndcgs    = self._calc_data_ndcg( queries_list=self.query_ids_test,\n",
    "                                                  true_labels=self.ys_test, \n",
    "                                                  preds=results\n",
    "                                                )            \n",
    "\n",
    "            self.ndcg_lst.append(mean_ndcgs)\n",
    "            \n",
    "            print(\"##estimator: {} ## TEST --> mean_ndcg: {}\".format(i, mean_ndcgs))\n",
    "        \n",
    "        self.best_tree_indx = np.argmax(self.ndcg_lst)\n",
    "        \n",
    "        if self.best_tree_indx > 0:\n",
    "            self.trees = self.trees[:self.best_tree_indx]\n",
    "            self.tree_cols_indx = self.tree_cols_indx[:self.best_tree_indx]\n",
    "        else:\n",
    "            pass\n",
    "                \n",
    "        self.best_ndcg = self.ndcg_lst[self.best_tree_indx] \n",
    "        \n",
    "\n",
    "    def predict(self, data: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        \n",
    "        results = torch.zeros_like(self.ys_test) + self.eps\n",
    "        \n",
    "        for i, tree in enumerate(self.trees):\n",
    "            X_ts = data[:,self.tree_cols_indx[i]]\n",
    "            lamb_update_ts = tree.predict(X_ts)  \n",
    "            results -= self.lr * torch.Tensor(lamb_update_ts[:,np.newaxis])\n",
    "       \n",
    "        return results.type(torch.float64) \n",
    "\n",
    "\n",
    "    def save_model(self, fname: str):\n",
    "        \"\"\"\n",
    "        Saves the model into a \".lmart\" file with the name given as a parameter.\n",
    "        Parameters\n",
    "        ----------\n",
    "        fname : string\n",
    "            Filename of the file you want to save\n",
    "\n",
    "        \"\"\"\n",
    "        directory = \"./pkl\"\n",
    "\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "        for filename in os.listdir(directory):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            try:\n",
    "                if os.path.isfile(filename) or os.path.islink(filename):\n",
    "                    os.unlink(filename)\n",
    "                elif os.path.isdir(filename):\n",
    "                    shutil.rmtree(filename)\n",
    "            except Exception as e:\n",
    "                print('Failed to delete %s. Reason: %s' % (filename, e))\n",
    "        \n",
    "        pickle.dump(self, open(os.path.join(directory,'%s.lmart' % (fname)), \"wb\"), protocol=2)\n",
    "\n",
    "        \n",
    "    def load_model(self, fname: str):\n",
    "        \"\"\"\n",
    "        Loads the model from the \".lmart\" file given as a parameter.\n",
    "        Parameters\n",
    "        ----------\n",
    "        fname : string\n",
    "            Filename of the file you want to load\n",
    "        \"\"\"\n",
    "        \n",
    "        model = pickle.load(open(fname , \"rb\"))\n",
    "        self.lr = model.lr\n",
    "        self.tree_cols_indx = model.tree_cols_indx\n",
    "        self.trees = model.trees   \n",
    "        self.best_tree_indx = model.best_tree_indx\n",
    "        self.best_ndcg = model.best_ndcg\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354bddc2",
   "metadata": {},
   "source": [
    "## Load best params from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d9091015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4819d94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = 'best'\n",
    "max_depth = 25\n",
    "min_samples_split = 2\n",
    "min_samples_leaf = 4\n",
    "min_weight_fraction_leaf = 1.e-4\n",
    "max_features = 'auto' #'sqrt', 'log2'\n",
    "max_leaf_nodes = 30\n",
    "min_impurity_decrease = 1.e-7\n",
    "ccp_alpha = 1.e-6\n",
    "\n",
    "dct_params = {\n",
    "                'splitter': splitter,\n",
    "                'max_depth': max_depth,\n",
    "                'min_samples_split': min_samples_split,\n",
    "                'min_samples_leaf': min_samples_leaf,\n",
    "                'min_weight_fraction_leaf': min_weight_fraction_leaf, \n",
    "                'max_features': max_features,\n",
    "                'max_leaf_nodes': max_leaf_nodes,\n",
    "                'min_impurity_decrease': min_impurity_decrease, \n",
    "                'ccp_alpha': ccp_alpha\n",
    "             }  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "60130fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretuned_lst = []\n",
    "with open(\"./csv/bestparams.csv\", 'r') as fin:\n",
    "    fin.readline()\n",
    "    for line in fin.readlines():\n",
    "        out = re.sub(r'\\n', \"\",line).split(\",\",2)\n",
    "        pretuned_lst.append([out[0], out[1], json.loads(out[2].replace('\"\"', '\\\\\"'), parse_float=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4fde428a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ccp_alpha': 1e-06,\n",
       " 'criterion': 'mse',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': 10,\n",
       " 'min_impurity_decrease': 0.0001,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0001,\n",
       " 'splitter': 'best'}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pretuned_lst, columns=['lr', 'ndcg', 'params'])\n",
    "df = df.sort_values(by=['ndcg'], ascending=False).reset_index(drop=True)\n",
    "\n",
    "lr, ndcg, params = df.loc[0].values.tolist()\n",
    "dct_params = json.loads(params)\n",
    "rem_key = dct_params.pop('random_state')\n",
    "dct_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cf587be9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##estimator: 1 ## TEST --> mean_ndcg: 0.34864071699909494\n",
      "##estimator: 2 ## TEST --> mean_ndcg: 0.3713904609733536\n",
      "##estimator: 3 ## TEST --> mean_ndcg: 0.38422925708306155\n",
      "##estimator: 4 ## TEST --> mean_ndcg: 0.4032068456218902\n",
      "##estimator: 5 ## TEST --> mean_ndcg: 0.42050661175160964\n",
      "##estimator: 6 ## TEST --> mean_ndcg: 0.44006618125889907\n",
      "##estimator: 7 ## TEST --> mean_ndcg: 0.4304423492994758\n",
      "##estimator: 8 ## TEST --> mean_ndcg: 0.4409100774958878\n",
      "##estimator: 9 ## TEST --> mean_ndcg: 0.44300007230436694\n",
      "##estimator: 10 ## TEST --> mean_ndcg: 0.4461061490027676\n",
      "##estimator: 11 ## TEST --> mean_ndcg: 0.43125544246857106\n",
      "##estimator: 12 ## TEST --> mean_ndcg: 0.4221370475880277\n",
      "##estimator: 13 ## TEST --> mean_ndcg: 0.42780850810323723\n",
      "##estimator: 14 ## TEST --> mean_ndcg: 0.42092615842190173\n",
      "##estimator: 15 ## TEST --> mean_ndcg: 0.4217810477719741\n",
      "##estimator: 16 ## TEST --> mean_ndcg: 0.42538767954847745\n",
      "##estimator: 17 ## TEST --> mean_ndcg: 0.42550854841947083\n",
      "##estimator: 18 ## TEST --> mean_ndcg: 0.428269442501087\n",
      "##estimator: 19 ## TEST --> mean_ndcg: 0.4279496741468026\n",
      "##estimator: 20 ## TEST --> mean_ndcg: 0.4278351304907184\n",
      "##estimator: 21 ## TEST --> mean_ndcg: 0.42686097729453215\n",
      "##estimator: 22 ## TEST --> mean_ndcg: 0.4279453872247702\n",
      "##estimator: 23 ## TEST --> mean_ndcg: 0.42769236436684577\n",
      "##estimator: 24 ## TEST --> mean_ndcg: 0.43110083101897795\n",
      "##estimator: 25 ## TEST --> mean_ndcg: 0.4352167675074801\n",
      "##estimator: 26 ## TEST --> mean_ndcg: 0.4326200924859027\n",
      "##estimator: 27 ## TEST --> mean_ndcg: 0.43183341292350885\n",
      "##estimator: 28 ## TEST --> mean_ndcg: 0.4320044172914898\n",
      "##estimator: 29 ## TEST --> mean_ndcg: 0.4322085345586911\n",
      "##estimator: 30 ## TEST --> mean_ndcg: 0.4317642570438141\n",
      "##estimator: 31 ## TEST --> mean_ndcg: 0.4322594419218669\n",
      "##estimator: 32 ## TEST --> mean_ndcg: 0.43199301198646173\n",
      "##estimator: 33 ## TEST --> mean_ndcg: 0.43282305030823576\n",
      "##estimator: 34 ## TEST --> mean_ndcg: 0.4307604716646311\n",
      "##estimator: 35 ## TEST --> mean_ndcg: 0.42963955961588185\n",
      "##estimator: 36 ## TEST --> mean_ndcg: 0.4299338515904566\n",
      "##estimator: 37 ## TEST --> mean_ndcg: 0.42988810679888445\n",
      "##estimator: 38 ## TEST --> mean_ndcg: 0.4296286427681208\n",
      "##estimator: 39 ## TEST --> mean_ndcg: 0.4297054696836547\n",
      "##estimator: 40 ## TEST --> mean_ndcg: 0.427151969577228\n",
      "##estimator: 41 ## TEST --> mean_ndcg: 0.4276219099756377\n",
      "##estimator: 42 ## TEST --> mean_ndcg: 0.42634408520658806\n",
      "##estimator: 43 ## TEST --> mean_ndcg: 0.4263674969710838\n",
      "##estimator: 44 ## TEST --> mean_ndcg: 0.4271871610955023\n",
      "##estimator: 45 ## TEST --> mean_ndcg: 0.42299697686072624\n",
      "##estimator: 46 ## TEST --> mean_ndcg: 0.4230753212221401\n",
      "##estimator: 47 ## TEST --> mean_ndcg: 0.4233301803322432\n",
      "##estimator: 48 ## TEST --> mean_ndcg: 0.4233301803322432\n",
      "##estimator: 49 ## TEST --> mean_ndcg: 0.4233301803322432\n",
      "##estimator: 50 ## TEST --> mean_ndcg: 0.4233301803322432\n"
     ]
    }
   ],
   "source": [
    "sol = Solution(lr = float(lr), **dct_params)\n",
    "sol.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0b8dd468",
   "metadata": {},
   "outputs": [],
   "source": [
    "sol.save_model(\"ftuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe987be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_37_env",
   "language": "python",
   "name": "py_37_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
