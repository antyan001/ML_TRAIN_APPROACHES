{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b2e6adfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "from typing import Dict, List, Tuple, Union, Callable\n",
    "\n",
    "from itertools import permutations\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def compute_gain(y_value: float, gain_scheme: str) -> float:\n",
    "    if gain_scheme == \"const\":\n",
    "        return y_value\n",
    "    elif gain_scheme == \"exp2\":\n",
    "        return 2 ** y_value - 1\n",
    "\n",
    "def dcg_k(ys_true: torch.Tensor, ys_pred: torch.Tensor, k: int) -> float:\n",
    "    _, indices = torch.sort(ys_pred, descending=True)\n",
    "    sorted_true = ys_true[indices][:k].numpy()\n",
    "    gain = compute_gain(sorted_true, gain_scheme=\"exp2\")\n",
    "    discount = [math.log2(float(x)) for x in range(2, len(sorted_true) + 2)]\n",
    "    discounted_gain = float((gain / discount).sum())\n",
    "    return discounted_gain\n",
    "\n",
    "\n",
    "class GaussianKernel(torch.nn.Module):\n",
    "    def __init__(self, mu: float = 1., sigma: float = 1., requires_grad = False):\n",
    "        super().__init__()\n",
    "        mu_ = np.array(mu)\n",
    "        sigma_ = np.array(sigma)\n",
    "        self.requires_grad = requires_grad\n",
    "        self.mu = torch.nn.Parameter(torch.Tensor(mu_), requires_grad=self.requires_grad)\n",
    "        self.sigma = torch.nn.Parameter(torch.Tensor(sigma_), requires_grad=self.requires_grad)\n",
    "\n",
    "    def forward(self, x):\n",
    "        adj = x - self.mu\n",
    "        return torch.exp(-0.5 * adj * adj / self.sigma / self.sigma)        \n",
    "        \n",
    "        \n",
    "# class GaussianKernel(torch.nn.Module):\n",
    "#     def __init__(self, mu: float = 1., sigma: float = 1.):\n",
    "#         super().__init__()\n",
    "#         self.mu = mu\n",
    "#         self.sigma = sigma\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # допишите ваш код здесь \n",
    "#         x = (x - self.mu) ** 2 / (2 * self.sigma ** 2)\n",
    "#         x = torch.exp( - x)\n",
    "#         return x\n",
    "        \n",
    "        \n",
    "class KNRM(torch.nn.Module):\n",
    "    def __init__(self, embedding_matrix: np.ndarray, freeze_embeddings: bool = False, kernel_num: int = 21,\n",
    "                 sigma: float = 0.1, exact_sigma: float = 0.001,\n",
    "                 out_layers: List[int] = [10, 5]):\n",
    "        super().__init__()\n",
    "        self.embeddings = torch.nn.Embedding.from_pretrained(\n",
    "            torch.FloatTensor(embedding_matrix),\n",
    "            freeze=freeze_embeddings,\n",
    "            padding_idx=0\n",
    "        )\n",
    "\n",
    "        self.kernel_num = kernel_num\n",
    "        self.sigma = sigma\n",
    "        self.exact_sigma = exact_sigma\n",
    "        self.out_layers = out_layers\n",
    "\n",
    "        self.kernels = self._get_kernels_layers()\n",
    "\n",
    "        self.mlp = self._get_mlp()\n",
    "\n",
    "        self.out_activation = torch.nn.Sigmoid()\n",
    "\n",
    "    def _get_kernels_layers(self) -> torch.nn.ModuleList:\n",
    "\n",
    "        mus = [1.0]\n",
    "        if self.kernel_num > 1:\n",
    "            bin_size = 2.0 / (self.kernel_num - 1)  \n",
    "            mus.append(1 - bin_size / 2)\n",
    "            for i in range(1, self.kernel_num - 1):\n",
    "                mus.append(mus[i] - bin_size)\n",
    "        mus = list(reversed(mus))\n",
    "        sigmas = [self.sigma] * (self.kernel_num - 1) + [self.exact_sigma]  \n",
    "        \n",
    "        gausskern_lst = [(GaussianKernel(mu,sigma)) for mu, sigma in zip(mus, sigmas)]\n",
    "        kernels = torch.nn.ModuleList(gausskern_lst)\n",
    "        return kernels\n",
    "\n",
    "    def _get_mlp(self) -> torch.nn.Sequential:        \n",
    "        if self.out_layers:\n",
    "            output = []\n",
    "            hidden_sizes = [self.kernel_num] + self.out_layers + [1]\n",
    "            for i, hidden in enumerate(hidden_sizes[1:],1):\n",
    "                output.append(torch.nn.ReLU())\n",
    "                output.append(torch.nn.Linear(hidden_sizes[i-1], hidden))\n",
    "        else:\n",
    "            output = [torch.nn.Linear(self.kernel_num, 1)]\n",
    "        return torch.nn.Sequential(*output)\n",
    "\n",
    "    def forward(self, input_1: Dict[str, torch.Tensor], input_2: Dict[str, torch.Tensor]) -> torch.FloatTensor:\n",
    "        logits_1 = self.predict(input_1)\n",
    "        logits_2 = self.predict(input_2)\n",
    "\n",
    "        logits_diff = logits_1 - logits_2\n",
    "\n",
    "        out = self.out_activation(logits_diff)\n",
    "        return out\n",
    "\n",
    "    def _get_matching_matrix(self, query: torch.Tensor, doc: torch.Tensor) -> torch.FloatTensor:\n",
    "        query = self.embeddings(query)\n",
    "        doc = self.embeddings(doc)\n",
    "        query = query / (query.norm(p=2, dim=-1, keepdim=True) + 1e-16)\n",
    "        doc = doc / (doc.norm(p=2, dim=-1, keepdim=True) + 1e-16)\n",
    "        return torch.bmm(query, doc.transpose(-1, -2))\n",
    "    \n",
    "#         q = query.numpy().tolist()\n",
    "#         d = doc.numpy().tolist()\n",
    "\n",
    "#         a_emb, b_emb = torch.Tensor(self.embeddings.weight[q]), torch.Tensor(self.embeddings.weight[d])\n",
    "#         norm_a = a_emb.norm(p=2, dim=1)\n",
    "#         norm_b = b_emb.norm(p=2, dim=1)\n",
    "\n",
    "#         sim = a_emb.matmul(b_emb.t())/norm_a/norm_b\n",
    "#         matching_matrix = sim.nan_to_num()\n",
    "#         return matching_matrix\n",
    "\n",
    "    def _apply_kernels(self, matching_matrix: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        KM = []\n",
    "        for kernel in self.kernels:\n",
    "            # shape = [B]\n",
    "            K = torch.log1p(kernel(matching_matrix).sum(dim=-1)).sum(dim=-1)\n",
    "            KM.append(K)\n",
    "                \n",
    "        # shape = [B, K]\n",
    "        kernels_out = torch.stack(KM, dim=1)\n",
    "        return kernels_out\n",
    "\n",
    "    def predict(self, inputs: Dict[str, torch.Tensor]) -> torch.FloatTensor:\n",
    "        # shape = [Batch, Left, Embedding], [Batch, Right, Embedding]\n",
    "        query, doc = inputs['query'], inputs['document']\n",
    "        \n",
    "        # shape = [Batch, Left, Right]\n",
    "        matching_matrix = self._get_matching_matrix(query, doc)\n",
    "        # shape = [Batch, Kernels]\n",
    "        kernels_out = self._apply_kernels(matching_matrix)\n",
    "        # shape = [Batch]\n",
    "        out = self.mlp(kernels_out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class RankingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, index_pairs_or_triplets: List[List[Union[str, float]]],\n",
    "                 idx_to_text_mapping: Dict[str, str], vocab: Dict[str, int], oov_val: int,\n",
    "                 preproc_func: Callable, max_len: int = 30):\n",
    "        self.index_pairs_or_triplets = index_pairs_or_triplets\n",
    "        self.idx_to_text_mapping = idx_to_text_mapping\n",
    "        self.vocab = vocab\n",
    "        self.oov_val = oov_val\n",
    "        self.preproc_func = preproc_func\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_pairs_or_triplets)\n",
    "\n",
    "    def _tokenized_text_to_index(self, tokenized_text: List[str]) -> List[int]:\n",
    "        return [self.vocab.get(item, self.oov_val) for item in tokenized_text[:self.max_len]]\n",
    "\n",
    "    def _convert_text_idx_to_token_idxs(self, idx: int) -> List[int]:\n",
    "        tokenized_text = self.preproc_func(self.idx_to_text_mapping[idx])\n",
    "        return self._tokenized_text_to_index(tokenized_text)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        pass\n",
    "\n",
    "class TrainTripletsDataset(RankingDataset):\n",
    "    def __getitem__(self, idx):\n",
    "        query, document0, document1, label = self.index_pairs_or_triplets[idx]\n",
    "        q_tokens = self._convert_text_idx_to_token_idxs(query)\n",
    "        d_tokens0 = self._convert_text_idx_to_token_idxs(document0)\n",
    "        d_tokens1 = self._convert_text_idx_to_token_idxs(document1)\n",
    "        left_elem = {'query': q_tokens, 'document': d_tokens0}\n",
    "        right_elem = {'query': q_tokens, 'document': d_tokens1}\n",
    "        \n",
    "        return left_elem, right_elem, label\n",
    "\n",
    "\n",
    "class ValPairsDataset(RankingDataset):\n",
    "    def __getitem__(self, idx):\n",
    "        query, document, label = self.index_pairs_or_triplets[idx]\n",
    "        q_tokens = self._convert_text_idx_to_token_idxs(query)\n",
    "        d_tokens = self._convert_text_idx_to_token_idxs(document)\n",
    "        qd_dct = {'query': q_tokens, 'document': d_tokens}\n",
    "        \n",
    "        return qd_dct, label\n",
    "       \n",
    "\n",
    "def collate_fn(batch_objs: List[Union[Dict[str, torch.Tensor], torch.FloatTensor]]):\n",
    "    max_len_q1 = -1\n",
    "    max_len_d1 = -1\n",
    "    max_len_q2 = -1\n",
    "    max_len_d2 = -1\n",
    "\n",
    "    is_triplets = False\n",
    "    for elem in batch_objs:\n",
    "        if len(elem) == 3:\n",
    "            left_elem, right_elem, label = elem\n",
    "            is_triplets = True\n",
    "        else:\n",
    "            left_elem, label = elem\n",
    "\n",
    "        max_len_q1 = max(len(left_elem['query']), max_len_q1)\n",
    "        max_len_d1 = max(len(left_elem['document']), max_len_d1)\n",
    "        if len(elem) == 3:\n",
    "            max_len_q2 = max(len(right_elem['query']), max_len_q2)\n",
    "            max_len_d2 = max(len(right_elem['document']), max_len_d2)\n",
    "\n",
    "    q1s = []\n",
    "    d1s = []\n",
    "    q2s = []\n",
    "    d2s = []\n",
    "    labels = []\n",
    "\n",
    "    for elem in batch_objs:\n",
    "        if is_triplets:\n",
    "            left_elem, right_elem, label = elem\n",
    "        else:\n",
    "            left_elem, label = elem\n",
    "\n",
    "        pad_len1 = max_len_q1 - len(left_elem['query'])\n",
    "        pad_len2 = max_len_d1 - len(left_elem['document'])\n",
    "        if is_triplets:\n",
    "            pad_len3 = max_len_q2 - len(right_elem['query'])\n",
    "            pad_len4 = max_len_d2 - len(right_elem['document'])\n",
    "\n",
    "        q1s.append(left_elem['query'] + [0] * pad_len1)\n",
    "        d1s.append(left_elem['document'] + [0] * pad_len2)\n",
    "        if is_triplets:\n",
    "            q2s.append(right_elem['query'] + [0] * pad_len3)\n",
    "            d2s.append(right_elem['document'] + [0] * pad_len4)\n",
    "        labels.append([label])\n",
    "    q1s = torch.LongTensor(q1s)\n",
    "    d1s = torch.LongTensor(d1s)\n",
    "    if is_triplets:\n",
    "        q2s = torch.LongTensor(q2s)\n",
    "        d2s = torch.LongTensor(d2s)\n",
    "    labels = torch.FloatTensor(labels)\n",
    "\n",
    "    ret_left = {'query': q1s, 'document': d1s}\n",
    "    if is_triplets:\n",
    "        ret_right = {'query': q2s, 'document': d2s}\n",
    "        return ret_left, ret_right, labels\n",
    "    else:\n",
    "        return ret_left, labels\n",
    "\n",
    "\n",
    "\n",
    "class Solution:\n",
    "    def __init__(self, glue_qqp_dir: str, glove_vectors_path: str,\n",
    "                 min_token_occurancies: int = 1,\n",
    "                 random_seed: int = 0,\n",
    "                 embed_size = 50,\n",
    "                 emb_rand_uni_bound: float = 0.2,\n",
    "                 freeze_knrm_embeddings: bool = True,\n",
    "                 knrm_kernel_num: int = 21,\n",
    "                 knrm_out_mlp: List[int] = [],\n",
    "                 dataloader_bs: int = 1024,\n",
    "                 train_lr: float = 0.001,\n",
    "                 change_train_loader_ep: int = 10\n",
    "                 ):\n",
    "        \n",
    "        self.gain_scheme = 'exp2'\n",
    "        self.glue_qqp_dir = glue_qqp_dir\n",
    "        self.glove_vectors_path = glove_vectors_path\n",
    "        self.glue_train_df = self.get_glue_df('train')\n",
    "        self.glue_dev_df = self.get_glue_df('dev')\n",
    "        \n",
    "        self.create_test_triples = self.sample_data_for_train_iter(self.glue_train_df)\n",
    "        self.dev_pairs_for_ndcg = self.create_val_pairs(self.glue_dev_df)\n",
    "        \n",
    "        self.min_token_occurancies = min_token_occurancies\n",
    "        self.all_tokens = self.get_all_tokens(\n",
    "            [self.glue_train_df, self.glue_dev_df], self.min_token_occurancies)\n",
    "\n",
    "        self.random_seed = random_seed\n",
    "        self.embed_size = embed_size\n",
    "        self.emb_rand_uni_bound = emb_rand_uni_bound\n",
    "        self.freeze_knrm_embeddings = freeze_knrm_embeddings\n",
    "        self.knrm_kernel_num = knrm_kernel_num\n",
    "        self.knrm_out_mlp = knrm_out_mlp\n",
    "        self.dataloader_bs = dataloader_bs\n",
    "        self.train_lr = train_lr\n",
    "        self.change_train_loader_ep = change_train_loader_ep\n",
    "\n",
    "        self.model, self.vocab, self.unk_words = self.build_knrm_model()\n",
    "        \n",
    "        self.idx_to_text_mapping_train = self.get_idx_to_text_mapping(\n",
    "            self.glue_train_df)\n",
    "        \n",
    "        self.idx_to_text_mapping_dev = self.get_idx_to_text_mapping(\n",
    "            self.glue_dev_df)\n",
    "        \n",
    "        self.val_dataset = ValPairsDataset(self.dev_pairs_for_ndcg, \n",
    "                                           self.idx_to_text_mapping_dev, \n",
    "                                           vocab=self.vocab, \n",
    "                                           oov_val=self.vocab['OOV'], \n",
    "                                           preproc_func=self.simple_preproc)\n",
    "        \n",
    "        self.val_dataloader = torch.utils.data.DataLoader(\n",
    "                                                           self.val_dataset, \n",
    "                                                           batch_size=self.dataloader_bs, \n",
    "                                                           num_workers=0, \n",
    "                                                           collate_fn=collate_fn, \n",
    "                                                           shuffle=False)\n",
    "       \n",
    "        \n",
    "    def get_glue_df(self, partition_type: str) -> pd.DataFrame:\n",
    "        assert partition_type in ['dev', 'train']\n",
    "        glue_df = pd.read_csv(\n",
    "            self.glue_qqp_dir + f'/{partition_type}.tsv', sep='\\t', dtype=object)\n",
    "        glue_df = glue_df.dropna(axis=0, how='any').reset_index(drop=True)\n",
    "        glue_df_fin = pd.DataFrame({\n",
    "            'id_left': glue_df['qid1'],\n",
    "            'id_right': glue_df['qid2'],\n",
    "            'text_left': glue_df['question1'],\n",
    "            'text_right': glue_df['question2'],\n",
    "            'label': glue_df['is_duplicate'].astype(int)\n",
    "        })\n",
    "        return glue_df_fin\n",
    "\n",
    "    def hadle_punctuation(self, inp_str: str) -> str:\n",
    "        regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "        out = regex.sub(' ', inp_str)\n",
    "        return out\n",
    "\n",
    "    def simple_preproc(self, inp_str: str) -> List[str]:\n",
    "        rem_puct_str = self.hadle_punctuation(inp_str).lower()       \n",
    "        return nltk.word_tokenize(rem_puct_str)\n",
    "    \n",
    "    def _filter_rare_words(self, vocab: Dict[str, int], min_occurancies: int) -> Dict[str, int]:\n",
    "        filter_dct = dict([(k,v) for k,v in vocab.items() if v >= min_occurancies])\n",
    "        return filter_dct\n",
    "    \n",
    "    def get_all_tokens(self, list_of_df: List[pd.DataFrame], min_occurancies: int) -> List[str]:\n",
    "        all_texts = []\n",
    "        fin_cnt = Counter()\n",
    "        for df in list_of_df:\n",
    "            all_texts += list(df.text_left)\n",
    "            all_texts += list(df.text_right)\n",
    "            counter = Counter(self.simple_preproc(\" \".join(list(set(all_texts)))))\n",
    "            fin_cnt.update(counter)\n",
    "        token_cnt = self._filter_rare_words(fin_cnt, min_occurancies)\n",
    "        return list(token_cnt.keys())\n",
    "        \n",
    "#         fin_cnt = Counter()    \n",
    "#         for df in list_of_df:\n",
    "#             tokens_union = np.concatenate(df['text_left'].apply(lambda x: self.simple_preproc(x)).values).ravel().tolist() + \\\n",
    "#                            np.concatenate(df['text_right'].apply(lambda x: self.simple_preproc(x)).values).ravel().tolist()\n",
    "#             fin_cnt.update(Counter(tokens_union))               \n",
    "#         filtered_dct = self._filter_rare_words(dict(fin_cnt), min_occurancies)           \n",
    "#         return [k for k, _ in filtered_dct.items()]\n",
    "                \n",
    "    def _read_glove_embeddings(self, file_path: str) -> Dict[str, List[str]]:\n",
    "        res = dict([tuple(line.split(\" \", 1)) for line in open(file_path, 'r')]) \n",
    "        fin = dict([(k, v.split(\" \")) for k,v in res.items() if k not in string.punctuation])\n",
    "        return fin\n",
    "\n",
    "    def create_glove_emb_from_file(self, file_path: str, inner_keys: List[str],\n",
    "                                   random_seed: int, rand_uni_bound: float\n",
    "                                   ) -> Tuple[np.ndarray, Dict[str, int], List[str]]:\n",
    "\n",
    "        word_embeddings = self._read_glove_embeddings(file_path)\n",
    "\n",
    "        unk_words = list(set(inner_keys) - set(word_embeddings.keys()))\n",
    "        known_words = list(set(inner_keys) & set(word_embeddings.keys()))\n",
    "        emb_size = len(word_embeddings[known_words[0]])\n",
    "        emb_array = np.zeros((len(inner_keys) + 2, emb_size))\n",
    "        \n",
    "        word2ind = {\"PAD\": 0, \"OOV\" : 1}\n",
    "        unk_embedding = np.random.uniform(-rand_uni_bound, rand_uni_bound, emb_size)\n",
    "        emb_array[1, :] = unk_embedding\n",
    "        for index, word in enumerate(inner_keys, 2):\n",
    "            emb_array[index, :] = word_embeddings.get(word, unk_embedding)\n",
    "            word2ind[word] = index                \n",
    "        unk_words += [\"PAD\", \"OOV\"]\n",
    "        return emb_array, word2ind, unk_words        \n",
    "        \n",
    "\n",
    "    def build_knrm_model(self) -> Tuple[torch.nn.Module, Dict[str, int], List[str]]:\n",
    "        emb_matrix, vocab, unk_words = self.create_glove_emb_from_file(\n",
    "            self.glove_vectors_path, self.all_tokens, self.random_seed, self.emb_rand_uni_bound)\n",
    "        torch.manual_seed(self.random_seed)\n",
    "        knrm = KNRM(emb_matrix, freeze_embeddings=self.freeze_knrm_embeddings,\n",
    "                    out_layers=self.knrm_out_mlp, kernel_num=self.knrm_kernel_num)\n",
    "        return knrm, vocab, unk_words\n",
    "\n",
    "\n",
    "    def create_val_pairs(self, \n",
    "                         inp_df: pd.DataFrame, \n",
    "                         fill_top_to: int = 15,\n",
    "                         min_group_size: int = 2, \n",
    "                         seed: int = 0) -> List[List[Union[str, float]]]:\n",
    "        \n",
    "        inp_df_select = inp_df[['id_left', 'id_right', 'label']]\n",
    "        inf_df_group_sizes = inp_df_select.groupby('id_left').size()\n",
    "        glue_dev_leftids_to_use = list(\n",
    "            inf_df_group_sizes[inf_df_group_sizes >= min_group_size].index)\n",
    "        groups = inp_df_select[inp_df_select.id_left.isin(\n",
    "            glue_dev_leftids_to_use)].groupby('id_left')\n",
    "\n",
    "        all_ids = set(inp_df['id_left']).union(set(inp_df['id_right']))\n",
    "\n",
    "        out_pairs = []\n",
    "\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        for id_left, group in groups:\n",
    "            ones_ids = group[group.label > 0].id_right.values\n",
    "            zeroes_ids = group[group.label == 0].id_right.values\n",
    "            sum_len = len(ones_ids) + len(zeroes_ids)\n",
    "            num_pad_items = max(0, fill_top_to - sum_len)\n",
    "            if num_pad_items > 0:\n",
    "                cur_chosen = set(ones_ids).union(\n",
    "                    set(zeroes_ids)).union({id_left})\n",
    "                pad_sample = np.random.choice(\n",
    "                    list(all_ids - cur_chosen), num_pad_items, replace=False).tolist()\n",
    "            else:\n",
    "                pad_sample = []\n",
    "            for i in ones_ids:\n",
    "                out_pairs.append([id_left, i, 2])\n",
    "            for i in zeroes_ids:\n",
    "                out_pairs.append([id_left, i, 1])\n",
    "            for i in pad_sample:\n",
    "                out_pairs.append([id_left, i, 0])\n",
    "        return out_pairs\n",
    " \n",
    "    def sample_data_for_train_iter( self, \n",
    "                                    inp_df: pd.DataFrame, \n",
    "#                                     fill_top_to: int = 15,\n",
    "                                    groupby_q: str = 'id_left',\n",
    "                                    min_group_size: int = 2,\n",
    "                                    perm_min_samples: int = 5,\n",
    "#                                     inp_frac_neg: float = 0.1,\n",
    "#                                     inp_frac_pos: float = 0.1,                            \n",
    "                                    perm_frac_neg: float = 0.5,\n",
    "                                    perm_frac_pos: float = 0.5,\n",
    "                                    seed: int = 0) -> List[List[Union[str, float]]]:\n",
    "\n",
    "        np.random.seed(seed)\n",
    "\n",
    "#         groups = inp_df[['id_left', 'id_right', 'label']].groupby('id_left')\n",
    "#         pairs_w_labels = []\n",
    "#         np.random.seed(seed)\n",
    "#         all_right_ids = inp_df.id_right.values\n",
    "#         for id_left, group in groups:\n",
    "#             labels = group.label.unique()\n",
    "#             if len(labels) > 1:\n",
    "#                 for label in labels:\n",
    "#                     same_label_samples = group[group.label ==\n",
    "#                                                label].id_right.values\n",
    "#                     if label == 0 and len(same_label_samples) > 1:\n",
    "#                         sample = np.random.choice(same_label_samples, 2, replace=False)\n",
    "#                         pairs_w_labels.append([id_left, sample[0], sample[1], 0])\n",
    "#                     elif label == 1:\n",
    "#                         less_label_samples = group[group.label < label].id_right.values\n",
    "#                         pos_sample = np.random.choice(same_label_samples, 1, replace=False)\n",
    "#                         if len(less_label_samples) > 0:\n",
    "#                             neg_sample = np.random.choice(less_label_samples, 1, replace=False)\n",
    "#                         else:\n",
    "#                             neg_sample = np.random.choice(all_right_ids, 1, replace=False)\n",
    "#                         pairs_w_labels.append([id_left, pos_sample[0], neg_sample[0], 1])\n",
    "                        \n",
    "#         return pairs_w_labels        \n",
    "        \n",
    "            \n",
    "#         inp_fracs = {0: inp_frac_neg, 1:  inp_frac_pos}     \n",
    "#         inp_sampled_df = pd.concat([dff.sample(frac=inp_fracs.get(i), \n",
    "#                                                random_state = seed) for i,dff in inp_df.groupby('label')])\n",
    "        inp_df_select = inp_df[['id_left', 'id_right', 'label']].sample(frac=0.7)\n",
    "    \n",
    "        inf_df_group_sizes = inp_df_select.groupby(groupby_q).size()\n",
    "        glue_test_leftids_to_use = list(\n",
    "            inf_df_group_sizes[inf_df_group_sizes >= min_group_size].index)\n",
    "        groups = inp_df_select[inp_df_select.id_left.isin(\n",
    "            glue_test_leftids_to_use)].groupby(groupby_q)\n",
    "\n",
    "        next_id = list(set(inp_df_select.columns.values) - set({groupby_q, 'label'}))[0] \n",
    "\n",
    "        out_pairs = []\n",
    "\n",
    "        for idx, group in groups:\n",
    "            ids = group[next_id].values\n",
    "            perm = list(permutations(ids, 2))\n",
    "            triples_lst = []\n",
    "            if len(perm) > 1:      \n",
    "                for pair in perm:\n",
    "                    label_0, label_1 = group[group[next_id].isin(pair)].label.values\n",
    "#                     diff    = ((label_0 >= label_1)&(label_1 != 0)).astype(int)\n",
    "                    if label_0 == 0:\n",
    "                        triples_lst.append(list([idx, pair[0], pair[1], 0]))\n",
    "                    else:\n",
    "                        triples_lst.append(list([idx, pair[0], pair[1], 1]))\n",
    "                        \n",
    "                triples_df = pd.DataFrame(triples_lst, columns=[groupby_q, next_id+'0', next_id+'1', 'label'])\n",
    "                \n",
    "                if len(perm) >= perm_min_samples:\n",
    "                    perm_fracs = {0: perm_frac_neg, 1: perm_frac_pos}\n",
    "                    sampled_df = pd.concat([dff.sample(frac=perm_fracs.get(i), \n",
    "                                                       random_state = seed) for i,dff in triples_df.groupby('label')])\n",
    "                    out_pairs.extend(sampled_df.values.tolist()) \n",
    "                else:\n",
    "                    out_pairs.extend(triples_df.values.tolist())\n",
    "            \n",
    "        return out_pairs    \n",
    "       \n",
    "       \n",
    "    \n",
    "    def get_idx_to_text_mapping(self, inp_df: pd.DataFrame) -> Dict[str, str]:\n",
    "        left_dict = (\n",
    "            inp_df\n",
    "            [['id_left', 'text_left']]\n",
    "            .drop_duplicates()\n",
    "            .set_index('id_left')\n",
    "            ['text_left']\n",
    "            .to_dict()\n",
    "        )\n",
    "        right_dict = (\n",
    "            inp_df\n",
    "            [['id_right', 'text_right']]\n",
    "            .drop_duplicates()\n",
    "            .set_index('id_right')\n",
    "            ['text_right']\n",
    "            .to_dict()\n",
    "        )\n",
    "        left_dict.update(right_dict)\n",
    "        return left_dict\n",
    "     \n",
    "\n",
    "    def ndcg_k(self, ys_true: np.array, ys_pred: np.array, ndcg_top_k: int = 10) -> float:\n",
    "        # допишите ваш код здесь  (обратите внимание, что используются вектора numpy)\n",
    "      \n",
    "        discounted_dsg = dcg_k(torch.Tensor(ys_true), torch.Tensor(ys_pred), ndcg_top_k)\n",
    "        ideal_dcg = dcg_k(torch.Tensor(ys_true), torch.Tensor(ys_true), ndcg_top_k)\n",
    "        \n",
    "        if ideal_dcg != 0:\n",
    "            ndcg = discounted_dsg / ideal_dcg\n",
    "        else:\n",
    "            ndcg = 0\n",
    "        return ndcg\n",
    "        \n",
    "        return current_dcg / ideal_dcg\n",
    "        \n",
    "    def compute_gain_diff(self, y_true, gain_scheme):\n",
    "        if gain_scheme == \"exp2\":\n",
    "            gain_diff = torch.pow(2.0, y_true) - torch.pow(2.0, y_true.t())\n",
    "        elif gain_scheme == \"diff\":\n",
    "            gain_diff = y_true - y_true.t()\n",
    "        else:\n",
    "            raise ValueError(f\"{gain_scheme} method not supported\")\n",
    "        return gain_diff \n",
    "\n",
    "\n",
    "    def valid(self, model: torch.nn.Module, val_dataloader: torch.utils.data.DataLoader) -> float:\n",
    "        labels_and_groups = val_dataloader.dataset.index_pairs_or_triplets\n",
    "        labels_and_groups = pd.DataFrame(labels_and_groups, columns=['left_id', 'right_id', 'rel'])\n",
    "        \n",
    "        all_preds = []\n",
    "        for batch in (val_dataloader):\n",
    "            inp_1, y = batch\n",
    "            preds = model.predict(inp_1)\n",
    "            preds_np = preds.detach().numpy()\n",
    "            all_preds.append(preds_np)\n",
    "        all_preds = np.concatenate(all_preds, axis=0)\n",
    "        labels_and_groups['preds'] = all_preds\n",
    "        \n",
    "        ndcgs = []\n",
    "        for cur_id in labels_and_groups.left_id.unique():\n",
    "            cur_df = labels_and_groups[labels_and_groups.left_id == cur_id]\n",
    "            ndcg = self.ndcg_k(cur_df.rel.values.reshape(-1), cur_df.preds.values.reshape(-1))\n",
    "            if np.isnan(ndcg):\n",
    "                ndcgs.append(0)\n",
    "            else:\n",
    "                ndcgs.append(ndcg)\n",
    "        return np.mean(ndcgs)\n",
    "\n",
    "    \n",
    "    def train(self, n_epochs: int):\n",
    "        opt = torch.optim.SGD(self.model.parameters(), lr=self.train_lr)\n",
    "        criterion = torch.nn.BCELoss()\n",
    "        # допишите ваш код здесь \n",
    "        ndcg = 0\n",
    "        for epoch in range(n_epochs):\n",
    "            epoch_loss = 0\n",
    "            if epoch % self.change_train_loader_ep == 0:\n",
    "                current_subset = self.sample_data_for_train_iter(inp_df = self.glue_train_df, \n",
    "                                                                 seed = epoch)\n",
    "                train_dataset = TrainTripletsDataset(current_subset,\n",
    "                                                     self.idx_to_text_mapping_train, \n",
    "                                                     vocab=self.vocab, \n",
    "                                                     oov_val=self.vocab['OOV'],\n",
    "                                                     preproc_func=self.simple_preproc)\n",
    "                train_dataloader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                                               batch_size=self.dataloader_bs,\n",
    "                                                               num_workers=0, \n",
    "                                                               collate_fn=collate_fn, \n",
    "                                                               shuffle=True)\n",
    "            for batch in train_dataloader:\n",
    "                inp_1, inp_2, y = batch\n",
    "                preds = self.model(inp_1, inp_2)\n",
    "                batch_loss = criterion(preds, y)\n",
    "                batch_loss.backward()\n",
    "                opt.step()\n",
    "                epoch_loss += batch_loss.item()\n",
    "            if epoch > 5:\n",
    "                ndcg = self.valid(self.model, self.val_dataloader)\n",
    "                print(\"epoch: {} ndcg: {}\".format(epoch, ndcg))\n",
    "            if ndcg > 0.925:\n",
    "                break\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1cb71b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_qqp_dir = './data/QQP'\n",
    "glove_path = './data/glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5dcc8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sbc_dev/.local/lib/python3.7/site-packages/ipykernel_launcher.py:279: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "/home/sbc_dev/.local/lib/python3.7/site-packages/ipykernel_launcher.py:280: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 26s, sys: 1.25 s, total: 1min 28s\n",
      "Wall time: 1min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sol = Solution(glue_qqp_dir = glue_qqp_dir, \n",
    "               glove_vectors_path = glove_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "582a829b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 ndcg: 0.5036094004582226\n",
      "epoch: 7 ndcg: 0.4911703318350175\n",
      "epoch: 8 ndcg: 0.6465260532099809\n",
      "epoch: 9 ndcg: 0.6790322702316374\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3406/4002622738.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, n_epochs)\u001b[0m\n\u001b[1;32m    571\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchange_train_loader_ep\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m                 \u001b[0mcurrent_subset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_data_for_train_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglue_train_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m                 train_dataset = TrainTripletsDataset(current_subset,\n\u001b[1;32m    575\u001b[0m                           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx_to_text_mapping_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3406/4002622738.py\u001b[0m in \u001b[0;36msample_data_for_train_iter\u001b[0;34m(self, inp_df, seed)\u001b[0m\n\u001b[1;32m    396\u001b[0m                         \u001b[0mpairs_w_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m                     \u001b[0;32melif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m                         \u001b[0mless_label_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid_right\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m                         \u001b[0mpos_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msame_label_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mless_label_samples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/core/arraylike.py\u001b[0m in \u001b[0;36m__lt__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__lt__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__lt__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cmp_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__le__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   5501\u001b[0m             \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomparison_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5503\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5505\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_logical_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_construct_result\u001b[0;34m(self, result, name)\u001b[0m\n\u001b[1;32m   2942\u001b[0m         \u001b[0;31m# We do not pass dtype to ensure that the Series constructor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2943\u001b[0m         \u001b[0;31m#  does inference in the case where `result` has object-dtype.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2944\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2945\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    439\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m                 \u001b[0mmanager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mode.data_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmanager\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"block\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/_config/config.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__func__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/_config/config.py\u001b[0m in \u001b[0;36m_get_option\u001b[0;34m(pat, silent)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;31m# walk the nested dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_root\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/_config/config.py\u001b[0m in \u001b[0;36m_get_root\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0mcursor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_global_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m         \u001b[0mcursor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sol.train(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd6fc51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa34f23f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_37_env",
   "language": "python",
   "name": "py_37_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
