{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-07-18 10:58:42--  https://dl.fbaipublicfiles.com/glue/data/QQP-clean.zip\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 172.67.9.4, 104.22.75.142, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 41696084 (40M) [application/zip]\n",
      "Saving to: ‘./data/QQP/QQP-clean.zip’\n",
      "\n",
      "QQP-clean.zip       100%[===================>]  39.76M  9.07MB/s    in 5.5s    \n",
      "\n",
      "2021-07-18 10:58:48 (7.21 MB/s) - ‘./data/QQP/QQP-clean.zip’ saved [41696084/41696084]\n",
      "\n",
      "Archive:  QQP-clean.zip\n",
      "  inflating: ./train.tsv             \n",
      "  inflating: ./dev.tsv               \n",
      "  inflating: ./test.tsv              \n"
     ]
    }
   ],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/glue/data/QQP-clean.zip -P ./data/QQP/ && cd ./data/QQP; unzip -j QQP-clean.zip -d ./  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0   308    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0   345    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "100  822M  100  822M    0     0  4484k      0  0:03:07  0:03:07 --:--:-- 4258k3  0:02:03 2853k3:07  0:01:04  0:02:03 2814k 305M    0     0  4406k      0  0:03:11  0:01:11  0:02:00 3870k    0  4683k      0  0:02:59  0:01:51  0:01:08 5272k  4521k      0  0:03:06  0:02:45  0:00:21 5512k62M    0     0  4541k      0  0:03:05  0:02:52  0:00:13 5070k21k      0  0:03:06  0:02:55  0:00:11 4137k\n",
      "Archive:  glove.6B.zip\n",
      "  inflating: ./glove.6B.50d.txt      \n",
      "  inflating: ./glove.6B.100d.txt     \n",
      "  inflating: ./glove.6B.200d.txt     \n",
      "  inflating: ./glove.6B.300d.txt     \n"
     ]
    }
   ],
   "source": [
    "!curl -L0 http://nlp.stanford.edu/data/glove.6B.zip --output ./data/glove.6B.zip && cd ./data/; unzip -j glove.6B.zip -d ./  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'/Users/a17194839/Library/Python/3.7/lib/python/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "from typing import Dict, List, Tuple, Union, Callable\n",
    "\n",
    "from itertools import permutations\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gain(y_value: float, gain_scheme: str) -> float:\n",
    "    if gain_scheme == \"const\":\n",
    "        return y_value\n",
    "    elif gain_scheme == \"exp2\":\n",
    "        return 2 ** y_value - 1\n",
    "\n",
    "def dcg_k(ys_true: torch.Tensor, ys_pred: torch.Tensor, k: int) -> float:\n",
    "    _, indices = torch.sort(ys_pred, descending=True)\n",
    "    sorted_true = ys_true[indices][:k].numpy()\n",
    "    gain = compute_gain(sorted_true, gain_scheme=\"exp2\")\n",
    "    discount = [math.log2(float(x)) for x in range(2, len(sorted_true) + 2)]\n",
    "    discounted_gain = float((gain / discount).sum())\n",
    "    return discounted_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_qqp_dir = './data/QQP'\n",
    "glove_path = './data/glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianKernel(torch.nn.Module):\n",
    "    def __init__(self, mu: float = 1., sigma: float = 1., requires_grad = False):\n",
    "        super().__init__()\n",
    "        mu_ = np.array(mu)\n",
    "        sigma_ = np.array(sigma)\n",
    "        self.requires_grad = requires_grad\n",
    "        self.mu = torch.nn.Parameter(torch.Tensor(mu_), requires_grad=self.requires_grad)\n",
    "        self.sigma = torch.nn.Parameter(torch.Tensor(sigma_), requires_grad=self.requires_grad)\n",
    "\n",
    "    def forward(self, x):\n",
    "        adj = x - self.mu\n",
    "        return torch.exp(-0.5 * adj * adj / self.sigma / self.sigma)        \n",
    "        \n",
    "        \n",
    "# class GaussianKernel(torch.nn.Module):\n",
    "#     def __init__(self, mu: float = 1., sigma: float = 1.):\n",
    "#         super().__init__()\n",
    "#         self.mu = mu\n",
    "#         self.sigma = sigma\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # допишите ваш код здесь \n",
    "#         x = (x - self.mu) ** 2 / (2 * self.sigma ** 2)\n",
    "#         x = torch.exp( - x)\n",
    "#         return x\n",
    "        \n",
    "        \n",
    "class KNRM(torch.nn.Module):\n",
    "    def __init__(self, embedding_matrix: np.ndarray, freeze_embeddings: bool = False, kernel_num: int = 21,\n",
    "                 sigma: float = 0.1, exact_sigma: float = 0.001,\n",
    "                 out_layers: List[int] = [10, 5]):\n",
    "        super().__init__()\n",
    "        self.embeddings = torch.nn.Embedding.from_pretrained(\n",
    "            torch.FloatTensor(embedding_matrix),\n",
    "            freeze=freeze_embeddings,\n",
    "            padding_idx=0\n",
    "        )\n",
    "\n",
    "        self.kernel_num = kernel_num\n",
    "        self.sigma = sigma\n",
    "        self.exact_sigma = exact_sigma\n",
    "        self.out_layers = out_layers\n",
    "\n",
    "        self.kernels = self._get_kernels_layers()\n",
    "\n",
    "        self.mlp = self._get_mlp()\n",
    "\n",
    "        self.out_activation = torch.nn.Sigmoid()\n",
    "\n",
    "    def _get_kernels_layers(self) -> torch.nn.ModuleList:\n",
    "\n",
    "        mus = [1.0]\n",
    "        if self.kernel_num > 1:\n",
    "            bin_size = 2.0 / (self.kernel_num - 1)  \n",
    "            mus.append(1 - bin_size / 2)\n",
    "            for i in range(1, self.kernel_num - 1):\n",
    "                mus.append(mus[i] - bin_size)\n",
    "        mus = list(reversed(mus))\n",
    "        sigmas = [self.sigma] * (self.kernel_num - 1) + [self.exact_sigma]  \n",
    "        \n",
    "        gausskern_lst = [(GaussianKernel(mu,sigma)) for mu, sigma in zip(mus, sigmas)]\n",
    "        kernels = torch.nn.ModuleList(gausskern_lst)\n",
    "        return kernels\n",
    "\n",
    "    def _get_mlp(self) -> torch.nn.Sequential:        \n",
    "        if self.out_layers:\n",
    "            output = []\n",
    "            hidden_sizes = [self.kernel_num] + self.out_layers + [1]\n",
    "            for i, hidden in enumerate(hidden_sizes[1:],1):\n",
    "                output.append(torch.nn.ReLU())\n",
    "                output.append(torch.nn.Linear(hidden_sizes[i-1], hidden))\n",
    "        else:\n",
    "            output = [torch.nn.Linear(self.kernel_num, 1)]\n",
    "        return torch.nn.Sequential(*output)\n",
    "\n",
    "    def forward(self, input_1: Dict[str, torch.Tensor], input_2: Dict[str, torch.Tensor]) -> torch.FloatTensor:\n",
    "        logits_1 = self.predict(input_1)\n",
    "        logits_2 = self.predict(input_2)\n",
    "\n",
    "        logits_diff = logits_1 - logits_2\n",
    "\n",
    "        out = self.out_activation(logits_diff)\n",
    "        return out\n",
    "\n",
    "    def _get_matching_matrix(self, query: torch.Tensor, doc: torch.Tensor) -> torch.FloatTensor:\n",
    "        query = self.embeddings(query)\n",
    "        doc = self.embeddings(doc)\n",
    "        query = query / (query.norm(p=2, dim=-1, keepdim=True) + 1e-16)\n",
    "        doc = doc / (doc.norm(p=2, dim=-1, keepdim=True) + 1e-16)\n",
    "        return torch.bmm(query, doc.transpose(-1, -2))\n",
    "    \n",
    "#         q = query.numpy().tolist()\n",
    "#         d = doc.numpy().tolist()\n",
    "\n",
    "#         a_emb, b_emb = torch.Tensor(self.embeddings.weight[q]), torch.Tensor(self.embeddings.weight[d])\n",
    "#         norm_a = a_emb.norm(p=2, dim=1)\n",
    "#         norm_b = b_emb.norm(p=2, dim=1)\n",
    "\n",
    "#         sim = a_emb.matmul(b_emb.t())/norm_a/norm_b\n",
    "#         matching_matrix = sim.nan_to_num()\n",
    "#         return matching_matrix\n",
    "\n",
    "    def _apply_kernels(self, matching_matrix: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        KM = []\n",
    "        for kernel in self.kernels:\n",
    "            # shape = [B]\n",
    "            K = torch.log1p(kernel(matching_matrix).sum(dim=-1)).sum(dim=-1)\n",
    "            KM.append(K)\n",
    "                \n",
    "        # shape = [B, K]\n",
    "        kernels_out = torch.stack(KM, dim=1)\n",
    "        return kernels_out\n",
    "\n",
    "    def predict(self, inputs: Dict[str, torch.Tensor]) -> torch.FloatTensor:\n",
    "        # shape = [Batch, Left, Embedding], [Batch, Right, Embedding]\n",
    "        query, doc = inputs['query'], inputs['document']\n",
    "        \n",
    "        # shape = [Batch, Left, Right]\n",
    "        matching_matrix = self._get_matching_matrix(query, doc)\n",
    "        # shape = [Batch, Kernels]\n",
    "        kernels_out = self._apply_kernels(matching_matrix)\n",
    "        # shape = [Batch]\n",
    "        out = self.mlp(kernels_out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, index_pairs_or_triplets: List[List[Union[str, float]]],\n",
    "                 idx_to_text_mapping: Dict[str, str], vocab: Dict[str, int], oov_val: int,\n",
    "                 preproc_func: Callable, max_len: int = 30):\n",
    "        self.index_pairs_or_triplets = index_pairs_or_triplets\n",
    "        self.idx_to_text_mapping = idx_to_text_mapping\n",
    "        self.vocab = vocab\n",
    "        self.oov_val = oov_val\n",
    "        self.preproc_func = preproc_func\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_pairs_or_triplets)\n",
    "\n",
    "    def _tokenized_text_to_index(self, tokenized_text: List[str]) -> List[int]:\n",
    "        return [self.vocab.get(item, self.oov_val) for item in tokenized_text[:self.max_len]]\n",
    "\n",
    "    def _convert_text_idx_to_token_idxs(self, idx: int) -> List[int]:\n",
    "        tokenized_text = self.preproc_func(self.idx_to_text_mapping[idx])\n",
    "        return self._tokenized_text_to_index(tokenized_text)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        pass\n",
    "\n",
    "class TrainTripletsDataset(RankingDataset):\n",
    "    def __getitem__(self, idx):\n",
    "        query, document0, document1, label = self.index_pairs_or_triplets[idx]\n",
    "        q_tokens = self._convert_text_idx_to_token_idxs(query)\n",
    "        d_tokens0 = self._convert_text_idx_to_token_idxs(document0)\n",
    "        d_tokens1 = self._convert_text_idx_to_token_idxs(document1)\n",
    "        left_elem = {'query': q_tokens, 'document': d_tokens0}\n",
    "        right_elem = {'query': q_tokens, 'document': d_tokens1}\n",
    "        \n",
    "        return left_elem, right_elem, label\n",
    "\n",
    "\n",
    "class ValPairsDataset(RankingDataset):\n",
    "    def __getitem__(self, idx):\n",
    "        query, document, label = self.index_pairs_or_triplets[idx]\n",
    "        q_tokens = self._convert_text_idx_to_token_idxs(query)\n",
    "        d_tokens = self._convert_text_idx_to_token_idxs(document)\n",
    "        qd_dct = {'query': q_tokens, 'document': d_tokens}\n",
    "        \n",
    "        return qd_dct, label\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch_objs: List[Union[Dict[str, torch.Tensor], torch.FloatTensor]]):\n",
    "    max_len_q1 = -1\n",
    "    max_len_d1 = -1\n",
    "    max_len_q2 = -1\n",
    "    max_len_d2 = -1\n",
    "\n",
    "    is_triplets = False\n",
    "    for elem in batch_objs:\n",
    "        if len(elem) == 3:\n",
    "            left_elem, right_elem, label = elem\n",
    "            is_triplets = True\n",
    "        else:\n",
    "            left_elem, label = elem\n",
    "\n",
    "        max_len_q1 = max(len(left_elem['query']), max_len_q1)\n",
    "        max_len_d1 = max(len(left_elem['document']), max_len_d1)\n",
    "        if len(elem) == 3:\n",
    "            max_len_q2 = max(len(right_elem['query']), max_len_q2)\n",
    "            max_len_d2 = max(len(right_elem['document']), max_len_d2)\n",
    "\n",
    "    q1s = []\n",
    "    d1s = []\n",
    "    q2s = []\n",
    "    d2s = []\n",
    "    labels = []\n",
    "\n",
    "    for elem in batch_objs:\n",
    "        if is_triplets:\n",
    "            left_elem, right_elem, label = elem\n",
    "        else:\n",
    "            left_elem, label = elem\n",
    "\n",
    "        pad_len1 = max_len_q1 - len(left_elem['query'])\n",
    "        pad_len2 = max_len_d1 - len(left_elem['document'])\n",
    "        if is_triplets:\n",
    "            pad_len3 = max_len_q2 - len(right_elem['query'])\n",
    "            pad_len4 = max_len_d2 - len(right_elem['document'])\n",
    "\n",
    "        q1s.append(left_elem['query'] + [0] * pad_len1)\n",
    "        d1s.append(left_elem['document'] + [0] * pad_len2)\n",
    "        if is_triplets:\n",
    "            q2s.append(right_elem['query'] + [0] * pad_len3)\n",
    "            d2s.append(right_elem['document'] + [0] * pad_len4)\n",
    "        labels.append([label])\n",
    "    q1s = torch.LongTensor(q1s)\n",
    "    d1s = torch.LongTensor(d1s)\n",
    "    if is_triplets:\n",
    "        q2s = torch.LongTensor(q2s)\n",
    "        d2s = torch.LongTensor(d2s)\n",
    "    labels = torch.FloatTensor(labels)\n",
    "\n",
    "    ret_left = {'query': q1s, 'document': d1s}\n",
    "    if is_triplets:\n",
    "        ret_right = {'query': q2s, 'document': d2s}\n",
    "        return ret_left, ret_right, labels\n",
    "    else:\n",
    "        return ret_left, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solution:\n",
    "    def __init__(self, glue_qqp_dir: str, glove_vectors_path: str,\n",
    "                 min_token_occurancies: int = 1,\n",
    "                 random_seed: int = 0,\n",
    "                 embed_size = 50,\n",
    "                 emb_rand_uni_bound: float = 0.2,\n",
    "                 freeze_knrm_embeddings: bool = True,\n",
    "                 knrm_kernel_num: int = 30,\n",
    "                 knrm_out_mlp: List[int] = [15, 7],\n",
    "                 dataloader_bs: int = 1024,\n",
    "                 train_lr: float = 0.01,\n",
    "                 change_train_loader_ep: int = 10\n",
    "                 ):\n",
    "        \n",
    "        self.gain_scheme = 'exp2'\n",
    "        self.glue_qqp_dir = glue_qqp_dir\n",
    "        self.glove_vectors_path = glove_vectors_path\n",
    "        self.glue_train_df = self.get_glue_df('train')\n",
    "        self.glue_dev_df = self.get_glue_df('dev')\n",
    "        \n",
    "        self.create_test_triples = self.sample_data_for_train_iter(self.glue_train_df)\n",
    "        self.dev_pairs_for_ndcg = self.create_val_pairs(self.glue_dev_df)\n",
    "        \n",
    "        self.min_token_occurancies = min_token_occurancies\n",
    "        self.all_tokens = self.get_all_tokens(\n",
    "            [self.glue_train_df, self.glue_dev_df], self.min_token_occurancies)\n",
    "\n",
    "        self.random_seed = random_seed\n",
    "        self.embed_size = embed_size\n",
    "        self.emb_rand_uni_bound = emb_rand_uni_bound\n",
    "        self.freeze_knrm_embeddings = freeze_knrm_embeddings\n",
    "        self.knrm_kernel_num = knrm_kernel_num\n",
    "        self.knrm_out_mlp = knrm_out_mlp\n",
    "        self.dataloader_bs = dataloader_bs\n",
    "        self.train_lr = train_lr\n",
    "        self.change_train_loader_ep = change_train_loader_ep\n",
    "\n",
    "        self.model, self.vocab, self.unk_words = self.build_knrm_model()\n",
    "        \n",
    "        self.idx_to_text_mapping_train = self.get_idx_to_text_mapping(\n",
    "            self.glue_train_df)\n",
    "        \n",
    "        self.idx_to_text_mapping_dev = self.get_idx_to_text_mapping(\n",
    "            self.glue_dev_df)\n",
    "        \n",
    "        self.val_dataset = ValPairsDataset(self.dev_pairs_for_ndcg, \n",
    "                                           self.idx_to_text_mapping_dev, \n",
    "                                           vocab=self.vocab, \n",
    "                                           oov_val=self.vocab['OOV'], \n",
    "                                           preproc_func=self.simple_preproc)\n",
    "        \n",
    "        self.val_dataloader = torch.utils.data.DataLoader(\n",
    "                                                           self.val_dataset, \n",
    "                                                           batch_size=self.dataloader_bs, \n",
    "                                                           num_workers=0, \n",
    "                                                           collate_fn=collate_fn, \n",
    "                                                           shuffle=False)\n",
    "\n",
    "#         self.test_dataset = TrainTripletsDataset(self.create_test_triples, \n",
    "#                                                  self.idx_to_text_mapping_train, \n",
    "#                                                  vocab=self.vocab, \n",
    "#                                                  oov_val=self.vocab['OOV'], \n",
    "#                                                  preproc_func=self.simple_preproc)\n",
    "        \n",
    "#         self.test_dataloader = torch.utils.data.DataLoader(\n",
    "#                                                            self.test_dataset, \n",
    "#                                                            batch_size=self.dataloader_bs, \n",
    "#                                                            num_workers=0, \n",
    "#                                                            collate_fn=collate_fn, \n",
    "#                                                            shuffle=False)        \n",
    "        \n",
    "    def get_glue_df(self, partition_type: str) -> pd.DataFrame:\n",
    "        assert partition_type in ['dev', 'train']\n",
    "        glue_df = pd.read_csv(\n",
    "            self.glue_qqp_dir + f'/{partition_type}.tsv', sep='\\t', dtype=object)\n",
    "        glue_df = glue_df.dropna(axis=0, how='any').reset_index(drop=True)\n",
    "        glue_df_fin = pd.DataFrame({\n",
    "            'id_left': glue_df['qid1'],\n",
    "            'id_right': glue_df['qid2'],\n",
    "            'text_left': glue_df['question1'],\n",
    "            'text_right': glue_df['question2'],\n",
    "            'label': glue_df['is_duplicate'].astype(int)\n",
    "        })\n",
    "        return glue_df_fin\n",
    "\n",
    "    def hadle_punctuation(self, inp_str: str) -> str:\n",
    "        regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "        out = regex.sub('', inp_str)\n",
    "        return out\n",
    "\n",
    "    def simple_preproc(self, inp_str: str) -> List[str]:\n",
    "        rem_puct_str = self.hadle_punctuation(inp_str).lower()       \n",
    "        return nltk.word_tokenize(rem_puct_str)\n",
    "    \n",
    "    def _filter_rare_words(self, vocab: Dict[str, int], min_occurancies: int) -> Dict[str, int]:\n",
    "        filter_dct = dict([(k,v) for k,v in vocab.items() if v >= min_occurancies])\n",
    "        return filter_dct\n",
    "    \n",
    "    def get_all_tokens(self, list_of_df: List[pd.DataFrame], min_occurancies: int) -> List[str]:\n",
    "        all_texts = []\n",
    "        fin_cnt = Counter()\n",
    "        for df in list_of_df:\n",
    "            all_texts += list(df.text_left)\n",
    "            all_texts += list(df.text_right)\n",
    "            counter = Counter(self.simple_preproc(\" \".join(list(set(all_texts)))))\n",
    "            fin_cnt.update(counter)\n",
    "        token_cnt = self._filter_rare_words(fin_cnt, min_occurancies)\n",
    "        return list(token_cnt.keys())\n",
    "        \n",
    "#         fin_cnt = Counter()    \n",
    "#         for df in list_of_df:\n",
    "#             tokens_union = np.concatenate(df['text_left'].apply(lambda x: self.simple_preproc(x)).values).ravel().tolist() + \\\n",
    "#                            np.concatenate(df['text_right'].apply(lambda x: self.simple_preproc(x)).values).ravel().tolist()\n",
    "#             fin_cnt.update(Counter(tokens_union))               \n",
    "#         filtered_dct = self._filter_rare_words(dict(fin_cnt), min_occurancies)           \n",
    "#         return [k for k, _ in filtered_dct.items()]\n",
    "                \n",
    "    def _read_glove_embeddings(self, file_path: str) -> Dict[str, List[str]]:\n",
    "        res = dict([tuple(line.split(\" \", 1)) for line in open(file_path, 'r')]) \n",
    "        fin = dict([(k, v.split(\" \")) for k,v in res.items() if k not in string.punctuation])\n",
    "        return fin\n",
    "\n",
    "    def create_glove_emb_from_file(self, file_path: str, inner_keys: List[str],\n",
    "                                   random_seed: int, rand_uni_bound: float\n",
    "                                   ) -> Tuple[np.ndarray, Dict[str, int], List[str]]:\n",
    "\n",
    "        word_embeddings = self._read_glove_embeddings(file_path)\n",
    "\n",
    "        unk_words = list(set(inner_keys) - set(word_embeddings.keys()))\n",
    "        known_words = list(set(inner_keys) & set(word_embeddings.keys()))\n",
    "        emb_size = len(word_embeddings[known_words[0]])\n",
    "        emb_array = np.zeros((len(inner_keys) + 2, emb_size))\n",
    "        \n",
    "        word2ind = {\"PAD\": 0, \"OOV\" : 1}\n",
    "        unk_embedding = np.random.uniform(-rand_uni_bound, rand_uni_bound, emb_size)\n",
    "        emb_array[1, :] = unk_embedding\n",
    "        for index, word in enumerate(inner_keys, 2):\n",
    "            emb_array[index, :] = word_embeddings.get(word, unk_embedding)\n",
    "            word2ind[word] = index                \n",
    "        unk_words += [\"PAD\", \"OOV\"]\n",
    "        return emb_array, word2ind, unk_words        \n",
    "        \n",
    "\n",
    "    def build_knrm_model(self) -> Tuple[torch.nn.Module, Dict[str, int], List[str]]:\n",
    "        emb_matrix, vocab, unk_words = self.create_glove_emb_from_file(\n",
    "            self.glove_vectors_path, self.all_tokens, self.random_seed, self.emb_rand_uni_bound)\n",
    "        torch.manual_seed(self.random_seed)\n",
    "        knrm = KNRM(emb_matrix, freeze_embeddings=self.freeze_knrm_embeddings,\n",
    "                    out_layers=self.knrm_out_mlp, kernel_num=self.knrm_kernel_num)\n",
    "        return knrm, vocab, unk_words\n",
    "\n",
    "\n",
    "    def create_val_pairs(self, \n",
    "                         inp_df: pd.DataFrame, \n",
    "                         fill_top_to: int = 15,\n",
    "                         min_group_size: int = 2, \n",
    "                         seed: int = 0) -> List[List[Union[str, float]]]:\n",
    "        \n",
    "        inp_df_select = inp_df[['id_left', 'id_right', 'label']]\n",
    "        inf_df_group_sizes = inp_df_select.groupby('id_left').size()\n",
    "        glue_dev_leftids_to_use = list(\n",
    "            inf_df_group_sizes[inf_df_group_sizes >= min_group_size].index)\n",
    "        groups = inp_df_select[inp_df_select.id_left.isin(\n",
    "            glue_dev_leftids_to_use)].groupby('id_left')\n",
    "\n",
    "        all_ids = set(inp_df['id_left']).union(set(inp_df['id_right']))\n",
    "\n",
    "        out_pairs = []\n",
    "\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        for id_left, group in groups:\n",
    "            ones_ids = group[group.label > 0].id_right.values\n",
    "            zeroes_ids = group[group.label == 0].id_right.values\n",
    "            sum_len = len(ones_ids) + len(zeroes_ids)\n",
    "            num_pad_items = max(0, fill_top_to - sum_len)\n",
    "            if num_pad_items > 0:\n",
    "                cur_chosen = set(ones_ids).union(\n",
    "                    set(zeroes_ids)).union({id_left})\n",
    "                pad_sample = np.random.choice(\n",
    "                    list(all_ids - cur_chosen), num_pad_items, replace=False).tolist()\n",
    "            else:\n",
    "                pad_sample = []\n",
    "            for i in ones_ids:\n",
    "                out_pairs.append([id_left, i, 2])\n",
    "            for i in zeroes_ids:\n",
    "                out_pairs.append([id_left, i, 1])\n",
    "            for i in pad_sample:\n",
    "                out_pairs.append([id_left, i, 0])\n",
    "        return out_pairs\n",
    " \n",
    "    def sample_data_for_train_iter( self, \n",
    "                                    inp_df: pd.DataFrame, \n",
    "#                                     fill_top_to: int = 15,\n",
    "#                                     groupby_q: str = 'id_left',\n",
    "#                                     min_group_size: int = 2,\n",
    "#                                     perm_min_samples: int = 5,\n",
    "#                                     inp_frac_neg: float = 0.1,\n",
    "#                                     inp_frac_pos: float = 0.1,                            \n",
    "#                                     perm_frac_neg: float = 0.9,\n",
    "#                                     perm_frac_pos: float = 0.9,\n",
    "                                    seed: int = 0) -> List[List[Union[str, float]]]:\n",
    "\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        groups = inp_df[['id_left', 'id_right', 'label']].groupby('id_left')\n",
    "        pairs_w_labels = []\n",
    "        np.random.seed(seed)\n",
    "        all_right_ids = inp_df.id_right.values\n",
    "        for id_left, group in groups:\n",
    "            labels = group.label.unique()\n",
    "            if len(labels) > 1:\n",
    "                for label in labels:\n",
    "                    same_label_samples = group[group.label ==\n",
    "                                               label].id_right.values\n",
    "                    if label == 0 and len(same_label_samples) > 1:\n",
    "                        sample = np.random.choice(same_label_samples, 2, replace=False)\n",
    "                        pairs_w_labels.append([id_left, sample[0], sample[1], 0.5])\n",
    "                    elif label == 1:\n",
    "                        less_label_samples = group[group.label < label].id_right.values\n",
    "                        pos_sample = np.random.choice(same_label_samples, 1, replace=False)\n",
    "                        if len(less_label_samples) > 0:\n",
    "                            neg_sample = np.random.choice(less_label_samples, 1, replace=False)\n",
    "                        else:\n",
    "                            neg_sample = np.random.choice(all_right_ids, 1, replace=False)\n",
    "                        pairs_w_labels.append([id_left, pos_sample[0], neg_sample[0], 1])\n",
    "                        \n",
    "        return pairs_w_labels        \n",
    "        \n",
    "            \n",
    "#         inp_fracs = {0: inp_frac_neg, 1:  inp_frac_pos}     \n",
    "#         inp_sampled_df = pd.concat([dff.sample(frac=inp_fracs.get(i), \n",
    "#                                                random_state = seed) for i,dff in inp_df.groupby('label')])\n",
    "# #         inp_sampled_df = inp_df.sample(frac=0.15)\n",
    "    \n",
    "#         inp_df_select = inp_sampled_df[['id_left', 'id_right', 'label']]\n",
    "#         inf_df_group_sizes = inp_df_select.groupby(groupby_q).size()\n",
    "#         glue_test_leftids_to_use = list(\n",
    "#             inf_df_group_sizes[inf_df_group_sizes >= min_group_size].index)\n",
    "#         groups = inp_df_select[inp_df_select.id_left.isin(\n",
    "#             glue_test_leftids_to_use)].groupby(groupby_q)\n",
    "\n",
    "#         next_id = list(set(inp_df_select.columns.values) - set({groupby_q, 'label'}))[0] \n",
    "\n",
    "#         out_pairs = []\n",
    "\n",
    "#         for idx, group in groups:\n",
    "#             ids = group[next_id].values\n",
    "#             perm = list(permutations(ids, 2))\n",
    "#             triples_lst = []\n",
    "#             if len(perm) > 1:      \n",
    "#                 for pair in perm:\n",
    "#                     label_0, label_1 = group[group[next_id].isin(pair)].label.values\n",
    "# #                     diff    = ((label_0 >= label_1)&(label_1 != 0)).astype(int)\n",
    "#                     if label_0 == 0:\n",
    "#                         triples_lst.append(list([idx, pair[0], pair[1], 0]))\n",
    "#                     else:\n",
    "#                         triples_lst.append(list([idx, pair[0], pair[1], 1]))\n",
    "                        \n",
    "#                 triples_df = pd.DataFrame(triples_lst, columns=[groupby_q, next_id+'0', next_id+'1', 'label'])\n",
    "                \n",
    "#                 if len(perm) >= perm_min_samples:\n",
    "#                     perm_fracs = {0: perm_frac_neg, 1: perm_frac_pos}\n",
    "#                     sampled_df = pd.concat([dff.sample(frac=perm_fracs.get(i), \n",
    "#                                                        random_state = seed) for i,dff in triples_df.groupby('label')])\n",
    "#                     out_pairs.extend(sampled_df.values.tolist()) \n",
    "#                 else:\n",
    "#                     out_pairs.extend(triples_df.values.tolist())\n",
    "            \n",
    "#         return out_pairs    \n",
    "\n",
    "    \n",
    "    def get_idx_to_text_mapping(self, inp_df: pd.DataFrame) -> Dict[str, str]:\n",
    "        left_dict = (\n",
    "            inp_df\n",
    "            [['id_left', 'text_left']]\n",
    "            .drop_duplicates()\n",
    "            .set_index('id_left')\n",
    "            ['text_left']\n",
    "            .to_dict()\n",
    "        )\n",
    "        right_dict = (\n",
    "            inp_df\n",
    "            [['id_right', 'text_right']]\n",
    "            .drop_duplicates()\n",
    "            .set_index('id_right')\n",
    "            ['text_right']\n",
    "            .to_dict()\n",
    "        )\n",
    "        left_dict.update(right_dict)\n",
    "        return left_dict\n",
    "     \n",
    "\n",
    "    def ndcg_k(self, ys_true: np.array, ys_pred: np.array, ndcg_top_k: int = 10) -> float:\n",
    "      \n",
    "        discounted_dsg = dcg_k(torch.Tensor(ys_true), torch.Tensor(ys_pred), ndcg_top_k)\n",
    "        ideal_dcg = dcg_k(torch.Tensor(ys_true), torch.Tensor(ys_true), ndcg_top_k)\n",
    "        \n",
    "        if ideal_dcg != 0:\n",
    "            ndcg = discounted_dsg / ideal_dcg\n",
    "        else:\n",
    "            ndcg = 0\n",
    "        return ndcg\n",
    "        \n",
    "        return current_dcg / ideal_dcg\n",
    "        \n",
    "    def compute_gain_diff(self, y_true, gain_scheme):\n",
    "        if gain_scheme == \"exp2\":\n",
    "            gain_diff = torch.pow(2.0, y_true) - torch.pow(2.0, y_true.t())\n",
    "        elif gain_scheme == \"diff\":\n",
    "            gain_diff = y_true - y_true.t()\n",
    "        else:\n",
    "            raise ValueError(f\"{gain_scheme} method not supported\")\n",
    "        return gain_diff \n",
    "\n",
    "\n",
    "    def valid(self, model: torch.nn.Module, val_dataloader: torch.utils.data.DataLoader) -> float:\n",
    "        labels_and_groups = val_dataloader.dataset.index_pairs_or_triplets\n",
    "        labels_and_groups = pd.DataFrame(labels_and_groups, columns=['left_id', 'right_id', 'rel'])\n",
    "        \n",
    "        all_preds = []\n",
    "        for batch in (val_dataloader):\n",
    "            inp_1, y = batch\n",
    "            preds = model.predict(inp_1)\n",
    "            preds_np = preds.detach().numpy()\n",
    "            all_preds.append(preds_np)\n",
    "        all_preds = np.concatenate(all_preds, axis=0)\n",
    "        labels_and_groups['preds'] = all_preds\n",
    "        \n",
    "        ndcgs = []\n",
    "        for cur_id in labels_and_groups.left_id.unique():\n",
    "            cur_df = labels_and_groups[labels_and_groups.left_id == cur_id]\n",
    "            ndcg = self.ndcg_k(cur_df.rel.values.reshape(-1), cur_df.preds.values.reshape(-1))\n",
    "            if np.isnan(ndcg):\n",
    "                ndcgs.append(0)\n",
    "            else:\n",
    "                ndcgs.append(ndcg)\n",
    "        return np.mean(ndcgs)\n",
    "\n",
    "    \n",
    "    def train(self, n_epochs: int):\n",
    "        opt = torch.optim.SGD(self.model.parameters(), lr=self.train_lr)\n",
    "        criterion = torch.nn.BCELoss()\n",
    "        ndcg = 0\n",
    "        for epoch in range(n_epochs):\n",
    "            epoch_loss = 0\n",
    "            if epoch % self.change_train_loader_ep == 0:\n",
    "                current_subset = self.sample_data_for_train_iter(inp_df = self.glue_train_df, \n",
    "                                                                 seed = epoch)\n",
    "                train_dataset = TrainTripletsDataset(current_subset,\n",
    "                                                     self.idx_to_text_mapping_train, \n",
    "                                                     vocab=self.vocab, \n",
    "                                                     oov_val=self.vocab['OOV'],\n",
    "                                                     preproc_func=self.simple_preproc)\n",
    "                train_dataloader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                                               batch_size=self.dataloader_bs,\n",
    "                                                               num_workers=0, \n",
    "                                                               collate_fn=collate_fn, \n",
    "                                                               shuffle=True)\n",
    "            for batch in train_dataloader:\n",
    "                inp_1, inp_2, y = batch\n",
    "                preds = self.model(inp_1, inp_2)\n",
    "                batch_loss = criterion(preds, y)\n",
    "                batch_loss.backward()\n",
    "                opt.step()\n",
    "                epoch_loss += batch_loss.item()\n",
    "            if epoch > 5:\n",
    "                ndcg = self.valid(self.model, self.val_dataloader)\n",
    "                print(\"epoch: {} ndcg: {}\".format(epoch, ndcg))\n",
    "            if ndcg > 0.950:\n",
    "                break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 44s, sys: 1.89 s, total: 1min 46s\n",
      "Wall time: 1min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sol = Solution(glue_qqp_dir = glue_qqp_dir, \n",
    "               glove_vectors_path = glove_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 ndcg: 0.5341467504074768\n",
      "epoch: 7 ndcg: 0.5477597276543643\n",
      "epoch: 8 ndcg: 0.7101372570389655\n",
      "epoch: 9 ndcg: 0.7892089136771846\n",
      "epoch: 10 ndcg: 0.9100668548371349\n",
      "epoch: 11 ndcg: 0.8955673357311789\n",
      "epoch: 12 ndcg: 0.8919433338673058\n",
      "epoch: 13 ndcg: 0.9278066723314595\n",
      "epoch: 14 ndcg: 0.9040322747393125\n",
      "epoch: 15 ndcg: 0.8838207987188396\n",
      "epoch: 16 ndcg: 0.8476004573701311\n",
      "epoch: 17 ndcg: 0.8415805832838761\n",
      "epoch: 18 ndcg: 0.8942976559708666\n",
      "epoch: 19 ndcg: 0.8829057468894735\n",
      "epoch: 20 ndcg: 0.9413478188337606\n",
      "epoch: 21 ndcg: 0.9386434862613925\n",
      "epoch: 22 ndcg: 0.9261453526861442\n",
      "epoch: 23 ndcg: 0.9135855712921562\n",
      "epoch: 24 ndcg: 0.05069075787896105\n",
      "epoch: 25 ndcg: 0.9029567457832243\n",
      "epoch: 26 ndcg: 0.040637576983257796\n",
      "epoch: 27 ndcg: 0.9422096206844152\n",
      "epoch: 28 ndcg: 0.6588493255564072\n",
      "epoch: 29 ndcg: 0.5217631565948775\n",
      "epoch: 30 ndcg: 0.8144786334990184\n",
      "epoch: 31 ndcg: 0.9052019563596706\n",
      "epoch: 32 ndcg: 0.9815772987206142\n",
      "CPU times: user 12min 33s, sys: 50.8 s, total: 13min 24s\n",
      "Wall time: 9min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sol.train(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_layer = sol.model.mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('1.weight',\n",
       "              tensor([[-1.3675e-03,  9.7939e-02, -1.5025e-01, -1.3421e-01, -6.9578e-02,\n",
       "                        5.1119e-02,  1.1802e-03,  1.5375e-01, -2.2322e-03,  6.3431e-02,\n",
       "                       -5.8350e-02, -9.0235e-02, -2.9208e-01, -2.8554e-01, -2.5503e-01,\n",
       "                       -1.5083e-01, -3.1750e-02,  6.9409e-02, -1.1293e-01, -4.6203e-02,\n",
       "                        1.0435e-01,  1.9258e-01,  7.3940e-03,  1.8652e-01,  2.4456e-02,\n",
       "                        7.1679e-02,  2.0797e-01, -1.4159e-01, -1.0320e-01, -5.9219e-02],\n",
       "                      [-7.1165e-02,  1.5776e-01, -1.1824e-01, -8.3621e-02, -1.2641e-01,\n",
       "                       -1.6893e-01, -1.0427e-01,  1.5859e-01,  8.2196e-02,  8.7204e-02,\n",
       "                       -6.6780e-04, -1.2414e-01, -1.4524e-02, -2.1770e-01, -1.7867e-01,\n",
       "                       -1.4649e-01,  5.4514e-02,  4.6886e-02, -1.3491e-01, -5.8005e-02,\n",
       "                        6.6114e-02,  1.3351e-01,  2.9996e-02, -1.1296e-02,  9.0604e-02,\n",
       "                       -1.3708e-01,  4.3652e-03, -1.7805e-01, -1.7186e-01, -1.4823e-01],\n",
       "                      [ 8.2604e-02,  7.3395e-02, -1.0821e-01,  5.5100e-02,  1.0009e-01,\n",
       "                       -2.3999e-02,  4.6066e-03,  4.0354e-02,  1.1432e-01,  1.7696e-01,\n",
       "                       -1.6353e-01, -2.2192e-01, -2.7808e-01, -3.2674e-01, -3.3824e-01,\n",
       "                       -2.3605e-01, -1.5469e-01, -1.4401e-01,  5.0416e-02, -1.6194e-01,\n",
       "                       -2.8050e-01,  1.9265e-02,  4.4113e-04, -2.8976e-01, -3.5038e-02,\n",
       "                       -6.9591e-02, -4.3255e-02, -5.7062e-02,  1.3592e-01, -2.2065e-02],\n",
       "                      [ 6.6930e-02,  9.2329e-02,  1.3070e-01,  6.8260e-02, -1.8075e-01,\n",
       "                       -1.1860e-01,  9.0826e-02,  3.7695e-02, -1.4307e-01, -1.0585e-01,\n",
       "                        1.7083e-01,  1.2145e-01, -8.2212e-02, -4.9681e-02, -1.7823e-01,\n",
       "                       -6.7770e-03, -1.3829e-01, -1.3738e-01, -1.2244e-03,  4.1863e-02,\n",
       "                       -5.5355e-02,  1.3148e-01, -8.5543e-02,  1.9280e-01,  1.5340e-01,\n",
       "                       -1.2504e-01, -1.6965e-02,  3.4709e-02,  4.9655e-02,  5.6266e-02],\n",
       "                      [ 7.1647e-02,  1.0932e-02, -8.9110e-02,  8.6264e-02, -1.7557e-01,\n",
       "                       -1.0922e-01, -4.7558e-02, -9.2617e-02, -7.2234e-02, -1.7059e-01,\n",
       "                       -9.9124e-02, -9.9455e-02, -3.4611e-01, -3.0741e-01, -2.0201e-01,\n",
       "                       -3.4600e-01, -2.8761e-01, -2.6292e-01, -1.7082e-01, -8.9623e-02,\n",
       "                       -5.4648e-02, -1.3252e-01, -8.8630e-02, -2.7965e-01, -4.2348e-01,\n",
       "                       -3.2679e-01, -8.5102e-02, -1.6841e-01, -2.0706e-01, -1.4625e-01],\n",
       "                      [-1.7086e-01, -1.5409e-01, -3.7032e-02,  1.0014e-01,  9.8780e-02,\n",
       "                       -1.7585e-01,  1.1435e-01, -1.4206e-01, -3.7236e-02, -7.2077e-02,\n",
       "                       -3.2984e-02, -3.4144e-02, -1.6417e-01, -1.6283e-01, -4.1093e-02,\n",
       "                       -1.7263e-02, -1.0737e-01,  3.9077e-02, -2.0353e-01, -6.2401e-02,\n",
       "                        1.0254e-01, -1.3533e-01,  1.0576e-01,  1.1095e-02, -2.1528e-01,\n",
       "                        6.8664e-02, -6.5254e-02, -1.2256e-01,  1.1047e-01, -1.6846e-01],\n",
       "                      [ 1.9591e-02, -3.8225e-02,  1.3038e-01,  5.0965e-02,  8.7728e-02,\n",
       "                        6.4478e-02, -4.3904e-02, -3.8396e-02, -1.5046e-01,  9.8927e-02,\n",
       "                        1.4496e-01,  1.2492e-01, -1.2878e-01,  8.1428e-03, -1.2870e-01,\n",
       "                       -1.0050e-01, -1.0639e-01,  6.2394e-02, -1.0880e-01, -3.9833e-03,\n",
       "                        7.6806e-03,  1.1769e-01, -1.3801e-01, -1.2534e-01, -1.0601e-01,\n",
       "                        1.2779e-01, -6.5629e-02,  1.5400e-01,  6.6020e-02,  2.3119e-02],\n",
       "                      [-1.3594e-03, -3.6095e-02,  2.2890e-02, -4.1759e-02, -1.4623e-03,\n",
       "                        2.3033e-02, -1.4299e-01, -9.5878e-02,  1.4635e-01, -1.5288e-01,\n",
       "                       -3.1298e-02,  1.3267e-01, -1.8734e-02, -1.1000e-01, -2.8768e-01,\n",
       "                       -3.4880e-02, -2.3927e-01, -1.4459e-01, -1.4525e-01, -1.1764e-01,\n",
       "                       -1.0283e-01,  3.2933e-02, -1.0376e-01,  3.3063e-02, -1.6873e-02,\n",
       "                       -2.3618e-01, -2.5405e-02, -2.1722e-01, -1.4215e-01,  4.2089e-02],\n",
       "                      [ 6.0277e-02,  1.3692e-01, -5.8982e-02, -2.8390e-04,  9.2906e-02,\n",
       "                       -1.7780e-01,  1.3213e-01, -1.4745e-01,  8.1407e-03, -4.2672e-02,\n",
       "                       -2.1562e-01, -3.4554e-01, -4.8649e-01, -8.6552e-01, -9.6219e-01,\n",
       "                       -8.1342e-01, -6.7843e-01, -1.0104e-01, -3.1493e-02,  3.1999e-02,\n",
       "                        7.3683e-02,  2.9937e-01,  3.0466e-01,  3.5825e-01,  2.9002e-01,\n",
       "                        1.5873e-01,  1.5875e-01,  2.6554e-01,  3.6666e-01,  2.1945e-01],\n",
       "                      [-8.6380e-02, -8.5798e-02, -1.7250e-01,  3.9800e-02, -1.0152e-01,\n",
       "                       -1.6058e-01,  1.6438e-01, -1.1118e-01, -8.5528e-03,  7.0220e-02,\n",
       "                        2.7261e-03, -2.4006e-01, -4.1537e-01, -2.2809e-01, -3.8236e-01,\n",
       "                       -2.2648e-01, -3.2855e-01, -3.3421e-02,  2.6588e-02,  9.3210e-02,\n",
       "                       -2.8708e-02,  1.2056e-01,  1.0061e-01,  3.0794e-02, -1.3783e-02,\n",
       "                       -1.0193e-01,  9.0619e-02, -2.6287e-01, -1.5016e-01, -3.7858e-01],\n",
       "                      [-8.8122e-02,  3.2620e-02, -9.5199e-02,  4.2624e-02,  4.0663e-02,\n",
       "                       -1.2320e-01,  5.2334e-02,  1.1287e-01,  1.0285e-01, -9.5329e-02,\n",
       "                       -4.9825e-01, -4.7958e-01, -2.1872e-01, -1.7265e-01,  6.7351e-02,\n",
       "                        1.5642e-01, -8.5995e-02, -6.8808e-02,  8.8609e-02,  3.1575e-01,\n",
       "                        4.5748e-01,  4.3708e-01,  1.1081e-01, -8.0735e-05, -1.9677e-01,\n",
       "                       -4.1278e-01, -4.5663e-01, -5.9714e-02,  6.5176e-01,  1.4373e+00],\n",
       "                      [-1.5128e-01, -1.5692e-01,  1.8232e-01,  1.1578e-01, -1.2778e-01,\n",
       "                        6.7202e-02,  1.3034e-01,  1.7183e-01,  1.5857e-01,  1.7566e-01,\n",
       "                       -8.6481e-02, -6.2443e-01, -1.0946e+00, -1.6220e+00, -2.0095e+00,\n",
       "                       -1.5893e+00, -1.2075e+00, -5.2892e-01, -2.8107e-01, -1.1710e-01,\n",
       "                       -3.1375e-01, -7.9870e-02, -1.1366e-01,  2.1085e-01,  4.0253e-01,\n",
       "                        6.0302e-01,  4.0091e-01,  1.6465e-01, -5.5519e-01, -1.1200e+00],\n",
       "                      [-1.2353e-01, -1.8010e-01, -1.4657e-01,  1.4427e-01,  9.8978e-02,\n",
       "                        1.7147e-01,  1.4605e-01, -1.6453e-01, -1.2934e-01, -4.3821e-02,\n",
       "                       -1.6474e-01,  9.8183e-03, -3.3439e-01, -3.4149e-01, -4.5829e-01,\n",
       "                       -3.9782e-01, -1.4843e-01, -1.5184e-01, -1.8306e-01, -1.9997e-01,\n",
       "                       -2.0765e-01, -4.3641e-01, -3.1664e-01, -2.3196e-01, -3.2390e-01,\n",
       "                       -1.7962e-01, -2.3554e-01, -1.8506e-01, -3.2552e-01, -6.3501e-02],\n",
       "                      [ 2.0886e-02, -4.4031e-02, -1.0359e-01, -9.9882e-02, -1.2468e-01,\n",
       "                        1.6262e-01,  2.0215e-01,  9.5709e-02,  9.5969e-03,  1.4758e-02,\n",
       "                       -6.1936e-01, -8.9491e-01, -5.6557e-01,  1.1666e-01,  2.8275e-01,\n",
       "                        1.6091e-01,  3.4097e-01, -2.3372e-01, -9.5643e-01, -5.5329e-01,\n",
       "                       -3.9387e-01, -2.1681e-02,  3.5979e-01,  1.8371e-01, -2.1429e-01,\n",
       "                       -5.1605e-01, -4.3605e-01,  6.9819e-01,  2.2366e+00,  4.3423e+00],\n",
       "                      [ 9.7751e-03, -1.1273e-01,  9.3577e-03,  8.7542e-02,  9.0564e-02,\n",
       "                       -1.6686e-01, -3.2670e-02, -1.3568e-01, -7.7902e-02,  6.5780e-02,\n",
       "                       -1.2965e-01,  6.7868e-02,  1.5496e-01,  1.1975e-02, -1.2168e-01,\n",
       "                       -6.5414e-02,  3.9868e-02, -1.3918e-01,  9.0705e-02, -1.6575e-01,\n",
       "                       -1.7551e-01, -1.7740e-01, -3.7038e-02,  1.2277e-01, -1.7280e-01,\n",
       "                        1.5175e-01, -7.3034e-02,  5.3473e-02,  8.3259e-03, -1.6463e-01]])),\n",
       "             ('1.bias',\n",
       "              tensor([ 0.1509,  0.0972,  0.1810,  0.0926, -0.1266,  0.1516,  0.0098,  0.0840,\n",
       "                      -0.1500, -0.0580, -0.1605, -0.0934,  0.0323, -0.0961,  0.0582])),\n",
       "             ('3.weight',\n",
       "              tensor([[ 1.2983e-01,  2.8194e-02, -1.7276e-01, -2.0261e-01, -8.2955e-02,\n",
       "                        1.0756e-01,  2.5469e-01,  1.4115e-01, -4.3655e-02,  9.0529e-02,\n",
       "                       -2.7386e-01, -2.2773e-01,  7.7246e-02, -1.4650e-01,  1.2422e-01],\n",
       "                      [-1.8158e-01, -1.1071e-01, -2.3807e-01,  1.3579e-01, -7.6854e-02,\n",
       "                        2.0293e-01,  1.5979e-01,  1.2312e-01,  8.7962e-03, -8.0546e-02,\n",
       "                       -1.2212e-01, -4.3566e-02,  4.9093e-02, -2.0212e-01,  2.1670e-01],\n",
       "                      [-3.9176e-02, -2.2999e-01, -5.4564e-01,  2.3093e-01, -1.5409e-01,\n",
       "                       -2.3316e-01,  1.8211e-01,  3.4601e-02,  1.3969e-01,  1.5146e-01,\n",
       "                       -6.9335e-01, -6.2792e-01, -6.5398e-01, -3.7268e-01,  8.6407e-02],\n",
       "                      [-9.0029e-02,  3.1797e-02, -4.1439e-01, -9.6850e-02,  1.2167e-01,\n",
       "                        8.7897e-02, -1.2615e-01,  4.1838e-02,  1.2043e-01, -2.2517e-02,\n",
       "                       -6.5903e-01, -7.7861e-02, -7.5666e-02, -1.9414e-01, -1.5183e-01],\n",
       "                      [-1.3106e-01,  2.3729e-01, -6.9477e-02, -7.0497e-04, -1.2510e-01,\n",
       "                        2.5776e-01,  2.5218e-01, -1.9473e-01, -2.0931e-01, -1.9571e-01,\n",
       "                       -1.2452e-03, -6.5816e-02, -1.6900e-01, -9.2609e-02,  4.8782e-02],\n",
       "                      [-1.8553e-01, -1.1956e-01, -1.1824e-01, -1.2748e-01,  8.4171e-02,\n",
       "                        2.0117e-01, -1.7456e-01,  2.2387e-02, -1.5295e-01,  2.5337e-01,\n",
       "                       -2.7256e-01, -2.5968e-01, -3.0421e-02, -1.3355e-01,  4.9503e-02],\n",
       "                      [ 4.6458e-01,  6.6984e-01, -1.0659e+00,  1.5858e-01, -5.0298e-01,\n",
       "                       -2.4947e-01, -1.5243e-01, -1.1199e-01,  3.1045e-01, -2.8822e-03,\n",
       "                       -5.8273e-01,  1.0396e+00,  2.5883e-01, -1.8503e+00,  1.2977e-01]])),\n",
       "             ('3.bias',\n",
       "              tensor([ 0.2301,  0.1422, -0.1062, -0.1817, -0.0192,  0.2151,  0.0035])),\n",
       "             ('5.weight',\n",
       "              tensor([[-0.3562,  0.1242,  0.1234,  0.1191, -0.3130, -0.1516, -1.1867]])),\n",
       "             ('5.bias', tensor([-0.0286]))])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_layer.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.save(mlp_layer,open(\"./pkl/mlp_pretr.tor\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_emb_krnm = sol.model.embeddings.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                      [-0.0022, -0.0369,  0.0055,  ..., -0.1204, -0.0544, -0.0585],\n",
       "                      [ 0.4532,  0.0598, -0.1058,  ...,  0.5324, -0.2510,  0.6255],\n",
       "                      ...,\n",
       "                      [-0.0022, -0.0369,  0.0055,  ..., -0.1204, -0.0544, -0.0585],\n",
       "                      [-0.5138, -0.9359, -0.1946,  ...,  0.2532,  0.1363, -0.0305],\n",
       "                      [-0.3993, -0.5018, -0.8642,  ...,  0.4815, -0.2657, -0.2634]]))])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_emb_krnm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pretrained_emb_krnm, open(\"./pkl/pretr_emb_krnm_50.tor\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./pkl/pretr_emb_krnm_50\", \"wb\") as f:\n",
    "    pickle.dump(pretrained_emb_krnm,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./pkl/pretr_emb_krnm_50\", \"rb\") as fin:\n",
    "#     emb_50 = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_matrix, vocab, unk_words = sol.create_glove_emb_from_file(\n",
    "            sol.glove_vectors_path, sol.all_tokens, sol.random_seed, sol.emb_rand_uni_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./pkl/vocab\", \"wb\") as f:\n",
    "    pickle.dump(vocab,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'query': [19, 115, 51, 7915, 142, 171, 6597, 44, 136],\n",
       "  'document': [137, 812, 587, 43, 17, 2215, 19864, 8065, 595, 19889]},\n",
       " 0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sol.val_dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'query': [19, 115, 51, 7915, 142, 171, 6597, 44, 136], 'document': [19, 115, 51, 11926, 301, 142, 171, 6597, 44, 136]}, 2)\n"
     ]
    }
   ],
   "source": [
    "for item in sol.val_dataset:\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in sol.val_dataloader:\n",
    "    res, label = item\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 30])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['document'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "knrm = sol.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 24, 30])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmat = knrm._get_matching_matrix(res['query'], res['document'])\n",
    "mmat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 22])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KM = []\n",
    "for kernel in sol.model.kernels:\n",
    "    # shape = [B]\n",
    "    K = torch.log1p(kernel(mmat).sum(dim=-1)).sum(dim=-1)\n",
    "    KM.append(K)\n",
    "\n",
    "# shape = [B, K]\n",
    "kernels_out = torch.stack(KM, dim=1)\n",
    "kernels_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): ReLU()\n",
       "  (1): Linear(in_features=22, out_features=10, bias=True)\n",
       "  (2): ReLU()\n",
       "  (3): Linear(in_features=10, out_features=5, bias=True)\n",
       "  (4): ReLU()\n",
       "  (5): Linear(in_features=5, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sol.model._get_mlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_df = sol.glue_train_df\n",
    "min_group_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_left</th>\n",
       "      <th>id_right</th>\n",
       "      <th>text_left</th>\n",
       "      <th>text_right</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>213221</td>\n",
       "      <td>213222</td>\n",
       "      <td>How is the life of a math student? Could you d...</td>\n",
       "      <td>Which level of prepration is enough for the ex...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>536040</td>\n",
       "      <td>536041</td>\n",
       "      <td>How do I control my horny emotions?</td>\n",
       "      <td>How do you control your horniness?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>364011</td>\n",
       "      <td>490273</td>\n",
       "      <td>What causes stool color to change to yellow?</td>\n",
       "      <td>What can cause stool to come out as little balls?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>155721</td>\n",
       "      <td>7256</td>\n",
       "      <td>What can one do after MBBS?</td>\n",
       "      <td>What do i do after my MBBS ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>279958</td>\n",
       "      <td>279959</td>\n",
       "      <td>Where can I find a power outlet for my laptop ...</td>\n",
       "      <td>Would a second airport in Sydney, Australia be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id_left id_right                                          text_left  \\\n",
       "0  213221   213222  How is the life of a math student? Could you d...   \n",
       "1  536040   536041                How do I control my horny emotions?   \n",
       "2  364011   490273       What causes stool color to change to yellow?   \n",
       "3  155721     7256                        What can one do after MBBS?   \n",
       "4  279958   279959  Where can I find a power outlet for my laptop ...   \n",
       "\n",
       "                                          text_right  label  \n",
       "0  Which level of prepration is enough for the ex...      0  \n",
       "1                 How do you control your horniness?      1  \n",
       "2  What can cause stool to come out as little balls?      0  \n",
       "3                       What do i do after my MBBS ?      1  \n",
       "4  Would a second airport in Sydney, Australia be...      0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(363846, 5)"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_q = 'id_right'\n",
    "\n",
    "inp_df_select = inp_df[['id_left', 'id_right', 'label']]\n",
    "inf_df_group_sizes = inp_df_select.groupby(groupby_q).size()\n",
    "glue_test_leftids_to_use = list(\n",
    "    inf_df_group_sizes[inf_df_group_sizes >= min_group_size].index)\n",
    "groups = inp_df_select[inp_df_select.id_left.isin(\n",
    "    glue_test_leftids_to_use)].groupby(groupby_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    229468\n",
       "1    134378\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8275/8275 [00:52<00:00, 157.49it/s]\n"
     ]
    }
   ],
   "source": [
    "groupby_q = 'id_left'\n",
    "perm_min_samples = 10\n",
    "\n",
    "inp_df_select = inp_df[['id_left', 'id_right', 'label']]\n",
    "inf_df_group_sizes = inp_df_select.groupby(groupby_q).size()\n",
    "glue_test_leftids_to_use = list(\n",
    "    inf_df_group_sizes[inf_df_group_sizes >= min_group_size].index)\n",
    "groups = inp_df_select[inp_df_select.id_left.isin(\n",
    "    glue_test_leftids_to_use)].groupby(groupby_q)\n",
    "\n",
    "next_id = list(set(inp_df_select.columns.values) - set({groupby_q, 'label'}))[0] \n",
    "\n",
    "out_pairs = []\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "for idx, group in tqdm(groups, total = len(groups)):\n",
    "    id_gr = set({str(idx)})\n",
    "    ids = group[next_id].values\n",
    "    perm = list(permutations(ids, 2))\n",
    "    if len(perm) > 0:\n",
    "        triples_lst = \\\n",
    "        [list(id_gr.union({id0, id1}))+[(group[group[next_id]==id0].label.values[0] >= \n",
    "                                         group[group[next_id]==id1].label.values[0]\n",
    "                                        ).astype(int)\n",
    "                                       ]\n",
    "\n",
    "         for id0, id1 in perm\n",
    "        ]\n",
    "\n",
    "        triples_df = pd.DataFrame(triples_lst, columns=[groupby_q, next_id+'0', next_id+'1', 'label'])\n",
    "\n",
    "        if len(perm) >= perm_min_samples:\n",
    "            fracs = {0: 0.1, 1: 0.5}\n",
    "        else:\n",
    "            fracs = {0: 1., 1: 1.}\n",
    "        sampled_df = pd.concat([dff.sample(frac=fracs.get(i), random_state = 0) for i,dff in triples_df.groupby('label')])\n",
    "\n",
    "        out_pairs.extend(sampled_df.values.tolist())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_left</th>\n",
       "      <th>id_right</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74599</th>\n",
       "      <td>10061</td>\n",
       "      <td>296775</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161359</th>\n",
       "      <td>10061</td>\n",
       "      <td>14160</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213407</th>\n",
       "      <td>10061</td>\n",
       "      <td>10062</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227129</th>\n",
       "      <td>10061</td>\n",
       "      <td>38068</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240820</th>\n",
       "      <td>10061</td>\n",
       "      <td>38069</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275839</th>\n",
       "      <td>10061</td>\n",
       "      <td>135394</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283645</th>\n",
       "      <td>10061</td>\n",
       "      <td>280645</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287616</th>\n",
       "      <td>10061</td>\n",
       "      <td>76075</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312024</th>\n",
       "      <td>10061</td>\n",
       "      <td>105493</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317904</th>\n",
       "      <td>10061</td>\n",
       "      <td>72773</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330907</th>\n",
       "      <td>10061</td>\n",
       "      <td>167403</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id_left id_right  label\n",
       "74599    10061   296775      1\n",
       "161359   10061    14160      1\n",
       "213407   10061    10062      1\n",
       "227129   10061    38068      1\n",
       "240820   10061    38069      1\n",
       "275839   10061   135394      1\n",
       "283645   10061   280645      0\n",
       "287616   10061    76075      1\n",
       "312024   10061   105493      1\n",
       "317904   10061    72773      1\n",
       "330907   10061   167403      1"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups.get_group('10061')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups.get_group('100021')['label'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'100001': [72169, 281245], '10001': [109028, 222911], '100014': [166599, 255056], '100016': [47531, 227668], '100017': [312234, 333088], '100021': [4489, 51524, 117562, 169432, 192293, 211110], '100022': [71771, 134299, 325283, 359495], '100024': [77008, 131059, 276451], '100028': [102332, 158540, 205807, 264483, 287050, 324542], '100041': [56880, 105325, 166916], '100048': [157076, 191857, 193508], '100068': [46372, 95341, 247219, 341451], '10008': [162671, 344368], '100084': [104374, 162216, 290276], '10009': [17486, 124792, 328372, 337960], '100091': [26535, 108070], '10010': [32927, 346526], '100104': [227867, 326091], '10011': [17702, 80430, 90966, 91742, 284918], '10013': [86060, 126705, 325087], '100134': [18002, 347315], '100136': [44476, 111770, 318911, 359155], '100149': [127622, 203513], '100153': [243359, 300529, 357390], '100154': [47938, 110460], '100160': [79695, 85570, 120549], '100162': [136875, 256736], '10017': [19970, 47004, 51892], '100172': [129669, 330482, 330484], '100176': [85056, 161751, 195958, 256124, 318302], '100177': [68231, 97784], '100180': [166483, 307237], '100197': [10445, 144010], '1002': [114349, 126293, 248436, 290150], '100200': [231691, 279167], '100212': [30342, 86644, 189506, 256024, 266889, 272184, 273564, 300586, 307444, 323113, 338195, 356934, 361386], '100217': [170162, 178750], '100238': [45596, 74232, 81809, 136643, 182570, 187037, 188618, 221213, 269457, 277343, 292193, 319245, 323370, 329551], '10024': [18623, 36292, 54955, 113459, 117166, 139644, 198999, 261942, 288171, 291969, 311485, 356474, 359690, 360646], '100241': [88985, 299874, 345967], '100259': [11446, 305927], '100260': [226869, 233704, 292896], '100268': [108270, 162848, 339842], '100271': [48863, 290930, 349465], '100278': [276599, 287763], '100283': [68153, 243871], '100294': [180127, 251346, 287597], '100296': [44984, 183878, 298723], '100299': [51115, 197213, 275635], '100321': [4437, 75884, 157476, 212247, 219635, 233784, 267139], '100325': [28959, 181060, 210259], '10033': [155164, 245372, 266930, 308870, 313915, 325030], '100337': [64504, 125364, 148742, 152340, 281728], '100343': [48462, 93363], '100344': [235894, 319741], '100345': [12681, 39092, 53735, 144883, 176961, 217425, 238429], '100346': [12792, 98223], '100349': [92024, 177596, 270170], '10035': [228073, 345119], '100359': [76753, 240933], '100361': [179784, 245310], '100363': [60849, 230852, 293009], '10037': [111611, 164211, 236199], '100370': [74355, 92832, 104238, 225855, 230588, 232500, 261001, 341658], '100377': [47824, 252949, 305029, 354276], '100383': [218641, 339113], '100393': [135633, 240614], '100401': [60321, 68024, 99416], '100406': [40055, 103584], '10041': [4495, 32863, 44318, 186027, 222852, 309310, 330563], '100415': [36491, 268380], '100416': [28954, 89585, 126678], '10043': [84539, 85267, 136686, 149542, 212951, 252942, 333828, 339158], '100449': [105111, 130074, 145839, 282929, 320466], '100450': [114064, 203625, 280504], '100475': [105933, 290486], '10048': [29093, 32725, 120332, 214636, 321247], '100484': [80962, 355242], '100486': [53586, 134957, 194845, 229731], '100509': [135341, 183924, 203461], '100513': [39316, 336651], '100518': [42961, 48469], '10052': [12327, 15998, 42003, 161047, 167372, 233662], '10053': [103676, 122626, 255054], '100530': [171818, 173460], '100533': [103234, 121573, 186617, 251005], '100536': [143914, 246501], '100540': [12037, 87301], '100546': [38415, 54981], '100549': [94225, 214161], '10055': [96953, 142498], '100551': [1597, 46893, 117439, 310765, 348384, 360954], '100552': [72225, 286957], '100558': [100678, 155749, 185958], '100567': [143915, 281142], '100572': [56923, 65764, 83930, 141700, 192530, 269601, 311595, 337528], '100581': [52277, 57735, 77501, 101308, 120738], '100585': [63746, 168749, 204248], '1006': [106882, 133202], '10061': [74599, 161359, 213407, 227129, 240820, 275839, 283645, 287616, 312024, 317904, 330907], ...}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups.groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr = groups.get_group('10061')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_left</th>\n",
       "      <th>id_right</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74599</th>\n",
       "      <td>10061</td>\n",
       "      <td>296775</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161359</th>\n",
       "      <td>10061</td>\n",
       "      <td>14160</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213407</th>\n",
       "      <td>10061</td>\n",
       "      <td>10062</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227129</th>\n",
       "      <td>10061</td>\n",
       "      <td>38068</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240820</th>\n",
       "      <td>10061</td>\n",
       "      <td>38069</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id_left id_right  label\n",
       "74599    10061   296775      1\n",
       "161359   10061    14160      1\n",
       "213407   10061    10062      1\n",
       "227129   10061    38068      1\n",
       "240820   10061    38069      1"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['100141', '75743', 2],\n",
       " ['100141', '100142', 2],\n",
       " ['100141', '264410', 0],\n",
       " ['100141', '275077', 0],\n",
       " ['100141', '367828', 0]]"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sol.dev_pairs_for_ndcg[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_tok = torch.Tensor([[22, 45, 11, 15, 0, 0],\n",
    "                          [22, 45, 11, 15, 0, 0]]).type(torch.int)\n",
    "doc_tok = torch.Tensor([[43, 21, 9, 15, 0, 0],\n",
    "                        [43, 21, 9, 15, 0, 0]]).type(torch.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "BAT, A, B = query_tok.shape[0], query_tok.shape[1], doc_tok.shape[1]\n",
    "assert doc_tok.shape[0] == BAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_padding(sim, query_tok, doc_tok, BAT, A, B):\n",
    "    nul = torch.zeros_like(sim)\n",
    "    sim = torch.where(query_tok.reshape(BAT, A, 1).expand(BAT, A, B) == self.padding, nul, sim)\n",
    "    sim = torch.where(doc_tok.reshape(BAT, 1, B).expand(BAT, A, B) == self.padding, nul, sim)\n",
    "    return sim\n",
    "\n",
    "def exact_match_matrix(query_tok, doc_tok, BAT, A, B):\n",
    "    sim = (query_tok.reshape(BAT, A, 1).expand(BAT, A, B) == doc_tok.reshape(BAT, 1, B).expand(BAT, A, B)).float()\n",
    "    sim = remove_padding(sim, query_tok, doc_tok, BAT, A, B)\n",
    "    return sim\n",
    "\n",
    "def cosine_similarity_matrix(query_tok, doc_tok, BAT, A, B):\n",
    "    a_emb, b_emb = torch.Tensor(emb_matrix[query_tok]), torch.Tensor(emb_matrix[doc_tok])\n",
    "    a_denom = a_emb.norm(p=2, dim=2).reshape(BAT, A, 1).expand(BAT, A, B) + 1e-9  # avoid 0div\n",
    "    b_denom = b_emb.norm(p=2, dim=2).reshape(BAT, 1, B).expand(BAT, A, B) + 1e-9  # avoid 0div\n",
    "    perm = b_emb.permute(0, 2, 1)\n",
    "    sim = a_emb.bmm(perm) / (a_denom * b_denom)\n",
    "    sim = remove_padding(sim, query_tok, doc_tok, BAT, A, B)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8262, 0.5856, 0.3818, 0.6570, 0.0000, 0.0000],\n",
       "        [0.4679, 0.2739, 0.4120, 0.4533, 0.0000, 0.0000],\n",
       "        [0.9873, 0.6609, 0.3957, 0.6451, 0.0000, 0.0000],\n",
       "        [0.4632, 0.6222, 0.3524, 1.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = query_tok[0].numpy().tolist()\n",
    "d = doc_tok[0].numpy().tolist()\n",
    "\n",
    "a_emb, b_emb = torch.Tensor(emb_matrix[q]), torch.Tensor(emb_matrix[d])\n",
    "norm_a = a_emb.norm(p=2, dim=1)\n",
    "norm_b = b_emb.norm(p=2, dim=1)\n",
    "\n",
    "sim = a_emb.matmul(b_emb.t())/norm_a/norm_b\n",
    "sim = sim.nan_to_num()\n",
    "\n",
    "sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "krnm = KNRM(emb_matrix, freeze_embeddings = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(5.0523e-19, grad_fn=<SumBackward1>), tensor(4.0939e-15, grad_fn=<SumBackward1>), tensor(1.2204e-11, grad_fn=<SumBackward1>), tensor(1.3383e-08, grad_fn=<SumBackward1>), tensor(5.3992e-06, grad_fn=<SumBackward1>), tensor(0.0008, grad_fn=<SumBackward1>), tensor(0.0435, grad_fn=<SumBackward1>), tensor(0.8049, grad_fn=<SumBackward1>), tensor(4.1637, grad_fn=<SumBackward1>), tensor(7.7497, grad_fn=<SumBackward1>), tensor(7.7836, grad_fn=<SumBackward1>), tensor(4.5836, grad_fn=<SumBackward1>), tensor(2.7560, grad_fn=<SumBackward1>), tensor(3.6246, grad_fn=<SumBackward1>), tensor(4.0500, grad_fn=<SumBackward1>), tensor(3.8926, grad_fn=<SumBackward1>), tensor(3.3240, grad_fn=<SumBackward1>), tensor(2.2559, grad_fn=<SumBackward1>), tensor(1.6252, grad_fn=<SumBackward1>), tensor(1.6986, grad_fn=<SumBackward1>), tensor(8.2863, grad_fn=<SumBackward1>)]\n"
     ]
    }
   ],
   "source": [
    "matching_matrix = krnm._get_matching_matrix(query_tok[0], doc_tok[0])\n",
    "# shape = [Batch, Kernels]\n",
    "kernels_out = krnm._apply_kernels(matching_matrix)\n",
    "# shape = [Batch]\n",
    "out = krnm.mlp(kernels_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): GaussianKernel()\n",
       "  (1): GaussianKernel()\n",
       "  (2): GaussianKernel()\n",
       "  (3): GaussianKernel()\n",
       "  (4): GaussianKernel()\n",
       "  (5): GaussianKernel()\n",
       "  (6): GaussianKernel()\n",
       "  (7): GaussianKernel()\n",
       "  (8): GaussianKernel()\n",
       "  (9): GaussianKernel()\n",
       "  (10): GaussianKernel()\n",
       "  (11): GaussianKernel()\n",
       "  (12): GaussianKernel()\n",
       "  (13): GaussianKernel()\n",
       "  (14): GaussianKernel()\n",
       "  (15): GaussianKernel()\n",
       "  (16): GaussianKernel()\n",
       "  (17): GaussianKernel()\n",
       "  (18): GaussianKernel()\n",
       "  (19): GaussianKernel()\n",
       "  (20): GaussianKernel()\n",
       ")"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "krnm.kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.0523e-19, 4.0939e-15, 1.2204e-11, 1.3383e-08, 5.3992e-06, 8.0124e-04,\n",
       "        4.3541e-02, 8.0487e-01, 4.1637e+00, 7.7497e+00, 7.7836e+00, 4.5836e+00,\n",
       "        2.7560e+00, 3.6246e+00, 4.0500e+00, 3.8926e+00, 3.3240e+00, 2.2559e+00,\n",
       "        1.6252e+00, 1.6986e+00, 8.2863e+00], grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernels_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = dict([tuple(re.sub(\"\\n\",\"\",line).split(\" \", 1)) for line in open(glove_path, 'r')])\n",
    "fin = dict([(k, v.split(\" \")) for k,v in res.items() if k not in string.punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = torch.empty(3,10).uniform_(-0.2,0.2).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-0.078511',\n",
       " '-0.059738',\n",
       " '-0.126076',\n",
       " '0.127243',\n",
       " '-0.151578',\n",
       " '-0.120386',\n",
       " '-0.054364',\n",
       " '-0.058489',\n",
       " '0.168536',\n",
       " '-0.010920']"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = np.random.uniform(-0.2,0.2, size=(10,))\n",
    "[\"{:8.6f}\".format(elem) for elem in vec]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
