{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Flask\n",
      "  Using cached Flask-2.0.1-py3-none-any.whl (94 kB)\n",
      "Collecting click>=7.1.2\n",
      "  Using cached click-8.0.1-py3-none-any.whl (97 kB)\n",
      "Collecting Werkzeug>=2.0\n",
      "  Using cached Werkzeug-2.0.1-py3-none-any.whl (288 kB)\n",
      "Collecting itsdangerous>=2.0\n",
      "  Using cached itsdangerous-2.0.1-py3-none-any.whl (18 kB)\n",
      "Collecting Jinja2>=3.0\n",
      "  Using cached Jinja2-3.0.1-py3-none-any.whl (133 kB)\n",
      "Collecting importlib-metadata; python_version < \"3.8\"\n",
      "  Using cached importlib_metadata-4.6.3-py3-none-any.whl (17 kB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Using cached MarkupSafe-2.0.1-cp37-cp37m-manylinux2010_x86_64.whl (31 kB)\n",
      "Collecting typing-extensions>=3.6.4; python_version < \"3.8\"\n",
      "  Using cached typing_extensions-3.10.0.0-py3-none-any.whl (26 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Using cached zipp-3.5.0-py3-none-any.whl (5.7 kB)\n",
      "\u001b[31mERROR: uvicorn 0.11.1 has requirement click==7.*, but you'll have click 8.0.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: ipykernel 6.0.2 has requirement importlib-metadata<4; python_version < \"3.8.0\", but you'll have importlib-metadata 4.6.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: deeppavlov 0.8.0 has requirement numpy==1.18.0, but you'll have numpy 1.21.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: deeppavlov 0.8.0 has requirement pandas==0.25.3, but you'll have pandas 1.3.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: deeppavlov 0.8.0 has requirement requests==2.22.0, but you'll have requests 2.24.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: deeppavlov 0.8.0 has requirement scikit-learn==0.21.2, but you'll have scikit-learn 0.24.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: deeppavlov 0.8.0 has requirement scipy==1.4.1, but you'll have scipy 1.7.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: deeppavlov 0.8.0 has requirement tqdm==4.41.1, but you'll have tqdm 4.61.2 which is incompatible.\u001b[0m\n",
      "Installing collected packages: typing-extensions, zipp, importlib-metadata, click, Werkzeug, itsdangerous, MarkupSafe, Jinja2, Flask\n",
      "Successfully installed Flask-2.0.1 Jinja2-3.0.1 MarkupSafe-2.0.1 Werkzeug-2.0.1 click-8.0.1 importlib-metadata-4.6.3 itsdangerous-2.0.1 typing-extensions-3.10.0.0 zipp-3.5.0\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 21.2.2 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3.7 install -U Flask -t /home/sbc_dev/.local/lib/python3.7/site-packages --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/72/a3add0e4eec4eb9e2569554f7c70f4a3c27712f40e3284d483e88094cc0e/langdetect-1.0.9.tar.gz (981kB)\n",
      "\u001b[K    100% |████████████████████████████████| 983kB 2.3MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from langdetect) (1.15.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/a17194839/Library/Caches/pip/wheels/7e/18/13/038c34057808931c7ddc6c92d3aa015cf1a498df5a70268996\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 21.2.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3.7 install langdetect --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'/Users/a17194839/Library/Python/3.7/lib/python/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "from typing import Dict, List, Tuple, Union, Callable\n",
    "\n",
    "from itertools import permutations\n",
    "\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "# try:\n",
    "#     _create_unverified_https_context = ssl._create_unverified_context\n",
    "# except AttributeError:\n",
    "#     pass\n",
    "# else:\n",
    "#     ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# nltk.download('punkt')\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import faiss\n",
    "from langdetect import detect, DetectorFactory\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "from flask import Flask, request, jsonify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_qqp_dir = './data/QQP'\n",
    "EMB_PATH_GLOVE = './data/glove.6B.50d.txt'\n",
    "EMB_PATH_KNRM = './pkl/pretr_emb_krnm_50'\n",
    "VOCAB_PATH = './pkl/vocab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en'"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DetectorFactory.seed = 0\n",
    "detect(\"Why do people still believe in flat earth?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianKernel(torch.nn.Module):\n",
    "    def __init__(self, mu: float = 1., sigma: float = 1., requires_grad = False):\n",
    "        super().__init__()\n",
    "        mu_ = np.array(mu)\n",
    "        sigma_ = np.array(sigma)\n",
    "        self.requires_grad = requires_grad\n",
    "        self.mu = torch.nn.Parameter(torch.Tensor(mu_), requires_grad=self.requires_grad)\n",
    "        self.sigma = torch.nn.Parameter(torch.Tensor(sigma_), requires_grad=self.requires_grad)\n",
    "\n",
    "    def forward(self, x):\n",
    "        adj = x - self.mu\n",
    "        return torch.exp(-0.5 * adj * adj / self.sigma / self.sigma)        \n",
    "\n",
    "        \n",
    "class KNRM(torch.nn.Module):\n",
    "    def __init__(self, embedding_matrix: np.ndarray, freeze_embeddings: bool = False, kernel_num: int = 30,\n",
    "                 sigma: float = 0.1, exact_sigma: float = 0.001,\n",
    "                 out_layers: List[int] = [15, 7]):\n",
    "        super().__init__()\n",
    "        self.embeddings = torch.nn.Embedding.from_pretrained(\n",
    "            torch.FloatTensor(embedding_matrix),\n",
    "            freeze=freeze_embeddings,\n",
    "            padding_idx=0\n",
    "        )\n",
    "\n",
    "        self.kernel_num = kernel_num\n",
    "        self.sigma = sigma\n",
    "        self.exact_sigma = exact_sigma\n",
    "        self.out_layers = out_layers\n",
    "\n",
    "        self.kernels = self._get_kernels_layers()\n",
    "\n",
    "        self.mlp = self._get_mlp()\n",
    "\n",
    "        self.out_activation = torch.nn.Sigmoid()\n",
    "\n",
    "    def _get_kernels_layers(self) -> torch.nn.ModuleList:\n",
    "\n",
    "        mus = [1.0]\n",
    "        if self.kernel_num > 1:\n",
    "            bin_size = 2.0 / (self.kernel_num - 1)  \n",
    "            mus.append(1 - bin_size / 2)\n",
    "            for i in range(1, self.kernel_num - 1):\n",
    "                mus.append(mus[i] - bin_size)\n",
    "        mus = list(reversed(mus))\n",
    "        sigmas = [self.sigma] * (self.kernel_num - 1) + [self.exact_sigma]  \n",
    "        \n",
    "        gausskern_lst = [(GaussianKernel(mu,sigma)) for mu, sigma in zip(mus, sigmas)]\n",
    "        kernels = torch.nn.ModuleList(gausskern_lst)\n",
    "        return kernels\n",
    "\n",
    "    def _get_mlp(self) -> torch.nn.Sequential:        \n",
    "        if self.out_layers:\n",
    "            output = []\n",
    "            hidden_sizes = [self.kernel_num] + self.out_layers + [1]\n",
    "            for i, hidden in enumerate(hidden_sizes[1:],1):\n",
    "                output.append(torch.nn.ReLU())\n",
    "                output.append(torch.nn.Linear(hidden_sizes[i-1], hidden))\n",
    "        else:\n",
    "            output = [torch.nn.Linear(self.kernel_num, 1)]\n",
    "        return torch.nn.Sequential(*output)\n",
    "\n",
    "    def forward(self, input_1: Dict[str, torch.Tensor], input_2: Dict[str, torch.Tensor]) -> torch.FloatTensor:\n",
    "        logits_1 = self.predict(input_1)\n",
    "        logits_2 = self.predict(input_2)\n",
    "\n",
    "        logits_diff = logits_1 - logits_2\n",
    "\n",
    "        out = self.out_activation(logits_diff)\n",
    "        return out\n",
    "\n",
    "    def _get_matching_matrix(self, query: torch.Tensor, doc: torch.Tensor) -> torch.FloatTensor:\n",
    "        query = self.embeddings(query)\n",
    "        doc = self.embeddings(doc)\n",
    "        query = query / (query.norm(p=2, dim=-1, keepdim=True) + 1e-16)\n",
    "        doc = doc / (doc.norm(p=2, dim=-1, keepdim=True) + 1e-16)\n",
    "        return torch.bmm(query, doc.transpose(-1, -2))\n",
    "    \n",
    "#         q = query.numpy().tolist()\n",
    "#         d = doc.numpy().tolist()\n",
    "\n",
    "#         a_emb, b_emb = torch.Tensor(self.embeddings.weight[q]), torch.Tensor(self.embeddings.weight[d])\n",
    "#         norm_a = a_emb.norm(p=2, dim=1)\n",
    "#         norm_b = b_emb.norm(p=2, dim=1)\n",
    "\n",
    "#         sim = a_emb.matmul(b_emb.t())/norm_a/norm_b\n",
    "#         matching_matrix = sim.nan_to_num()\n",
    "#         return matching_matrix\n",
    "\n",
    "    def _apply_kernels(self, matching_matrix: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        KM = []\n",
    "        for kernel in self.kernels:\n",
    "            # shape = [B]\n",
    "            K = torch.log1p(kernel(matching_matrix).sum(dim=-1)).sum(dim=-1)\n",
    "            KM.append(K)\n",
    "                \n",
    "        # shape = [B, K]\n",
    "        kernels_out = torch.stack(KM, dim=1)\n",
    "        return kernels_out\n",
    "\n",
    "    def predict(self, inputs: Dict[str, torch.Tensor]) -> torch.FloatTensor:\n",
    "        # shape = [Batch, Left, Embedding], [Batch, Right, Embedding]\n",
    "        query, doc = inputs['query'], inputs['document']\n",
    "        \n",
    "        # shape = [Batch, Left, Right]\n",
    "        matching_matrix = self._get_matching_matrix(query, doc)\n",
    "        # shape = [Batch, Kernels]\n",
    "        kernels_out = self._apply_kernels(matching_matrix)\n",
    "        # shape = [Batch]\n",
    "        out = self.mlp(kernels_out)\n",
    "        return out\n",
    "\n",
    "    \n",
    "def collate_fn(batch_objs: List[List[int]], name: str = 'query'):\n",
    "    max_len_q1 = -1\n",
    "\n",
    "    is_triplets = False\n",
    "    for elems in batch_objs:\n",
    "        max_len_q1 = max(len(elems), max_len_q1)\n",
    "\n",
    "    q1s = []\n",
    "\n",
    "    for elems in batch_objs:\n",
    "        pad_len1 = max_len_q1 - len(elems)\n",
    "\n",
    "        q1s.append(elems + [0] * pad_len1)\n",
    "\n",
    "    ret_left = {name: torch.LongTensor(q1s)}\n",
    "\n",
    "    return ret_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_PATH = './pkl/mlp_pretr.tor'\n",
    "EMB_PATH_KNRM = './pkl/pretr_emb_krnm_50.tor'\n",
    "EMB_PATH_KNRM_TXT = './pkl/pretr_emb_krnm_50.txt'\n",
    "\n",
    "VOCAB_PATH = './pkl/vocab'\n",
    "VOCAB_PATH_J = './pkl/vocab.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(VOCAB_PATH, \"rb\") as fin:\n",
    "#     vocab = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# with open(VOCAB_PATH_J, 'w') as fout:\n",
    "#     json.dump(vocab,fout,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(VOCAB_PATH_J, 'r') as fin:\n",
    "    vocab = json.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(EMB_PATH_KNRM, \"rb\") as fin:\n",
    "#     emb_matrix = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(EMB_PATH_KNRM_TXT, \"w\") as fout:\n",
    "#     for line in emb_matrix:\n",
    "#         str_arr = \" \".join([\"{:10.7f}\".format(ele).strip() for ele in line])\n",
    "#         fout.writelines(str_arr+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(EMB_PATH_KNRM_TXT, \"r\") as fin:\n",
    "#     emb_matrix = fin.readlines()\n",
    "    \n",
    "# emb_matrix = torch.Tensor([list(map(float,re.sub(r\"\\n\", \"\", ele).split(\" \"))) \n",
    "#                            for ele in emb_matrix\n",
    "#                           ]\n",
    "#                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('1.weight',\n",
       "              tensor([[-1.3675e-03,  9.7939e-02, -1.5025e-01, -1.3421e-01, -6.9578e-02,\n",
       "                        5.1119e-02,  1.1802e-03,  1.5375e-01, -2.2322e-03,  6.3431e-02,\n",
       "                       -5.8350e-02, -9.0235e-02, -2.9208e-01, -2.8554e-01, -2.5503e-01,\n",
       "                       -1.5083e-01, -3.1750e-02,  6.9409e-02, -1.1293e-01, -4.6203e-02,\n",
       "                        1.0435e-01,  1.9258e-01,  7.3940e-03,  1.8652e-01,  2.4456e-02,\n",
       "                        7.1679e-02,  2.0797e-01, -1.4159e-01, -1.0320e-01, -5.9219e-02],\n",
       "                      [-7.1165e-02,  1.5776e-01, -1.1824e-01, -8.3621e-02, -1.2641e-01,\n",
       "                       -1.6893e-01, -1.0427e-01,  1.5859e-01,  8.2196e-02,  8.7204e-02,\n",
       "                       -6.6780e-04, -1.2414e-01, -1.4524e-02, -2.1770e-01, -1.7867e-01,\n",
       "                       -1.4649e-01,  5.4514e-02,  4.6886e-02, -1.3491e-01, -5.8005e-02,\n",
       "                        6.6114e-02,  1.3351e-01,  2.9996e-02, -1.1296e-02,  9.0604e-02,\n",
       "                       -1.3708e-01,  4.3652e-03, -1.7805e-01, -1.7186e-01, -1.4823e-01],\n",
       "                      [ 8.2604e-02,  7.3395e-02, -1.0821e-01,  5.5100e-02,  1.0009e-01,\n",
       "                       -2.3999e-02,  4.6066e-03,  4.0354e-02,  1.1432e-01,  1.7696e-01,\n",
       "                       -1.6353e-01, -2.2192e-01, -2.7808e-01, -3.2674e-01, -3.3824e-01,\n",
       "                       -2.3605e-01, -1.5469e-01, -1.4401e-01,  5.0416e-02, -1.6194e-01,\n",
       "                       -2.8050e-01,  1.9265e-02,  4.4113e-04, -2.8976e-01, -3.5038e-02,\n",
       "                       -6.9591e-02, -4.3255e-02, -5.7062e-02,  1.3592e-01, -2.2065e-02],\n",
       "                      [ 6.6930e-02,  9.2329e-02,  1.3070e-01,  6.8260e-02, -1.8075e-01,\n",
       "                       -1.1860e-01,  9.0826e-02,  3.7695e-02, -1.4307e-01, -1.0585e-01,\n",
       "                        1.7083e-01,  1.2145e-01, -8.2212e-02, -4.9681e-02, -1.7823e-01,\n",
       "                       -6.7770e-03, -1.3829e-01, -1.3738e-01, -1.2244e-03,  4.1863e-02,\n",
       "                       -5.5355e-02,  1.3148e-01, -8.5543e-02,  1.9280e-01,  1.5340e-01,\n",
       "                       -1.2504e-01, -1.6965e-02,  3.4709e-02,  4.9655e-02,  5.6266e-02],\n",
       "                      [ 7.1647e-02,  1.0932e-02, -8.9110e-02,  8.6264e-02, -1.7557e-01,\n",
       "                       -1.0922e-01, -4.7558e-02, -9.2617e-02, -7.2234e-02, -1.7059e-01,\n",
       "                       -9.9124e-02, -9.9455e-02, -3.4611e-01, -3.0741e-01, -2.0201e-01,\n",
       "                       -3.4600e-01, -2.8761e-01, -2.6292e-01, -1.7082e-01, -8.9623e-02,\n",
       "                       -5.4648e-02, -1.3252e-01, -8.8630e-02, -2.7965e-01, -4.2348e-01,\n",
       "                       -3.2679e-01, -8.5102e-02, -1.6841e-01, -2.0706e-01, -1.4625e-01],\n",
       "                      [-1.7086e-01, -1.5409e-01, -3.7032e-02,  1.0014e-01,  9.8780e-02,\n",
       "                       -1.7585e-01,  1.1435e-01, -1.4206e-01, -3.7236e-02, -7.2077e-02,\n",
       "                       -3.2984e-02, -3.4144e-02, -1.6417e-01, -1.6283e-01, -4.1093e-02,\n",
       "                       -1.7263e-02, -1.0737e-01,  3.9077e-02, -2.0353e-01, -6.2401e-02,\n",
       "                        1.0254e-01, -1.3533e-01,  1.0576e-01,  1.1095e-02, -2.1528e-01,\n",
       "                        6.8664e-02, -6.5254e-02, -1.2256e-01,  1.1047e-01, -1.6846e-01],\n",
       "                      [ 1.9591e-02, -3.8225e-02,  1.3038e-01,  5.0965e-02,  8.7728e-02,\n",
       "                        6.4478e-02, -4.3904e-02, -3.8396e-02, -1.5046e-01,  9.8927e-02,\n",
       "                        1.4496e-01,  1.2492e-01, -1.2878e-01,  8.1428e-03, -1.2870e-01,\n",
       "                       -1.0050e-01, -1.0639e-01,  6.2394e-02, -1.0880e-01, -3.9833e-03,\n",
       "                        7.6806e-03,  1.1769e-01, -1.3801e-01, -1.2534e-01, -1.0601e-01,\n",
       "                        1.2779e-01, -6.5629e-02,  1.5400e-01,  6.6020e-02,  2.3119e-02],\n",
       "                      [-1.3594e-03, -3.6095e-02,  2.2890e-02, -4.1759e-02, -1.4623e-03,\n",
       "                        2.3033e-02, -1.4299e-01, -9.5878e-02,  1.4635e-01, -1.5288e-01,\n",
       "                       -3.1298e-02,  1.3267e-01, -1.8734e-02, -1.1000e-01, -2.8768e-01,\n",
       "                       -3.4880e-02, -2.3927e-01, -1.4459e-01, -1.4525e-01, -1.1764e-01,\n",
       "                       -1.0283e-01,  3.2933e-02, -1.0376e-01,  3.3063e-02, -1.6873e-02,\n",
       "                       -2.3618e-01, -2.5405e-02, -2.1722e-01, -1.4215e-01,  4.2089e-02],\n",
       "                      [ 6.0277e-02,  1.3692e-01, -5.8982e-02, -2.8390e-04,  9.2906e-02,\n",
       "                       -1.7780e-01,  1.3213e-01, -1.4745e-01,  8.1407e-03, -4.2672e-02,\n",
       "                       -2.1562e-01, -3.4554e-01, -4.8649e-01, -8.6552e-01, -9.6219e-01,\n",
       "                       -8.1342e-01, -6.7843e-01, -1.0104e-01, -3.1493e-02,  3.1999e-02,\n",
       "                        7.3683e-02,  2.9937e-01,  3.0466e-01,  3.5825e-01,  2.9002e-01,\n",
       "                        1.5873e-01,  1.5875e-01,  2.6554e-01,  3.6666e-01,  2.1945e-01],\n",
       "                      [-8.6380e-02, -8.5798e-02, -1.7250e-01,  3.9800e-02, -1.0152e-01,\n",
       "                       -1.6058e-01,  1.6438e-01, -1.1118e-01, -8.5528e-03,  7.0220e-02,\n",
       "                        2.7261e-03, -2.4006e-01, -4.1537e-01, -2.2809e-01, -3.8236e-01,\n",
       "                       -2.2648e-01, -3.2855e-01, -3.3421e-02,  2.6588e-02,  9.3210e-02,\n",
       "                       -2.8708e-02,  1.2056e-01,  1.0061e-01,  3.0794e-02, -1.3783e-02,\n",
       "                       -1.0193e-01,  9.0619e-02, -2.6287e-01, -1.5016e-01, -3.7858e-01],\n",
       "                      [-8.8122e-02,  3.2620e-02, -9.5199e-02,  4.2624e-02,  4.0663e-02,\n",
       "                       -1.2320e-01,  5.2334e-02,  1.1287e-01,  1.0285e-01, -9.5329e-02,\n",
       "                       -4.9825e-01, -4.7958e-01, -2.1872e-01, -1.7265e-01,  6.7351e-02,\n",
       "                        1.5642e-01, -8.5995e-02, -6.8808e-02,  8.8609e-02,  3.1575e-01,\n",
       "                        4.5748e-01,  4.3708e-01,  1.1081e-01, -8.0735e-05, -1.9677e-01,\n",
       "                       -4.1278e-01, -4.5663e-01, -5.9714e-02,  6.5176e-01,  1.4373e+00],\n",
       "                      [-1.5128e-01, -1.5692e-01,  1.8232e-01,  1.1578e-01, -1.2778e-01,\n",
       "                        6.7202e-02,  1.3034e-01,  1.7183e-01,  1.5857e-01,  1.7566e-01,\n",
       "                       -8.6481e-02, -6.2443e-01, -1.0946e+00, -1.6220e+00, -2.0095e+00,\n",
       "                       -1.5893e+00, -1.2075e+00, -5.2892e-01, -2.8107e-01, -1.1710e-01,\n",
       "                       -3.1375e-01, -7.9870e-02, -1.1366e-01,  2.1085e-01,  4.0253e-01,\n",
       "                        6.0302e-01,  4.0091e-01,  1.6465e-01, -5.5519e-01, -1.1200e+00],\n",
       "                      [-1.2353e-01, -1.8010e-01, -1.4657e-01,  1.4427e-01,  9.8978e-02,\n",
       "                        1.7147e-01,  1.4605e-01, -1.6453e-01, -1.2934e-01, -4.3821e-02,\n",
       "                       -1.6474e-01,  9.8183e-03, -3.3439e-01, -3.4149e-01, -4.5829e-01,\n",
       "                       -3.9782e-01, -1.4843e-01, -1.5184e-01, -1.8306e-01, -1.9997e-01,\n",
       "                       -2.0765e-01, -4.3641e-01, -3.1664e-01, -2.3196e-01, -3.2390e-01,\n",
       "                       -1.7962e-01, -2.3554e-01, -1.8506e-01, -3.2552e-01, -6.3501e-02],\n",
       "                      [ 2.0886e-02, -4.4031e-02, -1.0359e-01, -9.9882e-02, -1.2468e-01,\n",
       "                        1.6262e-01,  2.0215e-01,  9.5709e-02,  9.5969e-03,  1.4758e-02,\n",
       "                       -6.1936e-01, -8.9491e-01, -5.6557e-01,  1.1666e-01,  2.8275e-01,\n",
       "                        1.6091e-01,  3.4097e-01, -2.3372e-01, -9.5643e-01, -5.5329e-01,\n",
       "                       -3.9387e-01, -2.1681e-02,  3.5979e-01,  1.8371e-01, -2.1429e-01,\n",
       "                       -5.1605e-01, -4.3605e-01,  6.9819e-01,  2.2366e+00,  4.3423e+00],\n",
       "                      [ 9.7751e-03, -1.1273e-01,  9.3577e-03,  8.7542e-02,  9.0564e-02,\n",
       "                       -1.6686e-01, -3.2670e-02, -1.3568e-01, -7.7902e-02,  6.5780e-02,\n",
       "                       -1.2965e-01,  6.7868e-02,  1.5496e-01,  1.1975e-02, -1.2168e-01,\n",
       "                       -6.5414e-02,  3.9868e-02, -1.3918e-01,  9.0705e-02, -1.6575e-01,\n",
       "                       -1.7551e-01, -1.7740e-01, -3.7038e-02,  1.2277e-01, -1.7280e-01,\n",
       "                        1.5175e-01, -7.3034e-02,  5.3473e-02,  8.3259e-03, -1.6463e-01]])),\n",
       "             ('1.bias',\n",
       "              tensor([ 0.1509,  0.0972,  0.1810,  0.0926, -0.1266,  0.1516,  0.0098,  0.0840,\n",
       "                      -0.1500, -0.0580, -0.1605, -0.0934,  0.0323, -0.0961,  0.0582])),\n",
       "             ('3.weight',\n",
       "              tensor([[ 1.2983e-01,  2.8194e-02, -1.7276e-01, -2.0261e-01, -8.2955e-02,\n",
       "                        1.0756e-01,  2.5469e-01,  1.4115e-01, -4.3655e-02,  9.0529e-02,\n",
       "                       -2.7386e-01, -2.2773e-01,  7.7246e-02, -1.4650e-01,  1.2422e-01],\n",
       "                      [-1.8158e-01, -1.1071e-01, -2.3807e-01,  1.3579e-01, -7.6854e-02,\n",
       "                        2.0293e-01,  1.5979e-01,  1.2312e-01,  8.7962e-03, -8.0546e-02,\n",
       "                       -1.2212e-01, -4.3566e-02,  4.9093e-02, -2.0212e-01,  2.1670e-01],\n",
       "                      [-3.9176e-02, -2.2999e-01, -5.4564e-01,  2.3093e-01, -1.5409e-01,\n",
       "                       -2.3316e-01,  1.8211e-01,  3.4601e-02,  1.3969e-01,  1.5146e-01,\n",
       "                       -6.9335e-01, -6.2792e-01, -6.5398e-01, -3.7268e-01,  8.6407e-02],\n",
       "                      [-9.0029e-02,  3.1797e-02, -4.1439e-01, -9.6850e-02,  1.2167e-01,\n",
       "                        8.7897e-02, -1.2615e-01,  4.1838e-02,  1.2043e-01, -2.2517e-02,\n",
       "                       -6.5903e-01, -7.7861e-02, -7.5666e-02, -1.9414e-01, -1.5183e-01],\n",
       "                      [-1.3106e-01,  2.3729e-01, -6.9477e-02, -7.0497e-04, -1.2510e-01,\n",
       "                        2.5776e-01,  2.5218e-01, -1.9473e-01, -2.0931e-01, -1.9571e-01,\n",
       "                       -1.2452e-03, -6.5816e-02, -1.6900e-01, -9.2609e-02,  4.8782e-02],\n",
       "                      [-1.8553e-01, -1.1956e-01, -1.1824e-01, -1.2748e-01,  8.4171e-02,\n",
       "                        2.0117e-01, -1.7456e-01,  2.2387e-02, -1.5295e-01,  2.5337e-01,\n",
       "                       -2.7256e-01, -2.5968e-01, -3.0421e-02, -1.3355e-01,  4.9503e-02],\n",
       "                      [ 4.6458e-01,  6.6984e-01, -1.0659e+00,  1.5858e-01, -5.0298e-01,\n",
       "                       -2.4947e-01, -1.5243e-01, -1.1199e-01,  3.1045e-01, -2.8822e-03,\n",
       "                       -5.8273e-01,  1.0396e+00,  2.5883e-01, -1.8503e+00,  1.2977e-01]])),\n",
       "             ('3.bias',\n",
       "              tensor([ 0.2301,  0.1422, -0.1062, -0.1817, -0.0192,  0.2151,  0.0035])),\n",
       "             ('5.weight',\n",
       "              tensor([[-0.3562,  0.1242,  0.1234,  0.1191, -0.3130, -0.1516, -1.1867]])),\n",
       "             ('5.bias', tensor([-0.0286]))])"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = torch.load(MLP_PATH)\n",
    "mlp.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_matrix = torch.load(EMB_PATH_KNRM)\n",
    "knrm = KNRM(emb_matrix[\"weight\"], freeze_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0022, -0.0369,  0.0055,  ..., -0.1204, -0.0544, -0.0585],\n",
       "        [ 0.4532,  0.0598, -0.1058,  ...,  0.5324, -0.2510,  0.6255],\n",
       "        ...,\n",
       "        [-0.0022, -0.0369,  0.0055,  ..., -0.1204, -0.0544, -0.0585],\n",
       "        [-0.5138, -0.9359, -0.1946,  ...,  0.2532,  0.1363, -0.0305],\n",
       "        [-0.3993, -0.5018, -0.8642,  ...,  0.4815, -0.2657, -0.2634]])"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_matrix['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('1.weight',\n",
       "              tensor([[-1.3675e-03,  9.7939e-02, -1.5025e-01, -1.3421e-01, -6.9578e-02,\n",
       "                        5.1119e-02,  1.1802e-03,  1.5375e-01, -2.2322e-03,  6.3431e-02,\n",
       "                       -5.8350e-02, -9.0235e-02, -2.9208e-01, -2.8554e-01, -2.5503e-01,\n",
       "                       -1.5083e-01, -3.1750e-02,  6.9409e-02, -1.1293e-01, -4.6203e-02,\n",
       "                        1.0435e-01,  1.9258e-01,  7.3940e-03,  1.8652e-01,  2.4456e-02,\n",
       "                        7.1679e-02,  2.0797e-01, -1.4159e-01, -1.0320e-01, -5.9219e-02],\n",
       "                      [-7.1165e-02,  1.5776e-01, -1.1824e-01, -8.3621e-02, -1.2641e-01,\n",
       "                       -1.6893e-01, -1.0427e-01,  1.5859e-01,  8.2196e-02,  8.7204e-02,\n",
       "                       -6.6780e-04, -1.2414e-01, -1.4524e-02, -2.1770e-01, -1.7867e-01,\n",
       "                       -1.4649e-01,  5.4514e-02,  4.6886e-02, -1.3491e-01, -5.8005e-02,\n",
       "                        6.6114e-02,  1.3351e-01,  2.9996e-02, -1.1296e-02,  9.0604e-02,\n",
       "                       -1.3708e-01,  4.3652e-03, -1.7805e-01, -1.7186e-01, -1.4823e-01],\n",
       "                      [ 8.2604e-02,  7.3395e-02, -1.0821e-01,  5.5100e-02,  1.0009e-01,\n",
       "                       -2.3999e-02,  4.6066e-03,  4.0354e-02,  1.1432e-01,  1.7696e-01,\n",
       "                       -1.6353e-01, -2.2192e-01, -2.7808e-01, -3.2674e-01, -3.3824e-01,\n",
       "                       -2.3605e-01, -1.5469e-01, -1.4401e-01,  5.0416e-02, -1.6194e-01,\n",
       "                       -2.8050e-01,  1.9265e-02,  4.4113e-04, -2.8976e-01, -3.5038e-02,\n",
       "                       -6.9591e-02, -4.3255e-02, -5.7062e-02,  1.3592e-01, -2.2065e-02],\n",
       "                      [ 6.6930e-02,  9.2329e-02,  1.3070e-01,  6.8260e-02, -1.8075e-01,\n",
       "                       -1.1860e-01,  9.0826e-02,  3.7695e-02, -1.4307e-01, -1.0585e-01,\n",
       "                        1.7083e-01,  1.2145e-01, -8.2212e-02, -4.9681e-02, -1.7823e-01,\n",
       "                       -6.7770e-03, -1.3829e-01, -1.3738e-01, -1.2244e-03,  4.1863e-02,\n",
       "                       -5.5355e-02,  1.3148e-01, -8.5543e-02,  1.9280e-01,  1.5340e-01,\n",
       "                       -1.2504e-01, -1.6965e-02,  3.4709e-02,  4.9655e-02,  5.6266e-02],\n",
       "                      [ 7.1647e-02,  1.0932e-02, -8.9110e-02,  8.6264e-02, -1.7557e-01,\n",
       "                       -1.0922e-01, -4.7558e-02, -9.2617e-02, -7.2234e-02, -1.7059e-01,\n",
       "                       -9.9124e-02, -9.9455e-02, -3.4611e-01, -3.0741e-01, -2.0201e-01,\n",
       "                       -3.4600e-01, -2.8761e-01, -2.6292e-01, -1.7082e-01, -8.9623e-02,\n",
       "                       -5.4648e-02, -1.3252e-01, -8.8630e-02, -2.7965e-01, -4.2348e-01,\n",
       "                       -3.2679e-01, -8.5102e-02, -1.6841e-01, -2.0706e-01, -1.4625e-01],\n",
       "                      [-1.7086e-01, -1.5409e-01, -3.7032e-02,  1.0014e-01,  9.8780e-02,\n",
       "                       -1.7585e-01,  1.1435e-01, -1.4206e-01, -3.7236e-02, -7.2077e-02,\n",
       "                       -3.2984e-02, -3.4144e-02, -1.6417e-01, -1.6283e-01, -4.1093e-02,\n",
       "                       -1.7263e-02, -1.0737e-01,  3.9077e-02, -2.0353e-01, -6.2401e-02,\n",
       "                        1.0254e-01, -1.3533e-01,  1.0576e-01,  1.1095e-02, -2.1528e-01,\n",
       "                        6.8664e-02, -6.5254e-02, -1.2256e-01,  1.1047e-01, -1.6846e-01],\n",
       "                      [ 1.9591e-02, -3.8225e-02,  1.3038e-01,  5.0965e-02,  8.7728e-02,\n",
       "                        6.4478e-02, -4.3904e-02, -3.8396e-02, -1.5046e-01,  9.8927e-02,\n",
       "                        1.4496e-01,  1.2492e-01, -1.2878e-01,  8.1428e-03, -1.2870e-01,\n",
       "                       -1.0050e-01, -1.0639e-01,  6.2394e-02, -1.0880e-01, -3.9833e-03,\n",
       "                        7.6806e-03,  1.1769e-01, -1.3801e-01, -1.2534e-01, -1.0601e-01,\n",
       "                        1.2779e-01, -6.5629e-02,  1.5400e-01,  6.6020e-02,  2.3119e-02],\n",
       "                      [-1.3594e-03, -3.6095e-02,  2.2890e-02, -4.1759e-02, -1.4623e-03,\n",
       "                        2.3033e-02, -1.4299e-01, -9.5878e-02,  1.4635e-01, -1.5288e-01,\n",
       "                       -3.1298e-02,  1.3267e-01, -1.8734e-02, -1.1000e-01, -2.8768e-01,\n",
       "                       -3.4880e-02, -2.3927e-01, -1.4459e-01, -1.4525e-01, -1.1764e-01,\n",
       "                       -1.0283e-01,  3.2933e-02, -1.0376e-01,  3.3063e-02, -1.6873e-02,\n",
       "                       -2.3618e-01, -2.5405e-02, -2.1722e-01, -1.4215e-01,  4.2089e-02],\n",
       "                      [ 6.0277e-02,  1.3692e-01, -5.8982e-02, -2.8390e-04,  9.2906e-02,\n",
       "                       -1.7780e-01,  1.3213e-01, -1.4745e-01,  8.1407e-03, -4.2672e-02,\n",
       "                       -2.1562e-01, -3.4554e-01, -4.8649e-01, -8.6552e-01, -9.6219e-01,\n",
       "                       -8.1342e-01, -6.7843e-01, -1.0104e-01, -3.1493e-02,  3.1999e-02,\n",
       "                        7.3683e-02,  2.9937e-01,  3.0466e-01,  3.5825e-01,  2.9002e-01,\n",
       "                        1.5873e-01,  1.5875e-01,  2.6554e-01,  3.6666e-01,  2.1945e-01],\n",
       "                      [-8.6380e-02, -8.5798e-02, -1.7250e-01,  3.9800e-02, -1.0152e-01,\n",
       "                       -1.6058e-01,  1.6438e-01, -1.1118e-01, -8.5528e-03,  7.0220e-02,\n",
       "                        2.7261e-03, -2.4006e-01, -4.1537e-01, -2.2809e-01, -3.8236e-01,\n",
       "                       -2.2648e-01, -3.2855e-01, -3.3421e-02,  2.6588e-02,  9.3210e-02,\n",
       "                       -2.8708e-02,  1.2056e-01,  1.0061e-01,  3.0794e-02, -1.3783e-02,\n",
       "                       -1.0193e-01,  9.0619e-02, -2.6287e-01, -1.5016e-01, -3.7858e-01],\n",
       "                      [-8.8122e-02,  3.2620e-02, -9.5199e-02,  4.2624e-02,  4.0663e-02,\n",
       "                       -1.2320e-01,  5.2334e-02,  1.1287e-01,  1.0285e-01, -9.5329e-02,\n",
       "                       -4.9825e-01, -4.7958e-01, -2.1872e-01, -1.7265e-01,  6.7351e-02,\n",
       "                        1.5642e-01, -8.5995e-02, -6.8808e-02,  8.8609e-02,  3.1575e-01,\n",
       "                        4.5748e-01,  4.3708e-01,  1.1081e-01, -8.0735e-05, -1.9677e-01,\n",
       "                       -4.1278e-01, -4.5663e-01, -5.9714e-02,  6.5176e-01,  1.4373e+00],\n",
       "                      [-1.5128e-01, -1.5692e-01,  1.8232e-01,  1.1578e-01, -1.2778e-01,\n",
       "                        6.7202e-02,  1.3034e-01,  1.7183e-01,  1.5857e-01,  1.7566e-01,\n",
       "                       -8.6481e-02, -6.2443e-01, -1.0946e+00, -1.6220e+00, -2.0095e+00,\n",
       "                       -1.5893e+00, -1.2075e+00, -5.2892e-01, -2.8107e-01, -1.1710e-01,\n",
       "                       -3.1375e-01, -7.9870e-02, -1.1366e-01,  2.1085e-01,  4.0253e-01,\n",
       "                        6.0302e-01,  4.0091e-01,  1.6465e-01, -5.5519e-01, -1.1200e+00],\n",
       "                      [-1.2353e-01, -1.8010e-01, -1.4657e-01,  1.4427e-01,  9.8978e-02,\n",
       "                        1.7147e-01,  1.4605e-01, -1.6453e-01, -1.2934e-01, -4.3821e-02,\n",
       "                       -1.6474e-01,  9.8183e-03, -3.3439e-01, -3.4149e-01, -4.5829e-01,\n",
       "                       -3.9782e-01, -1.4843e-01, -1.5184e-01, -1.8306e-01, -1.9997e-01,\n",
       "                       -2.0765e-01, -4.3641e-01, -3.1664e-01, -2.3196e-01, -3.2390e-01,\n",
       "                       -1.7962e-01, -2.3554e-01, -1.8506e-01, -3.2552e-01, -6.3501e-02],\n",
       "                      [ 2.0886e-02, -4.4031e-02, -1.0359e-01, -9.9882e-02, -1.2468e-01,\n",
       "                        1.6262e-01,  2.0215e-01,  9.5709e-02,  9.5969e-03,  1.4758e-02,\n",
       "                       -6.1936e-01, -8.9491e-01, -5.6557e-01,  1.1666e-01,  2.8275e-01,\n",
       "                        1.6091e-01,  3.4097e-01, -2.3372e-01, -9.5643e-01, -5.5329e-01,\n",
       "                       -3.9387e-01, -2.1681e-02,  3.5979e-01,  1.8371e-01, -2.1429e-01,\n",
       "                       -5.1605e-01, -4.3605e-01,  6.9819e-01,  2.2366e+00,  4.3423e+00],\n",
       "                      [ 9.7751e-03, -1.1273e-01,  9.3577e-03,  8.7542e-02,  9.0564e-02,\n",
       "                       -1.6686e-01, -3.2670e-02, -1.3568e-01, -7.7902e-02,  6.5780e-02,\n",
       "                       -1.2965e-01,  6.7868e-02,  1.5496e-01,  1.1975e-02, -1.2168e-01,\n",
       "                       -6.5414e-02,  3.9868e-02, -1.3918e-01,  9.0705e-02, -1.6575e-01,\n",
       "                       -1.7551e-01, -1.7740e-01, -3.7038e-02,  1.2277e-01, -1.7280e-01,\n",
       "                        1.5175e-01, -7.3034e-02,  5.3473e-02,  8.3259e-03, -1.6463e-01]])),\n",
       "             ('1.bias',\n",
       "              tensor([ 0.1509,  0.0972,  0.1810,  0.0926, -0.1266,  0.1516,  0.0098,  0.0840,\n",
       "                      -0.1500, -0.0580, -0.1605, -0.0934,  0.0323, -0.0961,  0.0582])),\n",
       "             ('3.weight',\n",
       "              tensor([[ 1.2983e-01,  2.8194e-02, -1.7276e-01, -2.0261e-01, -8.2955e-02,\n",
       "                        1.0756e-01,  2.5469e-01,  1.4115e-01, -4.3655e-02,  9.0529e-02,\n",
       "                       -2.7386e-01, -2.2773e-01,  7.7246e-02, -1.4650e-01,  1.2422e-01],\n",
       "                      [-1.8158e-01, -1.1071e-01, -2.3807e-01,  1.3579e-01, -7.6854e-02,\n",
       "                        2.0293e-01,  1.5979e-01,  1.2312e-01,  8.7962e-03, -8.0546e-02,\n",
       "                       -1.2212e-01, -4.3566e-02,  4.9093e-02, -2.0212e-01,  2.1670e-01],\n",
       "                      [-3.9176e-02, -2.2999e-01, -5.4564e-01,  2.3093e-01, -1.5409e-01,\n",
       "                       -2.3316e-01,  1.8211e-01,  3.4601e-02,  1.3969e-01,  1.5146e-01,\n",
       "                       -6.9335e-01, -6.2792e-01, -6.5398e-01, -3.7268e-01,  8.6407e-02],\n",
       "                      [-9.0029e-02,  3.1797e-02, -4.1439e-01, -9.6850e-02,  1.2167e-01,\n",
       "                        8.7897e-02, -1.2615e-01,  4.1838e-02,  1.2043e-01, -2.2517e-02,\n",
       "                       -6.5903e-01, -7.7861e-02, -7.5666e-02, -1.9414e-01, -1.5183e-01],\n",
       "                      [-1.3106e-01,  2.3729e-01, -6.9477e-02, -7.0497e-04, -1.2510e-01,\n",
       "                        2.5776e-01,  2.5218e-01, -1.9473e-01, -2.0931e-01, -1.9571e-01,\n",
       "                       -1.2452e-03, -6.5816e-02, -1.6900e-01, -9.2609e-02,  4.8782e-02],\n",
       "                      [-1.8553e-01, -1.1956e-01, -1.1824e-01, -1.2748e-01,  8.4171e-02,\n",
       "                        2.0117e-01, -1.7456e-01,  2.2387e-02, -1.5295e-01,  2.5337e-01,\n",
       "                       -2.7256e-01, -2.5968e-01, -3.0421e-02, -1.3355e-01,  4.9503e-02],\n",
       "                      [ 4.6458e-01,  6.6984e-01, -1.0659e+00,  1.5858e-01, -5.0298e-01,\n",
       "                       -2.4947e-01, -1.5243e-01, -1.1199e-01,  3.1045e-01, -2.8822e-03,\n",
       "                       -5.8273e-01,  1.0396e+00,  2.5883e-01, -1.8503e+00,  1.2977e-01]])),\n",
       "             ('3.bias',\n",
       "              tensor([ 0.2301,  0.1422, -0.1062, -0.1817, -0.0192,  0.2151,  0.0035])),\n",
       "             ('5.weight',\n",
       "              tensor([[-0.3562,  0.1242,  0.1234,  0.1191, -0.3130, -0.1516, -1.1867]])),\n",
       "             ('5.bias', tensor([-0.0286]))])"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knrm.mlp = mlp\n",
    "knrm.mlp.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_glove_embeddings(file_path: str) -> Dict[str, List[str]]:\n",
    "    res = dict([tuple(line.split(\" \", 1)) for line in open(file_path, 'r')]) \n",
    "    fin = dict([(k, list(map(float,v.split(\" \")))) for k,v in res.items() if k not in string.punctuation])\n",
    "    return fin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.09 s, sys: 1.06 s, total: 8.16 s\n",
      "Wall time: 8.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "glove_emb = _read_glove_embeddings(EMB_PATH_GLOVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = glove_emb.get(\"the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = emb / np.linalg.norm(emb, axis = 0, keepdims = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAISS train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idx_to_text_mapping(inp_df: pd.DataFrame) -> Dict[str, str]:\n",
    "    left_dict = (\n",
    "        inp_df\n",
    "        [['id_left', 'text_left']]\n",
    "        .drop_duplicates()\n",
    "        .set_index('id_left')\n",
    "        ['text_left']\n",
    "        .to_dict()\n",
    "    )\n",
    "    right_dict = (\n",
    "        inp_df\n",
    "        [['id_right', 'text_right']]\n",
    "        .drop_duplicates()\n",
    "        .set_index('id_right')\n",
    "        ['text_right']\n",
    "        .to_dict()\n",
    "    )\n",
    "    left_dict.update(right_dict)\n",
    "    return left_dict\n",
    "\n",
    "def get_glue_df(partition_type: str) -> pd.DataFrame:\n",
    "    assert partition_type in ['dev', 'train']\n",
    "    glue_df = pd.read_csv(\n",
    "        glue_qqp_dir + f'/{partition_type}.tsv', sep='\\t', dtype=object)\n",
    "    glue_df = glue_df.dropna(axis=0, how='any').reset_index(drop=True)\n",
    "    glue_df_fin = pd.DataFrame({\n",
    "        'id_left': glue_df['qid1'],\n",
    "        'id_right': glue_df['qid2'],\n",
    "        'text_left': glue_df['question1'],\n",
    "        'text_right': glue_df['question2'],\n",
    "        'label': glue_df['is_duplicate'].astype(int)\n",
    "    })\n",
    "    return glue_df_fin\n",
    "\n",
    "def hadle_punctuation(inp_str: str) -> str:\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    out = regex.sub('', inp_str)\n",
    "    return out\n",
    "\n",
    "def simple_preproc(inp_str: str) -> List[str]:\n",
    "    rem_puct_str = hadle_punctuation(inp_str).lower()       \n",
    "    return nltk.word_tokenize(rem_puct_str)\n",
    "\n",
    "def tokenized_text_to_index(tokenized_text: List[str]) -> List[int]:\n",
    "    return [vocab.get(item, vocab['OOV']) for item in tokenized_text]\n",
    "\n",
    "def avg_emb(toks):\n",
    "    np.random.seed(17)\n",
    "#     dim_of_embed = len(glove_emb.get('the'))\n",
    "    zeros = np.zeros_like(glove_emb.get('the'))\n",
    "    unk_embedding = emb_matrix[\"weight\"][1].numpy() #np.random.uniform(-100, 100, dim_of_embed) \n",
    "    avg_emb = zeros #unk_embedding\n",
    "    \n",
    "    if len(toks) != 0:\n",
    "        if toks[0] is None: return None\n",
    "        for tok in toks:\n",
    "            avg_emb += glove_emb.get(tok, unk_embedding)\n",
    "        return avg_emb / len(toks) \n",
    "    else:\n",
    "        return unk_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr = get_glue_df('train')\n",
    "df_val = get_glue_df('dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_text_mapping = get_idx_to_text_mapping(df_tr)\n",
    "idx_to_text_mapping_val = get_idx_to_text_mapping(df_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sample  = np.random.choice(list(idx_to_text_mapping.keys()), 300000, replace=False)\n",
    "documents = dict([(k, idx_to_text_mapping[k]) for k in pos_sample])\n",
    "\n",
    "pos_sample_val  = np.random.choice(list(idx_to_text_mapping_val.keys()), 10, replace=False)\n",
    "query = {\"queries\": [idx_to_text_mapping_val[k] for k in pos_sample_val]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dict = {\"documents\": documents}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "query[\"queries\"].extend(['关于板球有哪些令人兴奋的事实？','计算机科学工程、软件工程和机电一体化之间有什么区别？'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'queries': ['Who might President Donald Trump appoint to their cabinet?',\n",
       "  'What is the difference between story writing and screenplay writing for movies?',\n",
       "  'What are the pros and cons of Ceasing Rs. 500 and Rs.1000 Currency Notes in India?',\n",
       "  'I have existing MR app that can run on YARN. Can I run this app on Tez on YARN without any modification? If not, will it be possible in future?',\n",
       "  'Why does my cat sing and meow at night right when I go to bed?',\n",
       "  'What is the structure of the cell membrane?',\n",
       "  'How do I earn money through online in India?',\n",
       "  'What are some good songs to make a texting lyric prank?',\n",
       "  'Why is high fructose corn syrup so bad?',\n",
       "  'How can I get more upvotes and views in Quora?',\n",
       "  '关于板球有哪些令人兴奋的事实？',\n",
       "  '计算机科学工程、软件工程和机电一体化之间有什么区别？']}"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./jl/query.json\", \"w\") as f:\n",
    "    json.dump(query,f, indent=2)\n",
    "    \n",
    "with open(\"./jl/documents.json\", \"w\") as f:\n",
    "    json.dump(doc_dict,f, indent=2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampled_emb     = np.array([(k, idx_to_text_mapping_samp[k]) for k in pos_sample],dtype=object)\n",
    "# sampled_emb_val = np.array([(k, idx_to_text_mapping_val_samp[k]) for k in pos_sample_val],dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_tokens   = np.array([(q,simple_preproc(text)) for q,text in doc_dict[\"documents\"].items()], dtype=object)\n",
    "query_tokens = np.array([simple_preproc(text) for text in query['queries']], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.72 s, sys: 52 ms, total: 1.77 s\n",
      "Wall time: 1.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_emb   = np.array([(q, avg_emb(toks)) for q,toks in doc_tokens], dtype=object)\n",
    "\n",
    "lang_check = [True if detect(text)=='en' else False for text in query['queries']]\n",
    "query_tokens = np.array([simple_preproc(text) if lang_check[i] == True else [None] \n",
    "                         for i, text in enumerate(query['queries'])], dtype=object)    \n",
    "query_emb    = np.array([avg_emb(toks) for toks in query_tokens], dtype=object)\n",
    "# query = query_emb #.astype('float32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors = np.array([emb for _,emb in doc_emb], dtype=object).astype('float32')\n",
    "len(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = np.linalg.norm(vectors, axis = 1, keepdims = True)\n",
    "norm_vec = vectors / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(norm_vec[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k = 1000\n",
    "code_size = 5\n",
    "dim = len(glove_emb.get('the'))\n",
    "quantizer = faiss.IndexFlatIP(dim) \n",
    "index = faiss.IndexIVFFlat(quantiser, dim, k)\n",
    "# index = faiss.IndexIVFPQ(quantizer, dim, k, code_size, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# index = faiss.index_factory(dim, 'IVF%s,PQ5' % str(k))\n",
    "print(index.is_trained)\n",
    "index.train(norm_vec)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "print(index.is_trained)\n",
    "print(index.ntotal)\n",
    "index.add(norm_vec)\n",
    "print(index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.ntotal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query = query_emb.astype('float32')\n",
    "len(query_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.3231325 ,  0.03252687, -0.13553125, -0.04481263,  0.35215922,\n",
       "         0.24063563, -0.47955576,  0.07188312,  0.14682372, -0.07151325,\n",
       "         0.36719375,  0.07929625, -0.04210988, -0.170715  ,  0.39910506,\n",
       "         0.50338725,  0.38878975, -0.27713   , -0.08214625, -0.189309  ,\n",
       "         0.24382862, -0.07910612,  0.05398825,  0.1336045 ,  0.24808225,\n",
       "        -1.64630375, -0.12428875, -0.22289987,  0.225227  , -0.364803  ,\n",
       "         3.0778125 ,  0.0458575 , -0.66569012, -0.50898875, -0.11025161,\n",
       "        -0.40969789,  0.0360045 ,  0.15198763, -0.012987  , -0.06125725,\n",
       "        -0.161864  , -0.18182025,  0.24411025, -0.06071225, -0.286624  ,\n",
       "         0.14335325, -0.14408114,  0.03562075, -0.15558975, -0.0684179 ]])"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(query_emb[0],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.31689   , 0.7277256 , 0.78914136, 0.8043233 , 0.8053964 ,\n",
       "         0.87429404, 0.8831511 , 0.9038441 , 0.9141702 , 0.9396117 ,\n",
       "         0.9397359 , 0.9474556 , 0.9692233 , 0.99531806, 1.0009531 ,\n",
       "         1.0317323 , 1.0401981 , 1.0459554 , 1.0464041 , 1.089001  ,\n",
       "         1.0985188 , 1.1051904 , 1.1065682 , 1.1101832 , 1.1203438 ,\n",
       "         1.1249537 , 1.1418786 , 1.1479276 , 1.1482043 , 1.1484969 ,\n",
       "         1.1485527 , 1.1502395 , 1.151597  , 1.1549463 , 1.1687927 ,\n",
       "         1.1733601 , 1.1758177 , 1.1784341 , 1.1859734 , 1.1899159 ,\n",
       "         1.1908655 , 1.2026486 , 1.2038497 , 1.2078103 , 1.2127271 ,\n",
       "         1.2155824 , 1.2184826 , 1.2184826 , 1.2223215 , 1.224473  ,\n",
       "         1.2289991 , 1.231406  , 1.235086  , 1.237471  , 1.2408818 ,\n",
       "         1.2414145 , 1.2504778 , 1.250543  , 1.25806   , 1.2597885 ,\n",
       "         1.2612245 , 1.2642522 , 1.2655798 , 1.2746127 , 1.2747462 ,\n",
       "         1.2767888 , 1.2781448 , 1.2786742 , 1.2787199 , 1.2791548 ,\n",
       "         1.2823589 , 1.2868885 , 1.2889705 , 1.2913146 , 1.2915255 ,\n",
       "         1.2953235 , 1.2964617 , 1.2964617 , 1.2966778 , 1.2969761 ,\n",
       "         1.2979493 , 1.3001583 , 1.3021234 , 1.3022466 , 1.3043581 ,\n",
       "         1.3045688 , 1.3073813 , 1.3079123 , 1.3097985 , 1.3141171 ,\n",
       "         1.3175198 , 1.318161  , 1.3204702 , 1.321149  , 1.3229848 ,\n",
       "         1.3252857 , 1.3313326 , 1.3316183 , 1.333126  , 1.3338422 ]],\n",
       "       dtype=float32),\n",
       " array([[ 21343, 146432, 167779, 168010, 153698, 193893,  44764,  78924,\n",
       "          53238,  36922, 135034,  29964,  94463,  90599,  62530,  68625,\n",
       "         161681,  44470, 198617, 187519,  11136,  64153,  69666,   2373,\n",
       "         162219, 181893, 178501,  81179, 126848, 116370,  29756, 196378,\n",
       "          33105, 186178, 190138,  70805,  32603,  76113,  78483, 127822,\n",
       "          86162,   1213,  99491, 105718,  45075,  82038, 170790,  82558,\n",
       "          92095,  75072, 107418,  24788, 181156,  73612, 119796,  17869,\n",
       "          94581, 117919,  31070, 182326,  80698, 104201, 104104, 192767,\n",
       "         180487,  16077, 124809,  74604,   2824,  83322,  59060, 165150,\n",
       "         176276,   6364, 128061, 163323,  49157,  64924, 118826,  39607,\n",
       "         149595,  49240,  12472,  30597,  39142,  78179, 174422, 118653,\n",
       "          43546, 111720, 197434,  90944, 156278,  12102, 170477, 198160,\n",
       "          88269, 159297,  26752,  18665]]))"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.nprobe = 10\n",
    "topn = 100\n",
    "D, I = index.search(np.expand_dims(query_emb[i],axis=0).astype(\"float32\"), topn) \n",
    "D, I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# indx = I[(I>-1)]\n",
    "# toks_to_idx = [tokenized_text_to_index(tok) for _,tok in doc_tokens[indx]]\n",
    "# padded_toks_to_idx = collate_fn(toks_to_idx, name = 'document')\n",
    "\n",
    "# toks_to_idx = [tokenized_text_to_index(tok) for tok in np.expand_dims(query_tokens[i],axis=0)]*topn\n",
    "# padded_toks_to_idx.update(collate_fn(toks_to_idx, name = 'query'))\n",
    "\n",
    "# preds = knrm.predict(padded_toks_to_idx)\n",
    "# ind_preds = preds.squeeze(1).sort(descending=True)[1].tolist()\n",
    "# rel_doc_ids = [k for k, toks in doc_tokens[indx][ind_preds]]\n",
    "\n",
    "# res = [tuple([k,text]) for k,text in doc_dict[\"documents\"].items() if k in rel_doc_ids][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([100, 14]) torch.Size([100, 59])\n",
      "1\n",
      "torch.Size([85, 12]) torch.Size([85, 28])\n",
      "2\n",
      "torch.Size([72, 9]) torch.Size([72, 30])\n",
      "3\n",
      "torch.Size([89, 13]) torch.Size([89, 40])\n",
      "4\n",
      "torch.Size([95, 20]) torch.Size([95, 37])\n",
      "5\n",
      "torch.Size([58, 7]) torch.Size([58, 24])\n",
      "6\n",
      "torch.Size([88, 8]) torch.Size([88, 30])\n",
      "7\n",
      "torch.Size([100, 12]) torch.Size([100, 57])\n",
      "9\n",
      "torch.Size([66, 20]) torch.Size([66, 37])\n"
     ]
    }
   ],
   "source": [
    "fin_rel = []\n",
    "lang_check = [True if detect(text)=='en' else False for text in query['queries']]\n",
    "query_tokens = np.array([simple_preproc(text) if lang_check[i] == True else [None] \n",
    "                         for i, text in enumerate(query['queries'])], dtype=object)    \n",
    "query_emb    = np.array([avg_emb(toks)  if len(toks) >= 2 else None for toks in query_tokens], dtype=object)\n",
    "#         query = query_emb.astype('float32')   \n",
    "\n",
    "for i,q in enumerate(query_emb):\n",
    "    if q is not None:  \n",
    "        print(i)\n",
    "        index.nprobe = 5\n",
    "        topn = 100 #index.ntotal\n",
    "        D, I = index.search(np.expand_dims(q,axis=0).astype(\"float32\"), topn)      \n",
    "        indx = I[(I>-1)]\n",
    "        toks_to_idx = [tokenized_text_to_index(tok) for _,tok in doc_tokens[indx]]\n",
    "        padded_toks_to_idx = collate_fn(toks_to_idx, name = 'document')\n",
    "    \n",
    "        toks_to_idx = [tokenized_text_to_index(tok) for tok in np.expand_dims(query_tokens[i],axis=0)]*len(indx)\n",
    "        padded_toks_to_idx.update(collate_fn(toks_to_idx, name = 'query'))\n",
    "\n",
    "        print(padded_toks_to_idx[\"query\"].shape, padded_toks_to_idx[\"document\"].shape)\n",
    "                \n",
    "        preds = knrm.predict(padded_toks_to_idx)\n",
    "        ind_preds = preds.squeeze(1).sort(descending=True)[1].tolist()\n",
    "        rel_doc_ids = [k for k, toks in doc_tokens[indx][ind_preds]]\n",
    "\n",
    "        res = [tuple([k,text]) for k,text in doc_dict[\"documents\"].items() if k in rel_doc_ids][:10]\n",
    "        fin_rel.append(res)\n",
    "    else:\n",
    "        fin_rel.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([85, 28])"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_toks_to_idx[\"document\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
