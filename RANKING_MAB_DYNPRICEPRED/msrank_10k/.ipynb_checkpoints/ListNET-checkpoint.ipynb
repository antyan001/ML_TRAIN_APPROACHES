{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/sbc_dev/.local/lib/python3.7/site-packages',\n",
       " '/mnt/data/sbc_dev/notebooks/LMS/msrank_10k',\n",
       " '/usr/local/lib/python37.zip',\n",
       " '/usr/local/lib/python3.7',\n",
       " '/usr/local/lib/python3.7/lib-dynload',\n",
       " '',\n",
       " '/home/sbc_dev/.local/lib/python3.7/site-packages',\n",
       " '/usr/local/lib/python3.7/site-packages',\n",
       " '/home/sbc_dev/.local/lib/python3.7/site-packages/IPython/extensions',\n",
       " '/home/sbc_dev/.ipython']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 22.3 MB 11.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy>=1.13.3\n",
      "  Using cached numpy-1.21.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n",
      "\u001b[K     |████████████████████████████████| 303 kB 55.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.2.0-py3-none-any.whl (12 kB)\n",
      "Collecting scipy>=0.19.1\n",
      "  Using cached scipy-1.7.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.5 MB)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1316 sha256=866f10c7365fcb31bf4427108d93ee4c1d304ed0ceb9ab2a4e5fa11bcbbc670c\n",
      "  Stored in directory: /home/sbc_dev/.cache/pip/wheels/46/ef/c3/157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\n",
      "Successfully built sklearn\n",
      "\u001b[31mERROR: deeppavlov 0.8.0 has requirement numpy==1.18.0, but you'll have numpy 1.21.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: deeppavlov 0.8.0 has requirement pandas==0.25.3, but you'll have pandas 1.3.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: deeppavlov 0.8.0 has requirement requests==2.22.0, but you'll have requests 2.24.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: deeppavlov 0.8.0 has requirement scikit-learn==0.21.2, but you'll have scikit-learn 0.24.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: deeppavlov 0.8.0 has requirement scipy==1.4.1, but you'll have scipy 1.7.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: numpy, joblib, threadpoolctl, scipy, scikit-learn, sklearn\n",
      "Successfully installed joblib-1.0.1 numpy-1.21.0 scikit-learn-0.24.2 scipy-1.7.0 sklearn-0.0 threadpoolctl-2.2.0\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3.7 install -U sklearn -t /home/sbc_dev/.local/lib/python3.7/site-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from math import log2\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from catboost.datasets import msrank_10k\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "\n",
    "from typing import List, Union, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListNet(torch.nn.Module):\n",
    "    def __init__(self, num_input_features: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.number_of_classes = 1\n",
    "\n",
    "        fc_layers = nn.ModuleList([\n",
    "        nn.Sequential(nn.Linear(num_input_features, self.hidden_dim),\n",
    "                      nn.Dropout(0.1), nn.ReLU()),\n",
    "#         nn.Sequential(nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "#                       nn.Dropout(0.1), nn.ReLU()),\n",
    "#         nn.Sequential(nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "#                       nn.Dropout(0.1), nn.ReLU()),            \n",
    "        nn.Sequential(nn.Linear(self.hidden_dim, self.number_of_classes))\n",
    "        ])\n",
    "        self.fc_layers = fc_layers\n",
    "        \n",
    "    def _get_nn_output(self, shape: tuple) -> Union[torch.Tensor, torch.Size]:\n",
    "\n",
    "        input  = torch.rand(shape) \n",
    "        output = input\n",
    "        # forward pass through conv layers\n",
    "        for i in range(len(self.fc_layers)):\n",
    "            output = self.fc_layers[i](output)\n",
    "            print('nn({0}) shape:{1}'.format(i,np.shape(output)))\n",
    "                \n",
    "        output = output.view(output.size(0), -1)\n",
    "        print('output reshape: {}'.format(np.shape(output)))\n",
    "        n_size = output.size(1)\n",
    "\n",
    "        return output, n_size        \n",
    "        \n",
    "    def forward(self, input_1: torch.Tensor) -> torch.Tensor:\n",
    "        # forward pass through fc layers\n",
    "        output = input_1\n",
    "        for i in range(len(self.fc_layers)):\n",
    "            output = self.fc_layers[i](output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solution:\n",
    "    def __init__(self, n_epochs: int = 5, listnet_hidden_dim: int = 30,\n",
    "                 lr: float = 0.001, ndcg_top_k: int = 10):\n",
    "        \n",
    "        self.eps = 1.e-10        \n",
    "        self.std_sc = StandardScaler()\n",
    "        self.max_sc = MaxAbsScaler() \n",
    "        \n",
    "        self._prepare_data()\n",
    "        self.listnet_hidden_dim = listnet_hidden_dim\n",
    "        self.num_input_features = self.X_train.shape[1]\n",
    "        self.number_of_classes = len(set(self.ys_train))\n",
    "        self.ndcg_top_k = ndcg_top_k\n",
    "        self.n_epochs = n_epochs\n",
    "        self.gain_scheme = 'exp2'\n",
    "\n",
    "        self.model = self._create_model(\n",
    "            self.num_input_features, self.listnet_hidden_dim)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "    def _groups_count_vectorizer(self, inp_query_ids: np.ndarray) -> Dict:\n",
    "        uniq_indx={}\n",
    "        for ind in inp_query_ids:\n",
    "            if ind not in uniq_indx:\n",
    "                uniq_indx[ind]=1\n",
    "            else:\n",
    "                uniq_indx[ind]+=1\n",
    "                \n",
    "        return uniq_indx\n",
    "        \n",
    "    def _get_data(self) -> List[np.ndarray]:\n",
    "        train_df, test_df = msrank_10k()\n",
    "\n",
    "        X_train = train_df.drop([0, 1], axis=1).values\n",
    "        y_train = train_df[0].values\n",
    "        query_ids_train = train_df[1].values.astype(int)\n",
    "\n",
    "        X_test = test_df.drop([0, 1], axis=1).values\n",
    "        y_test = test_df[0].values\n",
    "        query_ids_test = test_df[1].values.astype(int)\n",
    "\n",
    "        return [X_train, y_train, query_ids_train, X_test, y_test, query_ids_test]\n",
    "\n",
    "    def _prepare_data(self) -> None:\n",
    "        (X_train, y_train, self.query_ids_train,\n",
    "            X_test, y_test, self.query_ids_test) = self._get_data()\n",
    "        \n",
    "        self.X_train = torch.FloatTensor(self._scale_features_in_query_groups(X_train, \n",
    "                                                            self.query_ids_train))\n",
    "        self.X_test  = torch.FloatTensor(self._scale_features_in_query_groups(X_test, \n",
    "                                                            self.query_ids_test))\n",
    "        \n",
    "        self.ys_train = torch.FloatTensor(y_train)\n",
    "        self.ys_test  = torch.FloatTensor(y_test)\n",
    "        \n",
    "        \n",
    "    def _scale_features_in_query_groups(self, inp_feat_array: np.ndarray,\n",
    "                                        inp_query_ids: np.ndarray) -> np.ndarray:\n",
    "      \n",
    "        uniq_indx =self._groups_count_vectorizer(inp_query_ids)\n",
    "\n",
    "        shift = 0\n",
    "        for ind, cnt in uniq_indx.items():\n",
    "\n",
    "            sample = inp_feat_array[shift: shift+cnt]\n",
    "            std_lst = sample.std(axis=0)\n",
    "            const_cols = np.where(abs(std_lst) <= self.eps)[0]\n",
    "\n",
    "            mask = np.zeros(sample.shape[-1], dtype=bool)\n",
    "            mask[const_cols] = True   \n",
    "            sample[:, mask] = self.max_sc.fit_transform(sample[:,mask])\n",
    "\n",
    "            if len(const_cols) < sample.shape[-1]:\n",
    "                mask = np.ones(sample.shape[-1], dtype=bool)\n",
    "                mask[const_cols] = False  \n",
    "                sample[:, mask] = self.std_sc.fit_transform(sample[:,mask])\n",
    "\n",
    "            inp_feat_array[shift: shift+cnt] = sample\n",
    "            \n",
    "            shift+=cnt\n",
    "                                                            \n",
    "        return inp_feat_array \n",
    " \n",
    "\n",
    "    def _create_model(self, listnet_num_input_features: int,\n",
    "                      listnet_hidden_dim: int) -> torch.nn.Module:\n",
    "        torch.manual_seed(0)\n",
    "        net = ListNet(listnet_num_input_features, listnet_hidden_dim)\n",
    "        return net        \n",
    "    \n",
    "    def compute_gain(self, y_value: float, gain_scheme: str) -> float:\n",
    "        if gain_scheme == 'const':\n",
    "            return y_value\n",
    "        elif gain_scheme == 'exp2':\n",
    "            return 2.**y_value - 1.\n",
    "        else:\n",
    "            return y_value\n",
    "    \n",
    "    def dcg(self, ys_true: torch.Tensor, ys_pred: torch.Tensor,\n",
    "                ndcg_top_k: int, gain_scheme: str) -> float:\n",
    " \n",
    "        input_shape = ys_pred.shape[0]\n",
    "        cat = torch.hstack((ys_pred,ys_true))\n",
    "        t_sorted = cat[cat[:, 0].sort(descending=True)[1]]\n",
    "    #     t_sorted = t_sorted.unsqueeze(0)\n",
    "\n",
    "        input_tensor = t_sorted[:ndcg_top_k].numpy()\n",
    "\n",
    "        factors = torch.Tensor([\n",
    "                                self.compute_gain(y_value = float(input_tensor[i,1]), \n",
    "                                             gain_scheme = gain_scheme)/log2(i+2)\n",
    "                                for i in range(input_tensor.shape[0])\n",
    "                               ]\n",
    "                              ).type(torch.float64)\n",
    "\n",
    "        return float(torch.cumsum(factors,dim=0)[-1].numpy())    \n",
    "\n",
    "    def _ndcg_k(self, ys_true: torch.Tensor, ys_pred: torch.Tensor,\n",
    "                ndcg_top_k: int) -> float:\n",
    "            \n",
    "        _dcg = self.dcg(ys_true, ys_pred, ndcg_top_k, self.gain_scheme)\n",
    "\n",
    "        cat = torch.hstack((ys_pred,ys_true))\n",
    "        t_sorted = cat[cat[:, 1].sort(descending=True)[1]]\n",
    "\n",
    "        input_tensor = t_sorted[:ndcg_top_k].numpy()\n",
    "\n",
    "        factors = torch.Tensor([\n",
    "                                self.compute_gain(y_value = input_tensor[i,1], \n",
    "                                             gain_scheme = self.gain_scheme)/log2(i+2)\n",
    "                                for i in range(input_tensor.shape[0])\n",
    "                               ]\n",
    "                              )    \n",
    "\n",
    "        idcg = torch.cumsum(factors,dim=0)[-1].numpy() \n",
    "\n",
    "        if idcg != 0.:\n",
    "            res = _dcg/idcg\n",
    "        else:\n",
    "            res= 0.\n",
    "\n",
    "        return res      \n",
    "    \n",
    "    def _calc_loss(self, batch_ys: torch.FloatTensor,\n",
    "                   batch_pred: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        \n",
    "        eps = 1.e-14\n",
    "        y_pred = batch_pred.clone()\n",
    "        y_true = batch_ys.clone()\n",
    "\n",
    "#         mask = y_true == padded_value_indicator\n",
    "#         y_pred[mask] = float('-inf')\n",
    "#         y_true[mask] = float('-inf')\n",
    "\n",
    "        preds_smax = F.softmax(y_pred, dim=0)\n",
    "        true_smax = F.softmax(y_true, dim=0)\n",
    "\n",
    "        preds_smax = preds_smax + eps\n",
    "        preds_log = torch.log(preds_smax)\n",
    "\n",
    "        return -torch.sum(true_smax * preds_log, dim=0)   \n",
    "    \n",
    "    def fit(self) -> List[float]:\n",
    "        \n",
    "        mean_ndcg_arr = []\n",
    "        \n",
    "        for epoch in range(self.n_epochs):\n",
    "\n",
    "            print(\"## epoch -> {}\".format(epoch))\n",
    "            self._train_one_epoch()\n",
    "            curr_ndcg = self._eval_test_set()\n",
    "            mean_ndcg_arr.append(curr_ndcg)\n",
    "            \n",
    "        return mean_ndcg_arr\n",
    "\n",
    "\n",
    "    def _train_one_epoch(self) -> None:\n",
    "        \n",
    "        uniq_indx = uniq_indx = self._groups_count_vectorizer(self.query_ids_train)\n",
    "\n",
    "        shift = 0\n",
    "        for ind, cnt in uniq_indx.items():\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            batch_x = self.X_train[shift: shift+cnt]\n",
    "            batch_y = self.ys_train[shift: shift+cnt]\n",
    "                        \n",
    "            # perform rows shuffling within one batch\n",
    "            idx = torch.randperm(batch_x.shape[0])\n",
    "            batch_x = batch_x[idx]\n",
    "            batch_y = batch_y[idx].view(-1,1)            \n",
    "            \n",
    "            batch_pred = self.model(batch_x)\n",
    "            \n",
    "            batch_loss = self._calc_loss(batch_y, batch_pred)\n",
    "            #print(\"## group num: {}; loss val: {}\".format(ind, batch_loss))\n",
    "            batch_loss.backward(retain_graph=False)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            shift+=cnt\n",
    "             \n",
    "    def _eval_test_set(self) -> float:\n",
    "        \n",
    "        uniq_indx = uniq_indx = self._groups_count_vectorizer(self.query_ids_test)\n",
    "        with torch.no_grad(): \n",
    "            \n",
    "            shift = 0\n",
    "            ndcgs = []\n",
    "            for ind, cnt in uniq_indx.items():\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                batch_x = self.X_test[shift: shift+cnt]\n",
    "                batch_y = self.ys_test[shift: shift+cnt].view(-1,1)\n",
    "                valid_pred = self.model(batch_x)                 \n",
    "                ndcg_score = self._ndcg_k(batch_y, valid_pred, self.ndcg_top_k)            \n",
    "                ndcgs.append(ndcg_score)\n",
    "\n",
    "                shift+=cnt\n",
    "            \n",
    "            res = np.mean(ndcgs)\n",
    "            print(\"## mean ndcg per group: {}\".format(res))\n",
    "                \n",
    "            return res    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sol = Solution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## epoch -> 0\n",
      "## mean ndcg per group: 0.40425502109438227\n",
      "## epoch -> 1\n",
      "## mean ndcg per group: 0.43030293840910244\n",
      "## epoch -> 2\n",
      "## mean ndcg per group: 0.4294680735740625\n",
      "## epoch -> 3\n",
      "## mean ndcg per group: 0.4331150825476493\n",
      "## epoch -> 4\n",
      "## mean ndcg per group: 0.4328602619791688\n"
     ]
    }
   ],
   "source": [
    "out = sol.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = msrank_10k()\n",
    "\n",
    "X_train = train_df.drop([0, 1], axis=1).values\n",
    "y_train = train_df[0].values\n",
    "query_ids_train = train_df[1].values.astype(int)\n",
    "\n",
    "X_test = test_df.drop([0, 1], axis=1).values\n",
    "y_test = test_df[0].values\n",
    "query_ids_test = test_df[1].values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3., 4.])"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 138)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>62</td>\n",
       "      <td>11089534</td>\n",
       "      <td>2</td>\n",
       "      <td>116</td>\n",
       "      <td>64034</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>54</td>\n",
       "      <td>11089534</td>\n",
       "      <td>2</td>\n",
       "      <td>124</td>\n",
       "      <td>64034</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>124</td>\n",
       "      <td>3344</td>\n",
       "      <td>14</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>56</td>\n",
       "      <td>11089534</td>\n",
       "      <td>13</td>\n",
       "      <td>123</td>\n",
       "      <td>63933</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>256</td>\n",
       "      <td>49697</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>62</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>210</td>\n",
       "      <td>49923</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>62</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>256</td>\n",
       "      <td>50023</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>115</td>\n",
       "      <td>63318</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>51</td>\n",
       "      <td>11089534</td>\n",
       "      <td>15</td>\n",
       "      <td>124</td>\n",
       "      <td>63363</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>541</td>\n",
       "      <td>63363</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>44</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>16711</td>\n",
       "      <td>61690</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>66</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>220</td>\n",
       "      <td>48022</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>220</td>\n",
       "      <td>48022</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1322</td>\n",
       "      <td>11454</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>208</td>\n",
       "      <td>48022</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>10122</td>\n",
       "      <td>65535</td>\n",
       "      <td>57</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1049</td>\n",
       "      <td>60996</td>\n",
       "      <td>79</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>46</td>\n",
       "      <td>222</td>\n",
       "      <td>0</td>\n",
       "      <td>1066</td>\n",
       "      <td>203</td>\n",
       "      <td>11</td>\n",
       "      <td>71</td>\n",
       "      <td>3576</td>\n",
       "      <td>1037</td>\n",
       "      <td>98.218976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3492</td>\n",
       "      <td>65535</td>\n",
       "      <td>101</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>280</td>\n",
       "      <td>20296</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 138 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1    2    3    4    5    6    7         8         9    ...  128  \\\n",
       "0   2.0    1    3    3    0    0    3  1.0  1.000000  0.000000  ...   62   \n",
       "1   2.0    1    3    0    3    0    3  1.0  0.000000  1.000000  ...   54   \n",
       "2   0.0    1    3    0    2    0    3  1.0  0.000000  0.666667  ...   45   \n",
       "3   2.0    1    3    0    3    0    3  1.0  0.000000  1.000000  ...   56   \n",
       "4   1.0    1    3    0    3    0    3  1.0  0.000000  1.000000  ...   64   \n",
       "5   1.0    1    3    0    3    0    3  1.0  0.000000  1.000000  ...   62   \n",
       "6   1.0    1    3    0    3    0    3  1.0  0.000000  1.000000  ...   62   \n",
       "7   2.0    1    3    0    3    0    3  1.0  0.000000  1.000000  ...   73   \n",
       "8   1.0    1    3    0    3    0    3  1.0  0.000000  1.000000  ...   51   \n",
       "9   0.0    1    3    0    3    0    3  1.0  0.000000  1.000000  ...   48   \n",
       "10  0.0    1    3    0    3    0    3  1.0  0.000000  1.000000  ...   44   \n",
       "11  0.0    1    3    1    3    0    3  1.0  0.333333  1.000000  ...   66   \n",
       "12  0.0    1    3    0    3    0    3  1.0  0.000000  1.000000  ...   55   \n",
       "13  0.0    1    3    0    3    0    3  1.0  0.000000  1.000000  ...   25   \n",
       "14  0.0    1    3    0    3    0    3  1.0  0.000000  1.000000  ...   53   \n",
       "15  0.0    1    3    0    2    2    3  1.0  0.000000  0.666667  ...   72   \n",
       "16  0.0    1    0    0    0    0    0  0.0  0.000000  0.000000  ...   62   \n",
       "17  2.0    1    3    0    3    0    3  1.0  0.000000  1.000000  ...   46   \n",
       "18  0.0    1    3    0    2    2    3  1.0  0.000000  0.666667  ...   78   \n",
       "19  0.0    1    3    0    0    0    3  1.0  0.000000  0.000000  ...   65   \n",
       "\n",
       "         129  130    131    132  133  134   135   136        137  \n",
       "0   11089534    2    116  64034   13    3     0     0   0.000000  \n",
       "1   11089534    2    124  64034    1    2     0     0   0.000000  \n",
       "2          3    1    124   3344   14   67     0     0   0.000000  \n",
       "3   11089534   13    123  63933    1    3     0     0   0.000000  \n",
       "4          5    7    256  49697    1   13     0     0   0.000000  \n",
       "5          6    7    210  49923    5   15     0     0   0.000000  \n",
       "6          6    7    256  50023    3   14     0     0   0.000000  \n",
       "7          0   17    115  63318    1    3     0     0   0.000000  \n",
       "8   11089534   15    124  63363    1    3     0     0   0.000000  \n",
       "9          1    1    541  63363    1    5     0     0   0.000000  \n",
       "10         5   30  16711  61690    3    4     0     0   0.000000  \n",
       "11         6    7    220  48022    7   18     0     0   0.000000  \n",
       "12         6    7    220  48022    1   13     0     0   0.000000  \n",
       "13         0    0   1322  11454    1    4     0     0   0.000000  \n",
       "14         0    1    208  48022    2   13     0     0   0.000000  \n",
       "15         0    5  10122  65535   57   35     0     0   0.000000  \n",
       "16         1    0   1049  60996   79    9     0     0   0.000000  \n",
       "17       222    0   1066    203   11   71  3576  1037  98.218976  \n",
       "18         0    5   3492  65535  101   56     0     0   0.000000  \n",
       "19         0    0    280  20296   16    2     0     0   0.000000  \n",
       "\n",
       "[20 rows x 138 columns]"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_sc = StandardScaler()\n",
    "max_sc = MaxAbsScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 86,\n",
       " 16: 106,\n",
       " 31: 92,\n",
       " 46: 120,\n",
       " 61: 59,\n",
       " 76: 45,\n",
       " 91: 74,\n",
       " 106: 23,\n",
       " 121: 54,\n",
       " 136: 172,\n",
       " 151: 124,\n",
       " 166: 77,\n",
       " 181: 77,\n",
       " 196: 308,\n",
       " 211: 95,\n",
       " 226: 126,\n",
       " 241: 105,\n",
       " 256: 227,\n",
       " 271: 81,\n",
       " 286: 18,\n",
       " 301: 61,\n",
       " 316: 95,\n",
       " 331: 91,\n",
       " 346: 97,\n",
       " 361: 81,\n",
       " 376: 156,\n",
       " 391: 51,\n",
       " 406: 196,\n",
       " 421: 224,\n",
       " 436: 122,\n",
       " 451: 56,\n",
       " 466: 57,\n",
       " 481: 152,\n",
       " 496: 89,\n",
       " 511: 78,\n",
       " 526: 141,\n",
       " 541: 233,\n",
       " 556: 230,\n",
       " 571: 200,\n",
       " 586: 117,\n",
       " 601: 75,\n",
       " 616: 284,\n",
       " 631: 133,\n",
       " 646: 116,\n",
       " 661: 123,\n",
       " 676: 163,\n",
       " 691: 304,\n",
       " 706: 67,\n",
       " 721: 137,\n",
       " 736: 89,\n",
       " 751: 18,\n",
       " 766: 206,\n",
       " 781: 57,\n",
       " 796: 70,\n",
       " 811: 76,\n",
       " 826: 59,\n",
       " 841: 142,\n",
       " 856: 87,\n",
       " 871: 139,\n",
       " 886: 46,\n",
       " 901: 280,\n",
       " 916: 20,\n",
       " 931: 124,\n",
       " 946: 85,\n",
       " 961: 108,\n",
       " 976: 90,\n",
       " 991: 115,\n",
       " 1006: 174,\n",
       " 1021: 175,\n",
       " 1036: 103,\n",
       " 1051: 128,\n",
       " 1066: 77,\n",
       " 1081: 129,\n",
       " 1096: 103,\n",
       " 1111: 83,\n",
       " 1126: 165,\n",
       " 1141: 67,\n",
       " 1156: 146,\n",
       " 1171: 54,\n",
       " 1186: 77,\n",
       " 1201: 67,\n",
       " 1216: 71,\n",
       " 1231: 156,\n",
       " 1246: 133,\n",
       " 1261: 107,\n",
       " 1276: 52,\n",
       " 1291: 124}"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniq_indx={}\n",
    "for ind in query_ids_train:\n",
    "    if ind not in uniq_indx:\n",
    "        uniq_indx[ind]=1\n",
    "    else:\n",
    "        uniq_indx[ind]+=1\n",
    "uniq_indx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([v for k,v in uniq_indx.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eps = 1.e-10\n",
    "shift = 0\n",
    "for ind, cnt in uniq_indx.items():\n",
    "    \n",
    "    sample = X_train[shift: shift+cnt]\n",
    "    std_lst = sample.std(axis=0)\n",
    "    const_cols = np.where(abs(std_lst) <= eps)[0]\n",
    "    \n",
    "    mask = np.zeros(sample.shape[-1], dtype=bool)\n",
    "    mask[const_cols] = True   \n",
    "    sample[:, mask] = max_sc.fit_transform(sample[:,mask])\n",
    "    \n",
    "    if len(const_cols) < sample.shape[-1]:\n",
    "        mask = np.ones(sample.shape[-1], dtype=bool)\n",
    "        mask[const_cols] = False  \n",
    "        sample[:, mask] = std_sc.fit_transform(sample[:,mask])\n",
    "    \n",
    "    X_train[shift: shift+cnt] = sample\n",
    "    shift+=cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.74279196e-17,  1.35550486e-17,  2.32372261e-17, -2.19462691e-17,\n",
       "        1.74279196e-17, -2.84010541e-17,  4.13106242e-17, -4.51834952e-17,\n",
       "        2.58191401e-18, -2.84010541e-17,  1.93643551e-17,  4.51834952e-18,\n",
       "        4.51834952e-18,  2.45281831e-17,  2.58191401e-18,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        5.80930652e-18, -3.09829681e-17,  6.45478503e-19,  1.67824411e-17,\n",
       "       -1.42005271e-17, -2.87237934e-17,  7.74574203e-18,  4.13106242e-17,\n",
       "       -2.58191401e-18,  8.51224775e-18, -1.93643551e-17, -4.13106242e-17,\n",
       "        2.25917476e-18, -4.77654092e-17, -3.55013176e-18, -6.45478503e-18,\n",
       "        2.25917476e-17, -9.35943829e-18, -2.58191401e-17, -6.45478503e-18,\n",
       "        1.93643551e-18,  1.61369626e-17, -1.45232663e-18,  1.29095701e-18,\n",
       "       -2.74328364e-18,  4.84108877e-18, -3.74377532e-17, -1.09731345e-17,\n",
       "       -2.45281831e-17, -7.10026353e-18, -1.03276560e-17, -6.26114148e-17,\n",
       "       -1.12958738e-18, -7.10026353e-18, -3.35648821e-17,  6.45478503e-19,\n",
       "       -4.51834952e-18,  3.61467962e-17,  1.67824411e-17,  1.68631259e-17,\n",
       "        2.58191401e-18, -3.87287102e-17,  1.61369626e-17, -3.87287102e-18,\n",
       "       -5.16382802e-18,  1.29095701e-18,  1.35550486e-17,  4.51834952e-18,\n",
       "       -1.09731345e-17,  1.93643551e-18,  6.45478503e-18, -5.80930652e-18,\n",
       "        2.90465326e-18,  1.16186130e-17, -7.74574203e-18,  0.00000000e+00,\n",
       "        3.87287102e-17, -1.80733981e-17, -2.58191401e-18, -1.25868308e-17,\n",
       "       -5.80930652e-18, -3.16284466e-17, -7.74574203e-18, -1.03276560e-17,\n",
       "        5.80930652e-18, -7.10026353e-18, -6.45478503e-18, -8.55259016e-18,\n",
       "        7.74574203e-18, -1.71051803e-17,  3.22739251e-18, -1.54914841e-17,\n",
       "        1.77506588e-18, -1.03276560e-17,  5.64793690e-18, -1.93643551e-17,\n",
       "       -1.03276560e-17, -2.84010541e-17, -2.58191401e-18, -1.93643551e-17,\n",
       "        7.87483773e-17, -5.68021082e-17,  7.10026353e-17,  5.16382802e-18,\n",
       "       -1.16186130e-17, -6.45478503e-18, -1.80733981e-17,  1.45232663e-18,\n",
       "       -3.87287102e-18,  1.29095701e-18,  1.93643551e-17,  3.42103606e-17,\n",
       "       -3.87287102e-17, -5.16382802e-18,  2.58191401e-17,  7.42300278e-18,\n",
       "        4.51834952e-18,  1.54914841e-17, -1.80733981e-17,  1.47653207e-17,\n",
       "        1.16186130e-17, -5.74475867e-17, -3.22739251e-17, -1.03276560e-17,\n",
       "        1.29095701e-17, -7.22935923e-17, -7.42300278e-18,  3.67922747e-17,\n",
       "       -5.16382802e-18, -5.16382802e-17, -7.74574203e-18, -2.13007906e-17,\n",
       "        2.32372261e-17, -5.16382802e-18, -1.90416158e-17,  5.80930652e-18])"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means = sample.mean(axis=0)\n",
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 15,  19,  49,  50,  55,  60,  70,  75,  79,  85,  89,  90,  91,\n",
       "        109, 110, 113, 114, 115, 117, 119, 120, 121, 122]),)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(means==-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.926551, 6.926551, 6.926551, 6.926551, 6.926551, 6.926551,\n",
       "       6.926551, 6.926551, 6.926551, 6.926551, 6.926551, 6.926551,\n",
       "       6.926551, 6.926551, 6.926551, 6.926551, 6.926551, 6.926551,\n",
       "       6.926551, 6.926551, 6.926551, 6.926551, 6.926551, 6.926551,\n",
       "       6.926551, 6.926551, 6.926551, 6.926551, 6.926551, 6.926551,\n",
       "       6.926551, 6.926551, 6.926551, 6.926551, 6.926551, 6.926551,\n",
       "       6.926551, 6.926551, 6.926551, 6.926551, 6.926551, 6.926551,\n",
       "       6.926551, 6.926551, 6.926551, 6.926551, 6.926551, 6.926551,\n",
       "       6.926551, 6.926551, 6.926551, 6.926551, 6.926551, 6.926551,\n",
       "       6.926551, 6.926551, 6.926551, 6.926551, 6.926551, 6.926551,\n",
       "       6.926551, 6.926551, 6.926551, 6.926551, 6.926551, 6.926551,\n",
       "       6.926551, 6.926551, 6.926551, 6.926551, 6.926551, 6.926551,\n",
       "       6.926551, 6.926551, 6.926551, 6.926551, 6.926551, 6.926551,\n",
       "       6.926551, 6.926551, 6.926551, 6.926551, 6.926551, 6.926551,\n",
       "       6.926551, 6.926551])"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref = X_train[query_ids_train[query_ids_train==1] - 1,19]\n",
    "ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1.])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[:,16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = train_df.groupby([1,]).agg({1:'count'}).rename(columns={1:'cnt'}) \\\n",
    "              .sort_values(by='cnt',ascending=False).reset_index().rename(columns={1:'ids'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>691</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>616</td>\n",
       "      <td>284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>901</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>541</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>556</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>256</td>\n",
       "      <td>227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>421</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>766</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>571</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>406</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1021</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1006</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>136</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1126</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>676</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>376</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1231</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>481</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1156</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ids  cnt\n",
       "0    196  308\n",
       "1    691  304\n",
       "2    616  284\n",
       "3    901  280\n",
       "4    541  233\n",
       "5    556  230\n",
       "6    256  227\n",
       "7    421  224\n",
       "8    766  206\n",
       "9    571  200\n",
       "10   406  196\n",
       "11  1021  175\n",
       "12  1006  174\n",
       "13   136  172\n",
       "14  1126  165\n",
       "15   676  163\n",
       "16   376  156\n",
       "17  1231  156\n",
       "18   481  152\n",
       "19  1156  146"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_37_env",
   "language": "python",
   "name": "py_37_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
