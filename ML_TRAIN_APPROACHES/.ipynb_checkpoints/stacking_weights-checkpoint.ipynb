{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/ektov-av/python35-libs/lib/python3.5/site-packages/') \n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score\n",
    "import re, os, time, sys, datetime, csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.externals import joblib\n",
    "import xgboost.compat\n",
    "import xgboost as xgb\n",
    "import lightgbm as gbm\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV  \n",
    "\n",
    "import sklearn.cross_validation\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score,roc_curve\n",
    "from sklearn.decomposition import PCA\n",
    "import datetime\n",
    "import cx_Oracle\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as mplt\n",
    "from sklearn import preprocessing\n",
    "import re\n",
    "from sklearn.cross_validation import cross_val_score, KFold\n",
    "from sklearn.cross_validation import train_test_split, cross_val_predict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, \\\n",
    "                            roc_curve, precision_recall_curve, average_precision_score, f1_score, \\\n",
    "                            classification_report, average_precision_score, confusion_matrix \n",
    "from sklearn.preprocessing import normalize, binarize\n",
    "\n",
    "from label_encoder import SoftLabelEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from support_library.plot import Plot as plt\n",
    "from support_library.feature_eng import Featureng as fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_path(folder, file):\n",
    "    return os.path.join(folder, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def getboostparams(model_name, X_trn, y_trn, X_tst, y_tst, weights):\n",
    "    switcher={\n",
    "        LGBMClassifier:\n",
    "                        {'params': \n",
    "                                    {\n",
    "                                        'n_estimators':10000, \n",
    "                                        'num_leaves':150, \n",
    "                                        'max_bin':100, \n",
    "                                        'bagging_fraction':0.8, \n",
    "                                        'bagging_freq':10, \n",
    "                                        'n_jobs':60, \n",
    "                                        'feature_fraction':0.5, \n",
    "                                        'lambda_l1':0, \n",
    "                                        'lambda_l2':1, \n",
    "                                        'min_child_weight':1e-05,\n",
    "                                        'min_data_in_leaf':100, \n",
    "                                        'random_state':42\n",
    "                                    },\n",
    "                         'fit_params': \n",
    "                                    {\n",
    "                                        'eval_set': [(X_trn, y_trn),\n",
    "                                                     (X_tst, y_tst)], \n",
    "                                        'early_stopping_rounds': 15,\n",
    "                                        'eval_metric': ['auc', 'logloss'], \n",
    "                                        'sample_weight': weights,\n",
    "                                        'verbose': False\n",
    "                                    }\n",
    "                        },\n",
    "        XGBClassifier:\n",
    "                        {'params':        \n",
    "                                    {\n",
    "                                         'base_score': 0.5,\n",
    "                                         'booster': 'gbtree',\n",
    "                                         'colsample_bylevel': 1,\n",
    "                                         'colsample_bytree': 0.6,\n",
    "                                         'gamma': 5,\n",
    "                                         'learning_rate': 0.1,\n",
    "                                         'max_delta_step': 0,\n",
    "                                         'max_depth': 10,\n",
    "                                         'min_child_weight': 10,\n",
    "                                         'missing': None,\n",
    "                                         'n_estimators': 10000,\n",
    "                                         'n_jobs': 60,\n",
    "                                         'objective': 'binary:logistic',\n",
    "                                         'random_state': 0,\n",
    "                                         'reg_alpha': 0,\n",
    "                                         'reg_lambda': 1,\n",
    "                                         'scale_pos_weight': 1,\n",
    "                                         'seed': 42,\n",
    "                                         #'silent': True,\n",
    "                                         'subsample': 1.0\n",
    "                                    },\n",
    "                         'fit_params': \n",
    "                                    {\n",
    "                                        'eval_set': [(X_trn, y_trn),\n",
    "                                                     (X_tst, y_tst)], \n",
    "                                        'early_stopping_rounds': 15,\n",
    "                                        'eval_metric': ['auc', 'logloss'], \n",
    "                                        'sample_weight': weights,\n",
    "                                        'verbose': False\n",
    "                                    }                        \n",
    "                        \n",
    "                        },\n",
    "        \n",
    "        RandomForestClassifier:\n",
    "                        {'params':           \n",
    "                                    {\n",
    "                                        'n_estimators': 10000, \n",
    "                                        'criterion': 'gini', \n",
    "                                        'max_depth': None, \n",
    "                                        'min_samples_split':2, \n",
    "                                        'min_samples_leaf':1, \n",
    "                                        'min_weight_fraction_leaf':0.0, \n",
    "                                        'max_features': 'auto', \n",
    "                                        'max_leaf_nodes': None, \n",
    "                                        'min_impurity_decrease': 0.0,\n",
    "                                        'bootstrap': True, \n",
    "                                        'oob_score': False, \n",
    "                                        'n_jobs': 40, \n",
    "                                        'random_state': 32, \n",
    "                                        'verbose': 0\n",
    "                                    },\n",
    "                         'fit_params': {}\n",
    "                        }\n",
    "    }\n",
    "    \n",
    "\n",
    "    params     = switcher.get(model_name)['params']\n",
    "    fit_params = switcher.get(model_name)['fit_params']\n",
    "    \n",
    "    return params, fit_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def predict(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:,1]\n",
    "    print('AUC: %.5f, Precision: %.5f, Recall: %.5f, F1: %.5f \\n' % (roc_auc_score(y_test, y_pred_proba), \\\n",
    "                                    precision_score(y_test, y_pred), recall_score(y_test, y_pred), \n",
    "                                                                     f1_score(y_test, y_pred)))\n",
    "    print('Average precision: %.5f \\n' % average_precision_score(y_test, y_pred_proba))\n",
    "    print(classification_report(y_test, y_pred, digits=5))\n",
    "    plt.confusion_matrix_heatmap(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def eval_model(model, X, y, threshold=0.5):\n",
    "    if model is XGBClassifier: \n",
    "        y_pred_proba = model.predict_proba(X, ntree_limit=model.best_ntree_limit)[:, 1]\n",
    "    else:\n",
    "        y_pred_proba = model.predict_proba(X)[:, 1]\n",
    "        \n",
    "    y_pred = binarize(y_pred_proba.reshape(-1,1), threshold=threshold).reshape(-1).astype(int)\n",
    "    #y_pred[X.cluster == 0] = 0\n",
    "\n",
    "    TP = np.sum(np.logical_and(y_pred == 1, y == 1))\n",
    "    TN = np.sum(np.logical_and(y_pred == 0, y == 0))\n",
    "    FP = np.sum(np.logical_and(y_pred == 1, y == 0))\n",
    "    FN = np.sum(np.logical_and(y_pred == 0, y == 1))\n",
    "    \n",
    "    print('-' * 68)\n",
    "    if model is XGBClassifier:\n",
    "        print('AUC:%.5f, Precision:%.5f, Recall:%.5f, Trees:%d, True:%d of %d, Total:%d, Tr:%.2f' %\n",
    "              (roc_auc_score(y, y_pred_proba),\n",
    "               precision_score(y, y_pred),\n",
    "               recall_score(y, y_pred),\n",
    "               model.best_ntree_limit, TP, TP+FP, TP+FN,\n",
    "               threshold), flush=True)\n",
    "    else:\n",
    "         print('AUC:%.5f, Precision:%.5f, Recall:%.5f, True:%d of %d, Total:%d, Tr:%.2f' %\n",
    "              (roc_auc_score(y, y_pred_proba),\n",
    "               precision_score(y, y_pred),\n",
    "               recall_score(y, y_pred), TP, TP+FP, TP+FN,\n",
    "               threshold), flush=True)       \n",
    "    print('-' * 68)\n",
    "    \n",
    "    return y_pred, y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def train_full(X_train, X_test, \n",
    "               y_train, y_test, \n",
    "               model_name, \n",
    "               weights,\n",
    "               cat_features, \n",
    "               cattopoly, \n",
    "               threshold=0.5, \n",
    "               save_dict=False):\n",
    "    \n",
    "    X_trn, X_tst = fe.features_endineering( X_train, X_test, \n",
    "                                            y_train, \n",
    "                                            cat_features, \n",
    "                                            cattopoly, \n",
    "                                            save_dict=save_dict)\n",
    "    \n",
    "    params, fit_params = getboostparams(model_name, X_trn, y_train, X_tst, y_test, weights[:len(X_trn)])\n",
    "    model=model_name(**params)\n",
    "    model.fit( X_trn, y_train, **fit_params )\n",
    "        \n",
    "    joblib.dump(model, get_path('pkl_store', model_name.__name__ + '_weights_f.pkl'), compress=9)\n",
    "    \n",
    "    y_val, y_val_proba = eval_model(model, X_tst, y_test, threshold=threshold)\n",
    "\n",
    "    plt.auc_curve(y_test, y_val_proba)\n",
    "    plt.precision_recall_threshold_curve(y_test, y_val_proba)\n",
    "    plt.precision_recall_curve(y_test, y_val_proba)\n",
    "    plt.distplot(y_val_proba)\n",
    "    plt.feature_impotance(model, X_trn, count_top_features = 20)  \n",
    "    \n",
    "\n",
    "    return y_val, y_val_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def train(X, y, \n",
    "          columns, \n",
    "          model_name,\n",
    "          weights,\n",
    "          cat_features, \n",
    "          cattopoly, \n",
    "          n_folds=4, \n",
    "          log=True, \n",
    "          threshold=0.5, \n",
    "          save_dict=False):\n",
    "\n",
    "    blend_train       = np.zeros(X.shape[0])\n",
    "    blend_train_proba = np.zeros(X.shape[0])\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "        \n",
    "        X_trn, X_tst = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_trn, y_tst = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        X_trn, X_tst = fe.features_endineering(X_trn, X_tst, y_trn, \n",
    "                                                  cat_features, cattopoly, \n",
    "                                                  save_dict=save_dict)\n",
    "             \n",
    "        params, fit_params = getboostparams(model_name, X_trn, y_trn, X_tst, y_tst, weights[train_index])\n",
    "        model=model_name(**params)\n",
    "        model.fit( X_trn, y_trn, **fit_params )          \n",
    "\n",
    "        blend_train[test_index], blend_train_proba[test_index] = eval_model(model, X_tst, y_tst, \n",
    "                                                                            threshold=threshold)\n",
    "        #joblib.dump(model, model_name, compress=9)\n",
    "        \n",
    "    if log: \n",
    "        print('-' * 68)\n",
    "        #calculate metrics over whole dataset:\n",
    "        TP = np.sum(np.logical_and(blend_train == 1, y == 1))\n",
    "        TN = np.sum(np.logical_and(blend_train == 0, y == 0))\n",
    "        FP = np.sum(np.logical_and(blend_train == 1, y == 0))\n",
    "        FN = np.sum(np.logical_and(blend_train == 0, y == 1))\n",
    "\n",
    "        print('AUC: %7.5f, Precision: %7.5f, Recall: %7.5f, True:%d of %d, Total:%d' %\n",
    "              (roc_auc_score(y, blend_train_proba),\n",
    "               precision_score(y, blend_train),\n",
    "               recall_score(y, blend_train),\n",
    "               TP, TP+FP, TP+FN), flush=True)\n",
    "\n",
    "        print('f1_score: %.5f \\n' % f1_score(y, blend_train))\n",
    "        print('Average precision: %.5f \\n' % average_precision_score(y, blend_train_proba))\n",
    "\n",
    "        plt.auc_curve(y, blend_train_proba)\n",
    "        plt.precision_recall_threshold_curve(y, blend_train_proba)\n",
    "        plt.feature_impotance(model, X, count_top_features = 20)\n",
    "    \n",
    "    return blend_train, blend_train_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('pkl_store/total_dataset_unencoded.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "weights = joblib.load(get_path('pkl_store', 'weights_train_test_similarity.pkl')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True,drop=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "top_corr_features = pd.read_pickle('pkl_store/top_corr_features_le_all.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le=joblib.load(get_path('pkl_store', 'label_encoders_trn.pkl'))\n",
    "cat_features=list(set(le.keys()))\n",
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# define base features to calculate polynomial features interactions \n",
    "cattopoly=[ 'product_avg_mean',\n",
    "            'product_avg_std',\n",
    "            'product_group_avg_mean',\n",
    "            'product_group_avg_std',\n",
    "            'gosb_avg_mean',\n",
    "            'gosb_avg_std',\n",
    "            'tb_avg_mean',\n",
    "            'tb_avg_std',\n",
    "            'priority_client_avg_mean',\n",
    "            'priority_client_avg_std',\n",
    "            'days_between_km_dealdiff',\n",
    "            'sum_sold_total_cumulate_km_shift_3'\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "balance = pd.concat([df.groupby('product_group')['is_sold'].sum(), \n",
    "                     df.groupby('product_group')['is_sold'].count()], axis=1)\n",
    "balance.columns = ['Sum of sold', 'Count']\n",
    "balance['balance'] = balance['Sum of sold'] / balance['Count']\n",
    "balance = balance.sort_values('balance', ascending=False)\n",
    "balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.to_series().groupby(df.dtypes).groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dd=df.groupby('product_group')[['is_sold','days_between']].sum()\n",
    "dd.loc[:,['is_sold','days_between']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "product_groups_without_balance = list(balance[(balance['balance'] == 1) | (balance['balance'] == 0)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df = df[~df['product_group'].isin(product_groups_without_balance)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_counts = pd.DataFrame()\n",
    "for col in df.columns:\n",
    "    unique_counts = unique_counts.append([[col, len(df[col].unique())]])\n",
    "unique_counts.columns = ['colname', 'unique_counts']\n",
    "unique_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_unique_columns = unique_counts[unique_counts['unique_counts'] == 1]['colname'].values\n",
    "non_unique_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "target_column = 'is_sold'\n",
    "#drop_columns = ['ckp']\n",
    "#X, y = df.drop(target_column, axis = 1).drop(drop_columns, 1), df[target_column]\n",
    "X, y = df.drop(non_unique_columns, axis = 1).drop(target_column, axis = 1), df[target_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X[top_corr_features]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1=df.groupby('product_group')['is_sold'].transform(lambda x: np.log((x.count()-x.sum())/x.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby('product_group')['is_sold'].agg([('sum',lambda x: x.sum()),('count', lambda x: x.count()),\n",
    "                                            ('ratio_1', lambda x: x.sum()/x.count()),\n",
    "                                            ('ratio_0', lambda x: (x.count()-x.sum())/x.count())]).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X_test, X_train, y_test, y_train = train_test_split(X, y, shuffle=False, test_size=0.70, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "null_compare = pd.DataFrame(X_train.isnull().sum(), columns=['train_null_count'])\n",
    "null_compare['train_null_ratio'] = null_compare['train_null_count'] / X_train.shape[0] * 100\n",
    "null_compare = null_compare.join(X_test.isnull().sum().rename('test_null_count'))\n",
    "null_compare['test_null_ratio'] = null_compare['test_null_count'] / X_test.shape[0] * 100\n",
    "null_compare['diff_abs'] = np.abs(null_compare['train_null_ratio'] - null_compare['test_null_ratio'])\n",
    "null_compare.sort_values('diff_abs', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X_train_meta=X_train.copy()\n",
    "X_test_meta=X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_meta.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify a list of estimators to fit the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#cls={RandomForestClassifier: 'randomf', LGBMClassifier: 'lightgbm', XGBClassifier: 'xgboost'}\n",
    "cls={LGBMClassifier: 'lightgbm', XGBClassifier: 'xgboost'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the base model to each training fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## For each base model\n",
    "# Fit the base model to the training fold and make predictions on the test fold\n",
    "# Store tese predictions in X_train_meta to be used as features for the stacking model\n",
    "for model in list(cls.keys()):\n",
    "    print(model)\n",
    "    print('-'*50)\n",
    "    blend_train, blend_train_proba = train(\n",
    "                                               X_train, y_train, \n",
    "                                               columns=X.columns, \n",
    "                                               model_name=model,\n",
    "                                               weights=weights,\n",
    "                                               cattopoly=cattopoly,\n",
    "                                               cat_features=cat_features,\n",
    "                                               n_folds=3, \n",
    "                                               log=True,\n",
    "                                               threshold=0.5,\n",
    "                                               save_dict= False\n",
    "                                          )\n",
    "    X_train_meta=X_train_meta.join(pd.DataFrame(pd.Series(blend_train, name=cls[model], index=X_train_meta.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(X_train_meta, get_path('pkl_store', 'X_train_weights_meta.pkl'))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the base model to the full training dataset (No Cross-Folding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the base model to the full training dataset and make predictions on the test dataset\n",
    "# Store tese predictions inside X_test_meta\n",
    "for model in list(cls.keys()):\n",
    "    print(model)\n",
    "    print('-'*50)    \n",
    "    y_tst, y_tst_proba = train_full(\n",
    "                                        X_train, X_test, \n",
    "                                        y_train, y_test, \n",
    "                                        model_name=model,\n",
    "                                        weights=weights,\n",
    "                                        cattopoly=cattopoly, \n",
    "                                        cat_features=cat_features, \n",
    "                                        threshold=0.5,\n",
    "                                        save_dict= True\n",
    "                                    )\n",
    "\n",
    "    \n",
    "    X_test_meta=X_test_meta.join(pd.DataFrame(pd.Series(y_tst, name=cls[model], index=X_test_meta.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(X_test_meta, get_path('pkl_store', 'X_test_weights_meta.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X_train_meta=joblib.load(get_path('pkl_store', 'X_train_weights_meta.pkl'))\n",
    "X_test_meta=joblib.load(get_path('pkl_store', 'X_test_weights_meta.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Perform features engineering over X_train_meta, X_test_meta\n",
    "X_train_meta = fe.features_enc_from_dict(X_train_meta, cat_features, cattopoly)\n",
    "X_test_meta  = fe.features_enc_from_dict(X_test_meta, cat_features, cattopoly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.pipeline import make_pipeline, make_union, Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "imp=Imputer(missing_values=np.nan, strategy='median', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "temp=trn.loc[:,~trn.columns.isin(cat_features)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Initialize LogisticRegression with CV\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "clf=LogisticRegressionCV (Cs=4, fit_intercept=True, cv=2, dual=False, penalty='l2', scoring='roc_auc', \\\n",
    "                          solver='sag', max_iter=7000, class_weight='balanced', \\\n",
    "                          n_jobs=20, verbose=1, refit=True, intercept_scaling=1.0, \\\n",
    "                          multi_class='ovr', random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "estimator = Pipeline([('imp', Imputer(missing_values=np.nan, strategy='median', axis=0)), \n",
    "                      ('logreg', clf)])\n",
    "# estimator= make_pipeline( Imputer(missing_values=np.nan, strategy='median', axis=1),\n",
    "#                           LogisticRegressionCV (  Cs=4, fit_intercept=True, cv=3, dual=False, penalty='l2', \\\n",
    "#                                                   scoring='roc_auc', solver='sag', max_iter=8000, class_weight='balanced', \\\n",
    "#                                                   n_jobs=80, verbose=1, refit=True, intercept_scaling=1.0, \\\n",
    "#                                                   multi_class='ovr', random_state=32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# with imputer\n",
    "#clf.fit(imp.fit_transform(X_train_meta_enc), y_train)\n",
    "estimator.fit(X_train_meta, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(clf, 'pkl_store/LogisticRegressionCV.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(clf, X_test_meta, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "params, fit_params = getboostparams(XGBClassifier, \n",
    "                                    X_train_meta, y_train, \n",
    "                                    X_test_meta, y_test,\n",
    "                                    weights[:len(X_train_meta)])\n",
    "xgb = XGBClassifier(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "xgb.fit(X_train_meta, y_train, **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(xgb, get_path('pkl_store', XGBClassifier.__name__ + '_weights_stack.pkl'), compress=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(xgb, X_test_meta, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.distplot(xgb.predict_proba(X_test_meta)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "params, fit_params = getboostparams(LGBMClassifier, \n",
    "                                    X_train_meta, y_train, \n",
    "                                    X_test_meta, y_test,\n",
    "                                    weights[:len(X_train_meta)])\n",
    "gbm = LGBMClassifier(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "gbm.fit(X_train_meta, y_train, **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(gbm, get_path('pkl_store', LGBMClassifier.__name__ + '_weights_stack.pkl'), compress=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(gbm, X_test_meta, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.distplot(gbm.predict_proba(X_test_meta)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare metric between product's groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "le_dict              = joblib.load(get_path('pkl_store', 'le_dict.pkl'))['product_group']\n",
    "product_group_lables = pd.DataFrame({'label': list(le_dict.get_keys()), 'value': list(le_dict.get_labels())})\n",
    "product_group_lables = product_group_lables.set_index('value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "product_group_lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "xgb = joblib.load(get_path('pkl_store', XGBClassifier.__name__ + '_stack.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred= xgb.predict(X_test_meta)\n",
    "y_pred_proba= xgb.predict_proba(X_test_meta)\n",
    "\n",
    "y_df_pred  = pd.Series(index=X_test_meta.index)\n",
    "y_df_proba = pd.Series(index=X_test_meta.index)\n",
    "\n",
    "y_df_pred.loc[X_test_meta.index] = y_pred\n",
    "y_df_proba.loc[X_test_meta.index] = y_pred_proba[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name = 'product_group'\n",
    "scores_by_product_groups = pd.DataFrame(columns=[col_name, 'accuracy', 'precision', 'recall'])\n",
    "for val in product_group_lables.index:\n",
    "    indexes = X_test_meta[X_test_meta[col_name] == val].index\n",
    "    scores_by_product_groups = scores_by_product_groups.append({col_name: product_group_lables.loc[val]['label'], \\\n",
    "                                     'accuracy': accuracy_score(y_test.loc[indexes], y_df_pred.loc[indexes]), \\\n",
    "                                     'precision': precision_score(y_test.loc[indexes], y_df_pred.loc[indexes]), \\\n",
    "                                     'recall': recall_score(y_test.loc[indexes], y_df_pred.loc[indexes])\n",
    "                                                               }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "scores_by_product_groups.sort_values('accuracy', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mplt.subplots(figsize=(10, 10))\n",
    "fig = sns.barplot(x=scores_by_product_groups[col_name], y=scores_by_product_groups['accuracy'], \n",
    "                  palette=sns.color_palette('RdBu_d',12))\n",
    "_, labels = mplt.xticks()\n",
    "fig.set_xticklabels(labels, rotation=90)\n",
    "# fig.set(ylabel=y_label, title=title)\n",
    "mplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make prediction on a new validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_tst = pd.read_pickle('pkl_store/test_dataset_unencoded.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X_val=df_tst.loc[:,top_corr_features]\n",
    "X_val_meta=X_val.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_dict     = joblib.load(get_path('pkl_store', 'le_dict.pkl'))\n",
    "for col in cat_features:\n",
    "    ss=set(X_val[col])-set(le_dict[col].get_keys())\n",
    "    print(list(ss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_enc = fe.features_enc_from_dict(X_val, cat_features, cattopoly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cls={LGBMClassifier: 'lightgbm', XGBClassifier: 'xgboost'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For each base model\n",
    "# Fit the base model to the training fold and make predictions on the test fold\n",
    "# Store tese predictions in X_train_meta to be used as features for the stacking model\n",
    "for model in list(cls.keys()):\n",
    "    print(model.__name__)\n",
    "    cl=joblib.load(get_path('pkl_store', model.__name__+'_weights_f.pkl'))\n",
    "    y_val_pred=cl.predict(X_val_enc)\n",
    "    X_val_meta=X_val_meta.join(pd.DataFrame(pd.Series(y_val_pred, name=cls[model], index=X_val_meta.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_meta = fe.features_enc_from_dict(X_val_meta, cat_features, cattopoly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "y_avr_proba=[]\n",
    "for model in list(cls.keys()):\n",
    "    count=count+1\n",
    "    print(model.__name__)\n",
    "    cl=joblib.load(get_path('pkl_store', model.__name__+'_weights_stack.pkl'))\n",
    "    y_val_proba = cl.predict_proba(X_val_meta)\n",
    "    if count != len(cls):\n",
    "        temp=y_val_proba\n",
    "    else:\n",
    "        y_avr_proba = (temp+y_val_proba)*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "y_pred = binarize(y_avr_proba[:,1].reshape(-1,1), threshold=0.5).reshape(-1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.distplot(y_avr_proba[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = mplt.figure(figsize=(9,5))\n",
    "sns.distplot(y_avr_proba[:,1], label='Validate', color='skyblue', hist=False)\n",
    "sns.distplot(gbm.predict_proba(X_test_meta)[:,1], label='LGBM_pred', color='red', hist=False)\n",
    "sns.distplot(xgb.predict_proba(X_test_meta)[:,1], label='XGB_pred', color='black', hist=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Probability Calibration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X_train_meta=joblib.load(get_path('pkl_store', 'X_train_meta.pkl'))\n",
    "X_test_meta=joblib.load(get_path('pkl_store', 'X_test_meta.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Perform features engineering over X_train_meta, X_test_meta\n",
    "X_train_meta = fe.features_enc_from_dict(X_train_meta, cat_features, cattopoly)\n",
    "X_test_meta  = fe.features_enc_from_dict(X_test_meta, cat_features, cattopoly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model=LGBMClassifier\n",
    "cl=joblib.load(get_path('pkl_store', model.__name__+'_stack.pkl'))\n",
    "y_tst_proba = cl.predict_proba(X_test_meta)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = mplt.subplots(1,figsize=(12,8))\n",
    "\n",
    "fraction_of_pos, mean_pred_val = calibration_curve(y_test, y_tst_proba, n_bins=20)\n",
    "mplt.plot(mean_pred_val, fraction_of_pos, 's-', markersize=2, color='red', label='Uncalibrated')\n",
    "mplt.plot([0,1],[0,1],'--',color='gray')\n",
    "\n",
    "methodlst=['sigmoid','isotonic']\n",
    "clr=['blue','green']\n",
    "i=0\n",
    "for method in methodlst:\n",
    "    clf_calib=CalibratedClassifierCV(cl, cv='prefit', method=method)\n",
    "    clf_calib.fit(X_train_meta, y_train)\n",
    "    y_tst_clf_calib_proba=clf_calib.predict_proba(X_test_meta)[:,1]\n",
    "    fraction_of_pos, mean_pred_val = calibration_curve(y_test, y_tst_clf_calib_proba, n_bins=20)\n",
    "    mplt.plot(mean_pred_val, fraction_of_pos, 's-', markersize=2, color=clr[i], label='Calibrated %s' % method)\n",
    "    i=i+1\n",
    "    \n",
    "mplt.xlabel('Mean Predicted Value')\n",
    "mplt.ylabel('Fraction of Positives')\n",
    "mplt.legend(loc='best')\n",
    "mplt.title('Compare Calibration Curves')\n",
    "mplt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
